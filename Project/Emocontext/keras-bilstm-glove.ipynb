{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import emoji\n",
    "from keras import optimizers\n",
    "from keras.callbacks import Callback, EarlyStopping, ModelCheckpoint\n",
    "from keras.layers import Dense, Embedding, LSTM, Bidirectional, Dropout, Input\n",
    "from keras.models import Sequential, Model\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try:\n",
    "#     from nltk.corpus import words\n",
    "# except LookupError:\n",
    "#     import nltk\n",
    "#     print(\"Downloading nltk words...\")\n",
    "#     nltk.download(\"words\")\n",
    "#     from nltk.corpus import words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_vectors_file = \"data/glove.twitter.27B.200d.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_wordmap = {}\n",
    "with open(glove_vectors_file, \"r\", encoding=\"utf8\") as glove:\n",
    "    for line in glove:\n",
    "        name, vector = tuple(line.split(\" \", 1))\n",
    "        glove_wordmap[name] = np.fromstring(vector, sep=\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Global Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_FILE = 'data/train.txt'\n",
    "DEV_FILE = 'data/dev.txt'\n",
    "TEST_FILE = 'data/test.txt'\n",
    "\n",
    "TURNS_NAMES = [\"turn1\", \"turn2\", \"turn3\"]\n",
    "LABEL = [\"label\"]\n",
    "CONCATENATED_TURNS = \"turns\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Emoticons map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70\n"
     ]
    }
   ],
   "source": [
    "EMOTICONS_MAP = {\n",
    "    'üòò': ' emoticon',\n",
    "    'üòç': ' happyemoticon',\n",
    "    'üòÅ': ' happyemoticon',\n",
    "    'üò≠': ' sademoticon',\n",
    "    'üòë': ' sademoticon',\n",
    "    'üòª': ' happyemoticon',\n",
    "    'üòÇ': ' happyemoticon',\n",
    "    'üëç': ' emoticon',\n",
    "    'üòÄ': ' happyemoticon',\n",
    "    ':D': ' happyemoticon',\n",
    "    'üôÇ':  ' happyemoticon',\n",
    "    '<3': ' happyemoticon',\n",
    "    'üòì' : ' sademoticon',\n",
    "    'üòí' : ' angryemoticon',\n",
    "    'üòà' : ' emoticon',\n",
    "    'üëø' : ' angryemoticon',\n",
    "    'üñë' : ' happyemoticon',\n",
    "    'üòæ' : ' emoticon',\n",
    "    'üò†' : ' angryemoticon',\n",
    "    'üëª' : ' emoticon',\n",
    "    ':(' : ' sademoticon',\n",
    "    ':)' : ' happyemoticon',\n",
    "    'xD' : ' happyemoticon',\n",
    "    'üíî' : ' sademoticon',\n",
    "    'üò•' : ' emoticon',\n",
    "    'üòû' : ' sademoticon',\n",
    "    'üò§' : ' angryemoticon',\n",
    "    'üòÉ' : ' happyemoticon',\n",
    "    'üò¶' : ' sademoticon',\n",
    "    ':3' : ' emoticon',\n",
    "    'üòº' : ' emoticon',\n",
    "    'üòè' : ' happyemoticon',\n",
    "    'üò±' : ' sademoticon',\n",
    "    'üò¨' : ' sademoticon',\n",
    "    'üôÅ' : ' sademoticon',\n",
    "    '</3' : ' sademoticon',\n",
    "    'üò∫' : ' happyemoticon',\n",
    "    'üò£' : ' angryemoticon',\n",
    "    'üò¢' : ' sademoticon',\n",
    "    'üòÜ' : ' happyemoticon',\n",
    "    'üòÑ' : ' happyemoticon',\n",
    "    'üòÖ' : ' happyemoticon',\n",
    "    ':-)' : ' happyemoticon',\n",
    "    'üòä' : ' happyemoticon',\n",
    "    'üòï' : ' sademoticon',\n",
    "    'üòΩ' : ' happyemoticon',\n",
    "    'üôÄ' : ' angryemoticon',\n",
    "    'ü§£' : ' happyemoticon',\n",
    "    'ü§ê' : ' emoticon',\n",
    "    'üò°' : ' sademoticon',\n",
    "    'üëå' : ' happyemoticon', \n",
    "    'üòÆ' : ' emoticon',\n",
    "    '‚ù§Ô∏è' : ' happyemoticon',\n",
    "    'üôÑ' : ' happyemoticon',\n",
    "    'üòø' : ' sademoticon',\n",
    "    'üòâ' : ' happyemoticon',\n",
    "    'üòã' : ' happyemoticon',\n",
    "    'üòê' : ' emoticon',\n",
    "    'üòπ' : ' happyemoticon',\n",
    "    'üò¥' : ' sademoticon',\n",
    "    'üí§' : ' emoticon',\n",
    "    'üòú' : ' happyemoticon',\n",
    "    'üòá' : ' happyemoticon',\n",
    "    'üòî' : ' sademoticon',\n",
    "    'üò©' : ' sademoticon',\n",
    "    '‚ù§' : ' happyemoticon',\n",
    "    'üò≤' : ' emoticon',\n",
    "    'üò´' : ' sademoticon',\n",
    "    'üò≥' : ' sademoticon',\n",
    "    'üò∞' : ' sademoticon',\n",
    "}\n",
    "print(len(EMOTICONS_MAP.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### print_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def print_model(model_summary, parameters, accuracy, file_name=\"models/experiments.txt\"):\n",
    "    with open(file_name, \"a\") as f:\n",
    "        delimiter = \"==============================================\"\n",
    "        acc_delim = \"----------------------------------------------\"\n",
    "        format_string = \"===Experiment===\\n%s\\n%s\\n%s\\n%s\\n%s\\n\"\n",
    "        f.write(format_string % (model_summary,\n",
    "                                 delimiter,\n",
    "                                 parameters,\n",
    "                                 acc_delim,\n",
    "                                 str(accuracy)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_file(file_path):\n",
    "    output_dict = dict()\n",
    "    with open(file_path, newline='\\n', encoding='utf8') as csvfile:\n",
    "        return pd.read_csv(csvfile, sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = parse_file(TRAIN_FILE)\n",
    "dev_data = parse_file(DEV_FILE)\n",
    "test_data = parse_file(TEST_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concatenate_turns(df, delim=\"fullstop\"):\n",
    "    turns = [(\"%s %s %s %s %s\" %\n",
    "                 (row[TURNS_NAMES[0]], delim,\n",
    "                  row[TURNS_NAMES[1]], delim,\n",
    "                  row[TURNS_NAMES[2]])).lower()\n",
    "                 for index, row in df.iterrows()]\n",
    "    df[CONCATENATED_TURNS] = pd.Series(turns, index=df.index)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def emoticons_replace(df):\n",
    "    for index, row in df.iterrows():\n",
    "        turns = emoji.demojize(row[CONCATENATED_TURNS])\n",
    "        # remove delimiters \":\"  (:smiley: -> smiley)\n",
    "        for emoj in re.findall(\":\\w*:\", turns):\n",
    "            turns  = turns.replace(emoj, emoj[1:-1])\n",
    "        df.at[index, CONCATENATED_TURNS] = turns\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_tokenizer = TweetTokenizer()\n",
    "def tokenize_turns(df):\n",
    "    turns = [tweet_tokenizer.tokenize(row[CONCATENATED_TURNS]) \n",
    "                for idx, row in df.iterrows()]\n",
    "    df[CONCATENATED_TURNS] = pd.Series(turns, index=df.index)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = emoticons_replace(concatenate_turns(train_data))\n",
    "dev = emoticons_replace(concatenate_turns(dev_data))\n",
    "test = emoticons_replace(concatenate_turns(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tok = tokenize_turns(train)\n",
    "dev_tok = tokenize_turns(dev)\n",
    "test_tok = tokenize_turns(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(train_tok[CONCATENATED_TURNS])\n",
    "tokenizer.fit_on_texts(dev_tok[CONCATENATED_TURNS])\n",
    "tokenizer.fit_on_texts(test_tok[CONCATENATED_TURNS])\n",
    "VOCABULARY = tokenizer.word_index\n",
    "VOCABULARY['unk'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19486\n"
     ]
    }
   ],
   "source": [
    "print(len(VOCABULARY.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_size = len(VOCABULARY.keys()) + 1\n",
    "max_sentence = 189 # 163,82,189\n",
    "embed_dim = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((vocabulary_size, embed_dim))\n",
    "for word, i in VOCABULARY.items():\n",
    "    embedding_vector = glove_wordmap.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "### angry: [1 0 0 0]\n",
    "### happy: [0 1 0 0]\n",
    "### others: [0 0 1 0]\n",
    "### sad: [0 0 0 1]\n",
    "labels = {0: 'angry',\n",
    "          1: 'happy',\n",
    "          2: 'others',\n",
    "          3: 'sad'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pad_sequences(tokenizer.texts_to_sequences(\n",
    "                            train_tok[CONCATENATED_TURNS]),\n",
    "                        maxlen=max_sentence)\n",
    "X_dev = pad_sequences(tokenizer.texts_to_sequences(dev_tok[CONCATENATED_TURNS]), maxlen=max_sentence)\n",
    "X_test = pad_sequences(tokenizer.texts_to_sequences(test_tok[CONCATENATED_TURNS]), maxlen=max_sentence)\n",
    "### angry: [1 0 0 0]\n",
    "### happy: [0 1 0 0]\n",
    "### others: [0 0 1 0]\n",
    "### sad: [0 0 0 1]\n",
    "Y_train = pd.get_dummies(train[LABEL]).as_matrix()\n",
    "# for i, t in enumerate(train[LABEL].iterrows()):\n",
    "#     if t[1]['label']=='others':\n",
    "#         print(Y_train[i])\n",
    "#         break\n",
    "Y_dev = pd.get_dummies(dev[LABEL]).as_matrix()\n",
    "Y_test = pd.get_dummies(test[LABEL]).as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30160, 189)\n",
      "(30160, 4)\n",
      "(2755, 189)\n",
      "(2755, 4)\n",
      "(5509, 189)\n",
      "(5509, 4)\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(X_train))\n",
    "print(np.shape(Y_train))\n",
    "\n",
    "print(np.shape(X_dev))\n",
    "\n",
    "print(np.shape(Y_dev))\n",
    "\n",
    "\n",
    "print(np.shape(X_test))\n",
    "\n",
    "print(np.shape(Y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model eval metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Metrics(Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.val_f1s = []\n",
    "        self.val_recalls = []\n",
    "        self.val_precisions = []\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        val_predict = (np.asarray(self.model.predict(self.validation_data[0]))).round()\n",
    "        val_targ = self.validation_data[1]\n",
    "        _val_f1 = f1_score(val_targ, val_predict, average='micro')\n",
    "        _val_recall = recall_score(val_targ, val_predict, average='micro')\n",
    "        _val_precision = precision_score(val_targ, val_predict, average='micro')\n",
    "        print(_val_f1)\n",
    "        print(_val_recall)\n",
    "        self.val_f1s.append(_val_f1)\n",
    "        self.val_recalls.append(_val_recall)\n",
    "        self.val_precisions.append(_val_precision)\n",
    "        print(\"‚Äî val_f1: %f ‚Äî val_precision: %f ‚Äî val_recall %f\" %(_val_f1, _val_precision, _val_recall))\n",
    "        return\n",
    " \n",
    "metrics = Metrics()\n",
    "\n",
    "def print_metrics(model,X,Y, file_name):\n",
    "    predicts = (np.asarray(model.predict(X))).round()\n",
    "    tp =[0,0,0,0]\n",
    "    fp =[0,0,0,0]\n",
    "    fn =[0,0,0,0]\n",
    "    for i,pred in enumerate(predicts):\n",
    "        p = np.argmax(pred)\n",
    "        y = np.argmax(Y[i])\n",
    "        if p == y:\n",
    "            tp[p] += 1\n",
    "        else:\n",
    "            fp[p] +=1\n",
    "            fn[y] +=1\n",
    "    prec = sum(tp)/sum(tp+fp)\n",
    "    rec = sum(tp)/sum(tp+fn)\n",
    "    with open(file_name, 'a') as f:\n",
    "        print(\"F1 all\", file=f)\n",
    "        print(2*prec*rec/(prec+rec), file=f) \n",
    "        print(\"***\", file=f)\n",
    "        for i in range(4):\n",
    "            print(\"F1 %s: \" % labels[i], file=f)\n",
    "            prec = tp[i]/(tp[i]+fp[i])\n",
    "            rec = tp[i]/(tp[i]+fn[i])\n",
    "            print(2*prec*rec/(prec+rec), file=f)\n",
    "            print(\"****\", file=f)\n",
    "        tp.pop(2)\n",
    "        fp.pop(2)\n",
    "        fn.pop(2)\n",
    "        print(\"F1 happy angry sad\", file=f)\n",
    "        prec = sum(tp)/sum(tp+fp)\n",
    "        rec = sum(tp)/sum(tp+fn)\n",
    "        f1= 2*prec*rec/(prec+rec)\n",
    "        print(f1, file=f)\n",
    "        return f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "lstm_out = 128\n",
    "batch_size = 128\n",
    "drop_out = 0.3\n",
    "loss_fct = 'binary_crossentropy'\n",
    "activation_fct = 'softmax'\n",
    "optimizer = \"Adam-0.01\"\n",
    "\n",
    "parameters = \"\"\"Epochs:%s\\nEmbed_dim: %s\\nLstm_out: %s\\nBatch size: %s\\nDrop_out: %s\n",
    "Loss_fct: %s\\nActivaion_fct: %s\\nOptimizer: %s\\n\n",
    "\"\"\" %(str(epochs), str(embed_dim), str(lstm_out), str(batch_size), str(drop_out), loss_fct,\n",
    "      activation_fct, optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_6 (InputLayer)         (None, 189)               0         \n",
      "_________________________________________________________________\n",
      "embedding_6 (Embedding)      (None, 189, 200)          3897400   \n",
      "_________________________________________________________________\n",
      "bidirectional_6 (Bidirection (None, 256)               336896    \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 100)               25700     \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 4)                 404       \n",
      "=================================================================\n",
      "Total params: 4,260,400\n",
      "Trainable params: 363,000\n",
      "Non-trainable params: 3,897,400\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "input_layer = Input(shape=(max_sentence,), dtype='int32')\n",
    "\n",
    "embedding_layer = Embedding(vocabulary_size,\n",
    "                            embed_dim,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=max_sentence,\n",
    "                            trainable=False)(input_layer)\n",
    "bi_lstm = Bidirectional(LSTM(lstm_out))(embedding_layer)\n",
    "dropout = Dropout(0.4)(bi_lstm)\n",
    "dense = Dense(128,activation=activation_fct)(dropout)\n",
    "dropout = Dropout(0.2)(dense)\n",
    "dense = Dense(4,activation=activation_fct)(dropout)\n",
    "adam = optimizers.Adam(lr=0.01)\n",
    "rmsprop = optimizers.RMSprop(lr=0.005)#, rho=0.9, epsilon=None, decay=0.0)\n",
    "sgd = optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "\n",
    "model = Model(inputs=[input_layer], outputs=dense)\n",
    "model.compile(loss = loss_fct, optimizer=adam, metrics = ['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_CHECKPOINT = 'models/8_emb-bilstm1280.42-2dense-glove200'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 30160 samples, validate on 2755 samples\n",
      "Epoch 1/10\n",
      "30160/30160 [==============================] - 1771s 59ms/step - loss: 0.4831 - acc: 0.7738 - val_loss: 0.2751 - val_acc: 0.9159\n",
      "0.8070759625390218\n",
      "0.7038112522686025\n",
      "‚Äî val_f1: 0.807076 ‚Äî val_precision: 0.945854 ‚Äî val_recall 0.703811\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.91588, saving model to models/8_emb-bilstm128-2dense-glove200\n",
      "Epoch 2/10\n",
      "30160/30160 [==============================] - 2285s 76ms/step - loss: 0.4368 - acc: 0.7952 - val_loss: 0.2490 - val_acc: 0.9170\n",
      "0.8078134845620667\n",
      "0.6980036297640654\n",
      "‚Äî val_f1: 0.807813 ‚Äî val_precision: 0.958624 ‚Äî val_recall 0.698004\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.91588 to 0.91697, saving model to models/8_emb-bilstm128-2dense-glove200\n",
      "Epoch 3/10\n",
      "30160/30160 [==============================] - 1963s 65ms/step - loss: 0.4063 - acc: 0.8190 - val_loss: 0.2338 - val_acc: 0.9165\n",
      "0.8053322048243758\n",
      "0.6907441016333938\n",
      "‚Äî val_f1: 0.805332 ‚Äî val_precision: 0.965500 ‚Äî val_recall 0.690744\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.91697\n",
      "Epoch 4/10\n",
      "30160/30160 [==============================] - 1881s 62ms/step - loss: 0.3929 - acc: 0.8280 - val_loss: 0.2215 - val_acc: 0.9123\n",
      "0.7921770900494305\n",
      "0.6689655172413793\n",
      "‚Äî val_f1: 0.792177 ‚Äî val_precision: 0.971022 ‚Äî val_recall 0.668966\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.91697\n",
      "Epoch 5/10\n",
      "30160/30160 [==============================] - 1879s 62ms/step - loss: 0.3832 - acc: 0.8320 - val_loss: 0.2019 - val_acc: 0.9219\n",
      "0.8201378734071444\n",
      "0.7125226860254084\n",
      "‚Äî val_f1: 0.820138 ‚Äî val_precision: 0.966043 ‚Äî val_recall 0.712523\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.91697 to 0.92187, saving model to models/8_emb-bilstm128-2dense-glove200\n",
      "Epoch 6/10\n",
      "30160/30160 [==============================] - 1882s 62ms/step - loss: 0.3876 - acc: 0.8232 - val_loss: 0.2240 - val_acc: 0.9191\n",
      "0.8119730185497471\n",
      "0.6990925589836661\n",
      "‚Äî val_f1: 0.811973 ‚Äî val_precision: 0.968326 ‚Äî val_recall 0.699093\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.92187\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x15cff8a3eb8>"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, Y_train, epochs=epochs, verbose=1, batch_size=batch_size,\n",
    "          validation_data=(X_dev, Y_dev),\n",
    "          callbacks=[metrics,\n",
    "                     EarlyStopping(),\n",
    "                     ModelCheckpoint(MODEL_CHECKPOINT, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "                    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model.evaluate(X_test, Y_test, callbacks=[metrics])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.25542368246338765, 0.9074695951970222]\n",
      "['loss', 'acc']\n"
     ]
    }
   ],
   "source": [
    "print(results)\n",
    "print(model.metrics_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5373134328358208\n"
     ]
    }
   ],
   "source": [
    "f1 = print_metrics(model, X_test, Y_test, 'models/5_opts_emb-bilstm64-glove')\n",
    "print(f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model save & load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = '%s-%s.json' % (MODEL_CHECKPOINT, str(f1))\n",
    "MODEL_W_PATH = '%s-%s.h5' % (MODEL_CHECKPOINT, str(f1))\n",
    "OPTS_PATH = 'models/1_opts-emb-bilstm-dr-dense'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(OPTS_PATH, 'w') as f:\n",
    "    f.write(str(model.get_config()))\n",
    "    f.write(\"\\n%s\" % parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_path):\n",
    "    return load_model(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = load_model(MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "multiclass-multioutput is not supported",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-31-e6be62e4463f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# print(f1_score(y_true, y_pred, average='macro'))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf1_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'micro'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;31m# print(f1_score(y_true, y_pred, average='weighted'))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m# print(f1_score(y_true, y_pred, average='samples'))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mf:\\python 36 64\\lib\\site-packages\\sklearn\\metrics\\classification.py\u001b[0m in \u001b[0;36mf1_score\u001b[1;34m(y_true, y_pred, labels, pos_label, average, sample_weight)\u001b[0m\n\u001b[0;32m    712\u001b[0m     return fbeta_score(y_true, y_pred, 1, labels=labels,\n\u001b[0;32m    713\u001b[0m                        \u001b[0mpos_label\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpos_label\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maverage\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 714\u001b[1;33m                        sample_weight=sample_weight)\n\u001b[0m\u001b[0;32m    715\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    716\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mf:\\python 36 64\\lib\\site-packages\\sklearn\\metrics\\classification.py\u001b[0m in \u001b[0;36mfbeta_score\u001b[1;34m(y_true, y_pred, beta, labels, pos_label, average, sample_weight)\u001b[0m\n\u001b[0;32m    826\u001b[0m                                                  \u001b[0maverage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maverage\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    827\u001b[0m                                                  \u001b[0mwarn_for\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'f-score'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 828\u001b[1;33m                                                  sample_weight=sample_weight)\n\u001b[0m\u001b[0;32m    829\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    830\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mf:\\python 36 64\\lib\\site-packages\\sklearn\\metrics\\classification.py\u001b[0m in \u001b[0;36mprecision_recall_fscore_support\u001b[1;34m(y_true, y_pred, beta, labels, pos_label, average, warn_for, sample_weight)\u001b[0m\n\u001b[0;32m   1023\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"beta should be >0 in the F-beta score\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1024\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1025\u001b[1;33m     \u001b[0my_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1026\u001b[0m     \u001b[0mpresent_labels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0munique_labels\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1027\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mf:\\python 36 64\\lib\\site-packages\\sklearn\\metrics\\classification.py\u001b[0m in \u001b[0;36m_check_targets\u001b[1;34m(y_true, y_pred)\u001b[0m\n\u001b[0;32m     86\u001b[0m     \u001b[1;31m# No metrics support \"multiclass-multioutput\" format\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0my_type\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m\"binary\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"multiclass\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"multilabel-indicator\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 88\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"{0} is not supported\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_type\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     89\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0my_type\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m\"binary\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"multiclass\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: multiclass-multioutput is not supported"
     ]
    }
   ],
   "source": [
    "# y_true = [[0,0,0,1], [0,1,0,0], [1,0,0,0]]\n",
    "# y_pred = [[0,0,0,1], [1,0,0,0], [1,0,0,0]]\n",
    "# # print(f1_score(y_true, y_pred, average='macro'))\n",
    "# print(f1_score(y_true, y_pred, average='micro')) \n",
    "# # print(f1_score(y_true, y_pred, average='weighted')) \n",
    "# # print(f1_score(y_true, y_pred, average='samples')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect as i\n",
    "import sys\n",
    "sys.stdout.write(i.getsource(model.evaluate))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Emocontext",
   "language": "python",
   "name": "emocontext"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "notify_time": "0"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
