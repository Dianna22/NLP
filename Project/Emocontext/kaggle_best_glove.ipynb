{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install emoji","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\nimport emoji\nimport re\nimport csv\nfrom keras import optimizers\nfrom keras.callbacks import Callback, EarlyStopping, ModelCheckpoint\nfrom keras.layers import Dense, Embedding, LSTM, Bidirectional, Dropout, Input, Reshape\nfrom keras.models import Sequential, Model\nfrom keras.preprocessing.sequence import pad_sequences\n\nfrom keras.layers import GlobalAveragePooling2D\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.utils import to_categorical\nfrom nltk.tokenize import TweetTokenizer\nfrom sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score\nimport time\nimport tensorflow as tf\nfrom keras.wrappers.scikit_learn import KerasClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom matplotlib import pyplot as plt\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"path_prefix='../input/'\nTRAIN_FILE = path_prefix + 'emocontext/train.txt'\nDEV_FILE =  path_prefix + 'emocontext/dev.txt'\nTEST_FILE =  path_prefix + 'emocontext/test.txt'\n\nTURNS_NAMES = [\"turn1\", \"turn2\", \"turn3\"]\nLABEL = [\"label\"]\nTURNS_CONCAT = \"turns\"\ndef parse_file(file_path):\n    output_dict = dict()\n    with open(file_path, newline='\\n', encoding='utf8') as csvfile:\n        return pd.read_csv(csvfile, sep=\"\\t\")\n\ntrain_data = parse_file(TRAIN_FILE)\ndev_data = parse_file(DEV_FILE)\ntest_data = parse_file(TEST_FILE)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def concatenate_turns(df, delim=\"fullstop\"):\n    turns = [(\"%s %s %s %s %s\" %\n                 (row[TURNS_NAMES[0]], delim,\n                  row[TURNS_NAMES[1]], delim,\n                  row[TURNS_NAMES[2]])).lower()\n                 for index, row in df.iterrows()]\n    df[TURNS_CONCAT] = pd.Series(turns, index=df.index)\n    return df\ndef emoticons_replace(df):\n    for index, row in df.iterrows():\n        for turn in range(3):\n          turns = emoji.demojize(row[TURNS_NAMES[turn]])\n          # remove delimiters \":\"  (:smiley: -> smiley)\n          for emoj in re.findall(\":\\w*:\", turns):\n              turns  = turns.replace(emoj, emoj[1:-1]).replace(\"_\", \" \")\n          df.at[index, TURNS_NAMES[turn]] = turns\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = concatenate_turns(emoticons_replace(train_data))\ndev = concatenate_turns(emoticons_replace(dev_data))\ntest = concatenate_turns(emoticons_replace(test_data))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tweet_tokenizer = TweetTokenizer()\ndef tokenize_turns(df):\n    turns = [tweet_tokenizer.tokenize(row[TURNS_CONCAT]) \n                for idx, row in df.iterrows()]\n    df[TURNS_CONCAT] = pd.Series(turns, index=df.index)\n    return df\ntrain_tok = tokenize_turns(train)\ndev_tok = tokenize_turns(dev)\ntest_tok = tokenize_turns(test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.preprocessing.text import Tokenizer\ntokenizer = Tokenizer()\ntokenizer.fit_on_texts(train_tok[TURNS_CONCAT])\ntokenizer.fit_on_texts(dev_tok[TURNS_CONCAT])\ntokenizer.fit_on_texts(test_tok[TURNS_CONCAT])\nVOCABULARY = tokenizer.word_index\nVOCABULARY['unk'] = 0\nvocabulary_size = len(VOCABULARY.keys()) + 1\nmax_sentence = 189 # 163,82,189\nembed_dim = 200","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"glove_vectors_file = path_prefix+\"glove-global-vectors-for-word-representation/glove.twitter.27B.200d.txt\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"glove_wordmap = {}\nwith open(glove_vectors_file, \"r\", encoding=\"utf8\") as glove:\n    for line in glove:\n        name, vector = tuple(line.split(\" \", 1))\n        glove_wordmap[name] = np.fromstring(vector, sep=\" \")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"embedding_matrix = np.zeros((vocabulary_size, embed_dim))\nfor word, i in VOCABULARY.items():\n    embedding_vector = glove_wordmap.get(word)\n    if embedding_vector is not None:\n        # words not found in embedding index will be all-zeros.\n        embedding_matrix[i] = embedding_vector","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### angry: [1 0 0 0]\n### happy: [0 1 0 0]\n### others: [0 0 1 0]\n### sad: [0 0 0 1]\n\nlabels = {0: 'angry',\n          1: 'happy',\n          2: 'others',\n          3: 'sad'}\nX_train = pad_sequences(tokenizer.texts_to_sequences(\n                            train_tok[TURNS_CONCAT]),\n                        maxlen=max_sentence)\nX_dev = pad_sequences(tokenizer.texts_to_sequences(dev_tok[TURNS_CONCAT]), maxlen=max_sentence)\nX_test = pad_sequences(tokenizer.texts_to_sequences(test_tok[TURNS_CONCAT]), maxlen=max_sentence)\nY_train = pd.get_dummies(train[LABEL]).as_matrix()\nY_dev = pd.get_dummies(dev[LABEL]).as_matrix()\nY_test = pd.get_dummies(test[LABEL]).as_matrix()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Metrics(Callback):\n    def __init__(self, test_X, test_Y, tolerance):\n        self.test_X = test_X\n        self.test_Y = test_Y\n        self.max_f1 = 0\n        self.f1_prev = 0\n        self.tolerance = tolerance\n        self.decreasing_times = 0\n        \n        \n    def on_train_begin(self, logs={}):\n        self.val_f1s = []\n        self.val_recalls = []\n        self.val_precisions = []\n        self.i = 0\n        self.x = []\n        \n        self.f1s_test = []\n        self.f1s_val = []\n        self.losses = []\n        self.val_losses = []\n        \n        self.logs = []\n        self.fig = plt.figure()\n    \n    def plot_losses(self, f1_val,f1_test, logs):\n        self.logs.append(logs)\n        self.x.append(self.i)\n        self.losses.append(logs.get('loss'))\n        self.val_losses.append(logs.get('val_loss'))\n        self.f1s_test.append(f1_test)\n        self.f1s_val.append(f1_val)\n        self.i += 1\n        \n#         clear_output(wait=True)\n        \n        plt.subplot(2,1,1)\n        plt.plot(self.x, self.losses, label=\"train_loss\")\n        plt.plot(self.x, self.val_losses, label=\"val_loss\")\n        plt.legend()\n        plt.subplot(2,1,2)\n        plt.plot(self.x, self.f1s_val, label=\"f1_val \" + '{:.4f}'.format(max(self.f1s_val)))\n        plt.plot(self.x, self.f1s_test, label=\"f1_test \" + '{:.4f}'.format(max(self.f1s_test)))\n        \n        plt.legend(loc=0)\n\n        plt.show();\n\n    def on_epoch_end(self, epoch, logs={}):\n        val_predict = (np.asarray(self.model.predict(self.validation_data[0]))).round()\n        val_targ = self.validation_data[1]\n\n        _val_f1 = f1_score(val_targ, val_predict, average='micro')\n        _val_recall = recall_score(val_targ, val_predict, average='micro')\n        _val_precision = precision_score(val_targ, val_predict, average='micro')\n\n        self.val_f1s.append(_val_f1)\n        self.val_recalls.append(_val_recall)\n        self.val_precisions.append(_val_precision)\n        print(\"— val_f1: %f — val_precision: %f — val_recall %f\" %(_val_f1, _val_precision, _val_recall))\n        predicts = self.model.predict(self.test_X)\n        test_predict = (np.asarray(predicts)).round()\n        f1 = print_metrics_predicted(val_predict, val_targ)\n        print(\"#############\\nF1 test:\\n#############\")\n        f_test = print_metrics_predicted(test_predict, self.test_Y)\n        self.plot_losses(f1, f_test, logs)\n        if f_test > self.max_f1:\n            self.max_f1 = f_test\n        if f_test < self.f1_prev:\n            self.decreasing_times += 1\n            if self.decreasing_times > self.tolerance:\n                self.model.stop_training = True\n        else:\n            self.decreasing_times = 0\n        self.f1_prev = f_test\n        return\n \n# metrics = Metrics()\n\ndef print_metrics_predicted(predicts,Y,filename=None):\n    tp =[0,0,0,0]\n    fp =[0,0,0,0]\n    fn =[0,0,0,0]\n    for i,pred in enumerate(predicts):\n        p = np.argmax(pred)\n        y = np.argmax(Y[i])\n        if p == y:\n            tp[p] += 1\n        else:\n            fp[p] +=1\n            fn[y] +=1\n    prec = sum(tp)/(sum(tp+fp)+np.finfo(float).eps)\n    rec = sum(tp)/(sum(tp+fn)+np.finfo(float).eps)\n    print(\"F1 all\")\n    f1_all = 2*prec*rec/(prec+rec+np.finfo(float).eps)\n    print(f1_all) \n    print(\"***\")\n    for i in range(4):\n      print(\"F1 %s: \" % labels[i])\n      prec = tp[i]/(tp[i]+fp[i]+np.finfo(float).eps)\n      rec = tp[i]/(tp[i]+fn[i]+np.finfo(float).eps)\n      f1 = 2*prec*rec/(prec+rec+np.finfo(float).eps)\n      print(f1)\n      print(\"****\")\n    tp.pop(2)\n    fp.pop(2)\n    fn.pop(2)\n    print(\"F1 happy angry sad\")\n    prec = sum(tp)/(sum(tp+fp)+np.finfo(float).eps)\n    rec = sum(tp)/(sum(tp+fn)+np.finfo(float).eps)\n    f1= 2*prec*rec/(prec+rec+np.finfo(float).eps)\n    print(f1)\n    return f1\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Attention(tf.keras.Model):\n    def __init__(self, units):\n        super(Attention, self).__init__()\n        self.W1 = tf.keras.layers.Dense(units)\n        self.W2 = tf.keras.layers.Dense(units)\n        self.V = tf.keras.layers.Dense(1)\n \n    def call(self, features, hidden):\n        hidden_with_time_axis = tf.expand_dims(hidden, 1)\n        score = tf.nn.tanh(self.W1(features) + self.W2(hidden_with_time_axis))\n        attention_weights = tf.nn.softmax(self.V(score), axis=1)\n        context_vector = attention_weights * features\n        context_vector = tf.reduce_sum(context_vector, axis=1)\n \n        return context_vector, attention_weights","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"epochs = 10\nlstm_out = 128\nbatch_size = 128\ndrop_out = 0.3\nloss_fct = 'binary_crossentropy'\nactivation_fct = 'softmax'\noptimizer = \"Adam-0.01\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_model():\n    input_layer = Input(shape=(max_sentence,), dtype='int32')\n\n    embedding_layer = Embedding(vocabulary_size,\n                                embed_dim,\n                                weights=[embedding_matrix],\n                                input_length=max_sentence,\n                                trainable=True)(input_layer)\n    bi_lstm = Bidirectional(LSTM(256))(embedding_layer)\n    dropout = Dropout(0.4)(bi_lstm)\n    dense = Dense(128,activation='relu')(dropout)\n    dropout = Dropout(0.2)(dense)\n    dense = Dense(64,activation='relu')(dropout)\n    dropout = Dropout(0.2)(dense)\n    dense = Dense(4,activation='softmax')(dropout)\n    adam = optimizers.Adam(lr=0.01)\n    rmsprop = optimizers.RMSprop(lr=0.005)#, rho=0.9, epsilon=None, decay=0.0)\n    sgd = optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n\n    model = Model(inputs=[input_layer], outputs=dense)\n    model.compile(loss = 'binary_crossentropy', optimizer=rmsprop, metrics = ['accuracy'])\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = create_model()\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import keras\nimport IPython\n# SVG(model_to_dot(model).create(prog='dot', format='svg'))\nkeras.utils.plot_model(model, to_file='test_keras_plot_model.png', show_shapes=True,show_layer_names=False)\nIPython.display.Image('test_keras_plot_model.png')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.fit(X_train, Y_train, epochs=50, verbose=1, batch_size=128,\n          validation_data=(X_dev, Y_dev),\n          callbacks=[Metrics(X_test, Y_test, 3),\n                    ])","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}