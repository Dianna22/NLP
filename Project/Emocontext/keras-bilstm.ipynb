{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import emoji\n",
    "from keras import optimizers\n",
    "from keras.layers import Dense, Embedding, LSTM, Bidirectional, Dropout\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Global Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_FILE = 'data/train.txt'\n",
    "DEV_FILE = 'data/dev.txt'\n",
    "TEST_FILE = 'data/test.txt'\n",
    "\n",
    "TURNS_NAMES = [\"turn1\", \"turn2\", \"turn3\"]\n",
    "LABEL = [\"label\"]\n",
    "CONCATENATED_TURNS = \"turns\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "##### Emoticons map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70\n"
     ]
    }
   ],
   "source": [
    "EMOTICONS_MAP = {\n",
    "    'üòò': ' emoticon',\n",
    "    'üòç': ' happyemoticon',\n",
    "    'üòÅ': ' happyemoticon',\n",
    "    'üò≠': ' sademoticon',\n",
    "    'üòë': ' sademoticon',\n",
    "    'üòª': ' happyemoticon',\n",
    "    'üòÇ': ' happyemoticon',\n",
    "    'üëç': ' emoticon',\n",
    "    'üòÄ': ' happyemoticon',\n",
    "    ':D': ' happyemoticon',\n",
    "    'üôÇ':  ' happyemoticon',\n",
    "    '<3': ' happyemoticon',\n",
    "    'üòì' : ' sademoticon',\n",
    "    'üòí' : ' angryemoticon',\n",
    "    'üòà' : ' emoticon',\n",
    "    'üëø' : ' angryemoticon',\n",
    "    'üñë' : ' happyemoticon',\n",
    "    'üòæ' : ' emoticon',\n",
    "    'üò†' : ' angryemoticon',\n",
    "    'üëª' : ' emoticon',\n",
    "    ':(' : ' sademoticon',\n",
    "    ':)' : ' happyemoticon',\n",
    "    'xD' : ' happyemoticon',\n",
    "    'üíî' : ' sademoticon',\n",
    "    'üò•' : ' emoticon',\n",
    "    'üòû' : ' sademoticon',\n",
    "    'üò§' : ' angryemoticon',\n",
    "    'üòÉ' : ' happyemoticon',\n",
    "    'üò¶' : ' sademoticon',\n",
    "    ':3' : ' emoticon',\n",
    "    'üòº' : ' emoticon',\n",
    "    'üòè' : ' happyemoticon',\n",
    "    'üò±' : ' sademoticon',\n",
    "    'üò¨' : ' sademoticon',\n",
    "    'üôÅ' : ' sademoticon',\n",
    "    '</3' : ' sademoticon',\n",
    "    'üò∫' : ' happyemoticon',\n",
    "    'üò£' : ' angryemoticon',\n",
    "    'üò¢' : ' sademoticon',\n",
    "    'üòÜ' : ' happyemoticon',\n",
    "    'üòÑ' : ' happyemoticon',\n",
    "    'üòÖ' : ' happyemoticon',\n",
    "    ':-)' : ' happyemoticon',\n",
    "    'üòä' : ' happyemoticon',\n",
    "    'üòï' : ' sademoticon',\n",
    "    'üòΩ' : ' happyemoticon',\n",
    "    'üôÄ' : ' angryemoticon',\n",
    "    'ü§£' : ' happyemoticon',\n",
    "    'ü§ê' : ' emoticon',\n",
    "    'üò°' : ' sademoticon',\n",
    "    'üëå' : ' happyemoticon', \n",
    "    'üòÆ' : ' emoticon',\n",
    "    '‚ù§Ô∏è' : ' happyemoticon',\n",
    "    'üôÑ' : ' happyemoticon',\n",
    "    'üòø' : ' sademoticon',\n",
    "    'üòâ' : ' happyemoticon',\n",
    "    'üòã' : ' happyemoticon',\n",
    "    'üòê' : ' emoticon',\n",
    "    'üòπ' : ' happyemoticon',\n",
    "    'üò¥' : ' sademoticon',\n",
    "    'üí§' : ' emoticon',\n",
    "    'üòú' : ' happyemoticon',\n",
    "    'üòá' : ' happyemoticon',\n",
    "    'üòî' : ' sademoticon',\n",
    "    'üò©' : ' sademoticon',\n",
    "    '‚ù§' : ' happyemoticon',\n",
    "    'üò≤' : ' emoticon',\n",
    "    'üò´' : ' sademoticon',\n",
    "    'üò≥' : ' sademoticon',\n",
    "    'üò∞' : ' sademoticon',\n",
    "}\n",
    "print(len(EMOTICONS_MAP.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### print_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def print_model(model_summary, parameters, accuracy, file_name=\"models/experiments.txt\"):\n",
    "    with open(file_name, \"a\") as f:\n",
    "        delimiter = \"==============================================\"\n",
    "        acc_delim = \"----------------------------------------------\"\n",
    "        format_string = \"===Experiment===\\n%s\\n%s\\n%s\\n%s\\n%s\\n\"\n",
    "        f.write(format_string % (model_summary,\n",
    "                                 delimiter,\n",
    "                                 parameters,\n",
    "                                 acc_delim,\n",
    "                                 str(accuracy)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_file(file_path):\n",
    "    output_dict = dict()\n",
    "    with open(file_path, newline='\\n', encoding='utf8') as csvfile:\n",
    "        return pd.read_csv(csvfile, sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = parse_file(TRAIN_FILE)\n",
    "dev_data = parse_file(DEV_FILE)\n",
    "test_data = parse_file(TEST_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concatenate_turns(df, delim=\"fullstop\"):\n",
    "    turns = [(\"%s %s %s %s %s\" %\n",
    "                 (row[TURNS_NAMES[0]], delim,\n",
    "                  row[TURNS_NAMES[1]], delim,\n",
    "                  row[TURNS_NAMES[2]])).lower()\n",
    "                 for index, row in df.iterrows()]\n",
    "    df[CONCATENATED_TURNS] = pd.Series(turns, index=df.index)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def emoticons_replace(df):\n",
    "    for index, row in df.iterrows():\n",
    "        turns = emoji.demojize(row[CONCATENATED_TURNS])\n",
    "        # remove delimiters \":\"  (:smiley: -> smiley)\n",
    "        for emoj in re.findall(\":\\w*:\", turns):\n",
    "            turns  = turns.replace(emoj, emoj[1:-1])\n",
    "        df.at[index, CONCATENATED_TURNS] = turns\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_tokenizer = TweetTokenizer()\n",
    "def tokenize_turns(df):\n",
    "    turns = [tweet_tokenizer.tokenize(row[CONCATENATED_TURNS]) \n",
    "                for idx, row in df.iterrows()]\n",
    "    df[CONCATENATED_TURNS] = pd.Series(turns, index=df.index)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = emoticons_replace(concatenate_turns(train_data))\n",
    "dev = emoticons_replace(concatenate_turns(dev_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ordinal encoding\n",
    "# from sklearn import preprocessing\n",
    "# le = preprocessing.LabelEncoder()\n",
    "# print(le.fit(train[LABEL]))\n",
    "# print(le.classes_)\n",
    "# print(train[LABEL])\n",
    "# print(le.transform(train[LABEL]))\n",
    "# print(train[LABEL])\n",
    "\n",
    "# One Hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_text = train[CONCATENATED_TURNS]\n",
    "all_text = all_text.append(dev[CONCATENATED_TURNS])\n",
    "\n",
    "tokenizer.fit_on_texts(all_text)\n",
    "X_train = pad_sequences(tokenizer.texts_to_sequences(train[CONCATENATED_TURNS]))\n",
    "X_dev = pad_sequences(tokenizer.texts_to_sequences(dev[CONCATENATED_TURNS]))\n",
    "\n",
    "Y_train = pd.get_dummies(train[LABEL])\n",
    "Y_dev = pd.get_dummies(dev[LABEL])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', 'char_level', 'document_count', 'filters', 'fit_on_sequences', 'fit_on_texts', 'index_docs', 'lower', 'num_words', 'sequences_to_matrix', 'split', 'texts_to_matrix', 'texts_to_sequences', 'texts_to_sequences_generator', 'word_counts', 'word_docs', 'word_index']\n",
      "16974\n"
     ]
    }
   ],
   "source": [
    "# print(tokenizer.word_counts)\n",
    "print(dir(tokenizer))\n",
    "print(len(tokenizer.word_counts))\n",
    "# print(tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_size = len(tokenizer.word_counts) + 1\n",
    "\n",
    "\n",
    "epochs = 10\n",
    "embed_dim = 256\n",
    "lstm_out = 128\n",
    "batch_size = 128\n",
    "drop_out = 0.3\n",
    "loss_fct = 'binary_crossentropy'\n",
    "activation_fct = 'softmax'\n",
    "optimizer = \"Adam-0.01\"\n",
    "\n",
    "parameters = \"\"\"Epochs:%s\\nEmbed_dim: %s\\nLstm_out: %s\\nBatch size: %s\\nDrop_out: %s\n",
    "Loss_fct: %s\\nActivaion_fct: %s\\nOptimizer: %s\\n\n",
    "\"\"\" %(str(epochs), str(embed_dim), str(lstm_out), str(batch_size), str(drop_out), loss_fct,\n",
    "      activation_fct, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, 163, 256)          4345600   \n",
      "_________________________________________________________________\n",
      "bidirectional_4 (Bidirection (None, 256)               394240    \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 4)                 1028      \n",
      "=================================================================\n",
      "Total params: 4,740,868\n",
      "Trainable params: 4,740,868\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(vocabulary_size, embed_dim,input_length = X_train.shape[1]))\n",
    "model.add(Bidirectional(LSTM(lstm_out)))\n",
    "model.add(Dropout(drop_out))\n",
    "model.add(Dense(4,activation=activation_fct))\n",
    "adam = optimizers.Adam(lr=0.01)\n",
    "rmsprop = optimizers.RMSprop(lr=0.005)#, rho=0.9, epsilon=None, decay=0.0)\n",
    "sgd = optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss = loss_fct, optimizer=adam, metrics = ['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Emocontext",
   "language": "python",
   "name": "emocontext"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
