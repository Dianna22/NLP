{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install emoji","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\nimport emoji\nimport re\nimport csv\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom keras import optimizers\nfrom keras.callbacks import Callback, EarlyStopping, ModelCheckpoint\nfrom keras.layers import Dense, Embedding, LSTM, Bidirectional, Dropout, Input, CuDNNLSTM, CuDNNLSTM, RNN, LSTMCell,TimeDistributed, Flatten, Activation, RepeatVector, Permute, Lambda, multiply, Dot, Reshape, merge\nfrom keras.models import Sequential, Model\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.utils import to_categorical\nfrom nltk.tokenize import TweetTokenizer\nfrom sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score\nimport time\nimport tensorflow as tf\nfrom keras.wrappers.scikit_learn import KerasClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom matplotlib import pyplot as plt\n\n\nfrom tensorflow.keras import backend as K\nfrom tensorflow.python.keras.utils import tf_utils\n","execution_count":1,"outputs":[{"output_type":"stream","text":"['glove-global-vectors-for-word-representation', 'emocontext']\n","name":"stdout"},{"output_type":"stream","text":"Using TensorFlow backend.\n","name":"stderr"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"path_prefix='../input/'\nTRAIN_FILE = path_prefix + 'emocontext/train.txt'\nDEV_FILE =  path_prefix + 'emocontext/dev.txt'\nTEST_FILE =  path_prefix + 'emocontext/test.txt'\n\nTURNS_NAMES = [\"turn1\", \"turn2\", \"turn3\"]\nLABEL = [\"label\"]\nTURNS_CONCAT = \"turns\"\ndef parse_file(file_path):\n    output_dict = dict()\n    with open(file_path, newline='\\n', encoding='utf8') as csvfile:\n        return pd.read_csv(csvfile, sep=\"\\t\")\n\ntrain_data = parse_file(TRAIN_FILE)\ndev_data = parse_file(DEV_FILE)\ntest_data = parse_file(TEST_FILE)","execution_count":2,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def concatenate_turns(df, delim=\"fullstop\"):\n    turns = [(\"%s %s %s %s %s\" %\n                 (row[TURNS_NAMES[0]], delim,\n                  row[TURNS_NAMES[1]], delim,\n                  row[TURNS_NAMES[2]])).lower()\n                 for index, row in df.iterrows()]\n    df[TURNS_CONCAT] = pd.Series(turns, index=df.index)\n    return df\ndef emoticons_replace(df):\n    for index, row in df.iterrows():\n        for turn in range(3):\n          turns = emoji.demojize(row[TURNS_NAMES[turn]])\n          # remove delimiters \":\"  (:smiley: -> smiley)\n          for emoj in re.findall(\":\\w*:\", turns):\n              turns  = turns.replace(emoj, emoj[1:-1]).replace(\"_\", \" \")\n          df.at[index, TURNS_NAMES[turn]] = turns\n    return df","execution_count":3,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = concatenate_turns(emoticons_replace(train_data))\ndev = concatenate_turns(emoticons_replace(dev_data))\ntest = concatenate_turns(emoticons_replace(test_data))","execution_count":4,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tweet_tokenizer = TweetTokenizer()\ndef tokenize_turns(df):\n    turns = [tweet_tokenizer.tokenize(row[TURNS_CONCAT]) \n                for idx, row in df.iterrows()]\n    df[TURNS_CONCAT] = pd.Series(turns, index=df.index)\n    return df\ntrain_tok = tokenize_turns(train)\ndev_tok = tokenize_turns(dev)\ntest_tok = tokenize_turns(test)","execution_count":5,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.preprocessing.text import Tokenizer\ntokenizer = Tokenizer()\ntokenizer.fit_on_texts(train_tok[TURNS_CONCAT])\ntokenizer.fit_on_texts(dev_tok[TURNS_CONCAT])\ntokenizer.fit_on_texts(test_tok[TURNS_CONCAT])\nVOCABULARY = tokenizer.word_index\nVOCABULARY['unk'] = 0\nvocabulary_size = len(VOCABULARY.keys()) + 1\nmax_sentence = 189 # 163,82,189\nembed_dim = 200","execution_count":6,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"glove_vectors_file = path_prefix+\"glove-global-vectors-for-word-representation/glove.twitter.27B.200d.txt\"","execution_count":7,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"glove_wordmap = {}\nwith open(glove_vectors_file, \"r\", encoding=\"utf8\") as glove:\n    for line in glove:\n        name, vector = tuple(line.split(\" \", 1))\n        glove_wordmap[name] = np.fromstring(vector, sep=\" \")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"embedding_matrix = np.zeros((vocabulary_size, embed_dim))\nfor word, i in VOCABULARY.items():\n    embedding_vector = glove_wordmap.get(word)\n    if embedding_vector is not None:\n        # words not found in embedding index will be all-zeros.\n        embedding_matrix[i] = embedding_vector","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### angry: [1 0 0 0]\n### happy: [0 1 0 0]\n### others: [0 0 1 0]\n### sad: [0 0 0 1]\n\nlabels = {0: 'angry',\n          1: 'happy',\n          2: 'others',\n          3: 'sad'}\nX_train = pad_sequences(tokenizer.texts_to_sequences(\n                            train_tok[TURNS_CONCAT]),\n                        maxlen=max_sentence)\nX_dev = pad_sequences(tokenizer.texts_to_sequences(dev_tok[TURNS_CONCAT]), maxlen=max_sentence)\nX_test = pad_sequences(tokenizer.texts_to_sequences(test_tok[TURNS_CONCAT]), maxlen=max_sentence)\nY_train = pd.get_dummies(train[LABEL]).as_matrix()\nY_dev = pd.get_dummies(dev[LABEL]).as_matrix()\nY_test = pd.get_dummies(test[LABEL]).as_matrix()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Metrics callback"},{"metadata":{"trusted":true},"cell_type":"code","source":"class Metrics(Callback):\n    def __init__(self, test_X, test_Y, tolerance):\n        self.test_X = test_X\n        self.test_Y = test_Y\n        self.max_f1 = 0\n        self.f1_prev = 0\n        self.tolerance = tolerance\n        self.decreasing_times = 0\n        \n        \n    def on_train_begin(self, logs={}):\n        self.val_f1s = []\n        self.val_recalls = []\n        self.val_precisions = []\n        self.i = 0\n        self.x = []\n        \n        self.f1s_test = []\n        self.f1s_val = []\n        self.losses = []\n        self.val_losses = []\n        self.epoch=0\n        \n        self.logs = []\n        self.fig = plt.figure()\n    \n    def plot_losses(self, f1_val,f1_test, logs):\n        self.logs.append(logs)\n        self.x.append(self.i)\n        self.losses.append(logs.get('loss'))\n        self.val_losses.append(logs.get('val_loss'))\n        self.f1s_test.append(f1_test)\n        self.f1s_val.append(f1_val)\n        self.i += 1\n        \n#         clear_output(wait=True)\n        \n        plt.subplot(2,1,1)\n        plt.plot(self.x, self.losses, label=\"train_loss\")\n        plt.plot(self.x, self.val_losses, label=\"val_loss\")\n        plt.legend()\n        plt.subplot(2,1,2)\n        plt.plot(self.x, self.f1s_val, label=\"f1_val \" + '{:.4f}'.format(max(self.f1s_val)))\n        plt.plot(self.x, self.f1s_test, label=\"f1_test \" + '{:.4f}, e={:d}'.format(max(self.f1s_test), self.epoch))\n        \n        plt.legend(loc=0)\n\n        plt.show();\n\n    def on_epoch_end(self, epoch, logs={}):\n        val_predict = (np.asarray(self.model.predict(self.validation_data[0]))).round()\n        val_targ = self.validation_data[1]\n\n        _val_f1 = f1_score(val_targ, val_predict, average='micro')\n        _val_recall = recall_score(val_targ, val_predict, average='micro')\n        _val_precision = precision_score(val_targ, val_predict, average='micro')\n\n        self.val_f1s.append(_val_f1)\n        self.val_recalls.append(_val_recall)\n        self.val_precisions.append(_val_precision)\n        print(\"— val_f1: %f — val_precision: %f — val_recall %f\" %(_val_f1, _val_precision, _val_recall))\n        predicts = self.model.predict(self.test_X)\n        test_predict = (np.asarray(predicts)).round()\n        f1 = print_metrics_predicted(val_predict, val_targ)\n        print(\"#############\\nF1 test:\\n#############\")\n        f_test = print_metrics_predicted(test_predict, self.test_Y)\n        self.plot_losses(f1, f_test, logs)\n        if f_test > self.max_f1:\n            self.max_f1 = f_test\n            self.epoch = epoch\n        if f_test < self.f1_prev:\n            self.decreasing_times += 1\n            if self.decreasing_times > self.tolerance:\n                self.model.stop_training = True\n        else:\n            self.decreasing_times = 0\n        self.f1_prev = f_test\n        return\n \n# metrics = Metrics()\n\ndef print_metrics_predicted(predicts,Y,filename=None):\n    tp =[0,0,0,0]\n    fp =[0,0,0,0]\n    fn =[0,0,0,0]\n    for i,pred in enumerate(predicts):\n        p = np.argmax(pred)\n        y = np.argmax(Y[i])\n        if p == y:\n            tp[p] += 1\n        else:\n            fp[p] +=1\n            fn[y] +=1\n    prec = sum(tp)/(sum(tp+fp)+np.finfo(float).eps)\n    rec = sum(tp)/(sum(tp+fn)+np.finfo(float).eps)\n    print(\"F1 all\")\n    f1_all = 2*prec*rec/(prec+rec+np.finfo(float).eps)\n    print(f1_all) \n    print(\"***\")\n    for i in range(4):\n      print(\"F1 %s: \" % labels[i])\n      prec = tp[i]/(tp[i]+fp[i]+np.finfo(float).eps)\n      rec = tp[i]/(tp[i]+fn[i]+np.finfo(float).eps)\n      f1 = 2*prec*rec/(prec+rec+np.finfo(float).eps)\n      print(f1)\n      print(\"****\")\n    tp.pop(2)\n    fp.pop(2)\n    fn.pop(2)\n    print(\"F1 happy angry sad\")\n    prec = sum(tp)/(sum(tp+fp)+np.finfo(float).eps)\n    rec = sum(tp)/(sum(tp+fn)+np.finfo(float).eps)\n    f1= 2*prec*rec/(prec+rec+np.finfo(float).eps)\n    print(f1)\n    return f1\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Attention(tf.keras.Model):\n    def __init__(self, units):\n        super(Attention, self).__init__()\n        self.W1 = tf.keras.layers.Dense(units)\n        self.W2 = tf.keras.layers.Dense(units)\n        self.V = tf.keras.layers.Dense(1)\n \n    def call(self, features, hidden):\n        hidden_with_time_axis = tf.expand_dims(hidden, 1)\n        score = tf.nn.tanh(self.W1(features) + self.W2(hidden_with_time_axis))\n        attention_weights = tf.nn.softmax(self.V(score), axis=1)\n        context_vector = attention_weights * features\n        context_vector = tf.reduce_sum(context_vector, axis=1)\n \n        return context_vector, attention_weights","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"epochs = 10\nlstm_out = 128\nbatch_size = 128\ndrop_out = 0.3\nloss_fct = 'binary_crossentropy'\nactivation_fct = 'softmax'\noptimizer = \"Adam-0.01\"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### [Att seq2seq](https://github.com/neonbjb/ml-notebooks/blob/master/keras-seq2seq-with-attention/keras_translate_notebook.ipynb)"},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# RNN \"Cell\" classes in Keras perform the actual data transformations at each timestep. Therefore, in order\n# to add attention to LSTM, we need to make a custom subclass of LSTMCell.\nclass AttentionLSTMCell(LSTMCell):\n    def __init__(self, **kwargs):\n        self.attentionMode = False\n        super(AttentionLSTMCell, self).__init__(**kwargs)\n    \n    # Build is called to initialize the variables that our cell will use. We will let other Keras\n    # classes (e.g. \"Dense\") actually initialize these variables.\n    @tf_utils.shape_type_conversion\n    def build(self, input_shape):        \n        # Converts the input sequence into a sequence which can be matched up to the internal\n        # hidden state.\n        self.dense_constant = TimeDistributed(Dense(self.units, name=\"AttLstmInternal_DenseConstant\"))\n        \n        # Transforms the internal hidden state into something that can be used by the attention\n        # mechanism.\n        self.dense_state = Dense(self.units, name=\"AttLstmInternal_DenseState\")\n        \n        # Transforms the combined hidden state and converted input sequence into a vector of\n        # probabilities for attention.\n        self.dense_transform = Dense(1, name=\"AttLstmInternal_DenseTransform\")\n        \n        # We will augment the input into LSTMCell by concatenating the context vector. Modify\n        # input_shape to reflect this.\n        batch, input_dim = input_shape[0]\n        batch, timesteps, context_size = input_shape[-1]\n        lstm_input = (batch, input_dim + context_size)\n        \n        # The LSTMCell superclass expects no constant input, so strip that out.\n        return super(AttentionLSTMCell, self).build(lstm_input)\n    \n    # This must be called before call(). The \"input sequence\" is the output from the \n    # encoder. This function will do some pre-processing on that sequence which will\n    # then be used in subsequent calls.\n    def setInputSequence(self, input_seq):\n        self.input_seq = input_seq\n        self.input_seq_shaped = self.dense_constant(input_seq)\n        self.timesteps = tf.shape(self.input_seq)[-2]\n    \n    # This is a utility method to adjust the output of this cell. When attention mode is\n    # turned on, the cell outputs attention probability vectors across the input sequence.\n    def setAttentionMode(self, mode_on=False):\n        self.attentionMode = mode_on\n    \n    # This method sets up the computational graph for the cell. It implements the actual logic\n    # that the model follows.\n    def call(self, inputs, states, constants):\n        # Separate the state list into the two discrete state vectors.\n        # ytm is the \"memory state\", stm is the \"carry state\".\n        ytm, stm = states\n        # We will use the \"carry state\" to guide the attention mechanism. Repeat it across all\n        # input timesteps to perform some calculations on it.\n        stm_repeated = K.repeat(self.dense_state(stm), self.timesteps)\n        # Now apply our \"dense_transform\" operation on the sum of our transformed \"carry state\" \n        # and all encoder states. This will squash the resultant sum down to a vector of size\n        # [batch,timesteps,1]\n        # Note: Most sources I encounter use tanh for the activation here. I have found with this dataset\n        # and this model, relu seems to perform better. It makes the attention mechanism far more crisp\n        # and produces better translation performance, especially with respect to proper sentence termination.\n        combined_stm_input = self.dense_transform(\n            keras.activations.relu(stm_repeated + self.input_seq_shaped))\n        # Performing a softmax generates a log probability for each encoder output to receive attention.\n        score_vector = keras.activations.softmax(combined_stm_input, 1)\n        # In this implementation, we grant \"partial attention\" to each encoder output based on \n        # it's log probability accumulated above. Other options would be to only give attention\n        # to the highest probability encoder output or some similar set.\n        context_vector = K.sum(score_vector * self.input_seq, 1)\n        \n        # Finally, mutate the input vector. It will now contain the traditional inputs (like the seq2seq\n        # we trained above) in addition to the attention context vector we calculated earlier in this method.\n        inputs = K.concatenate([inputs, context_vector])\n        \n        # Call into the super-class to invoke the LSTM math.\n        res = super(AttentionLSTMCell, self).call(inputs=inputs, states=states)\n        \n        # This if statement switches the return value of this method if \"attentionMode\" is turned on.\n        if(self.attentionMode):\n            return (K.reshape(score_vector, (-1, self.timesteps)), res[1])\n        else:\n            return res\n\n# Custom implementation of the Keras LSTM that adds an attention mechanism.\n# This is implemented by taking an additional input (using the \"constants\" of the\n# RNN class) into the LSTM: The encoder output vectors across the entire input sequence.\nclass LSTMWithAttention(RNN):\n    def __init__(self, units, **kwargs):\n        cell = AttentionLSTMCell(units=units)\n        self.units = units\n        super(LSTMWithAttention, self).__init__(cell, **kwargs)\n        \n    @tf_utils.shape_type_conversion\n    def build(self, input_shape):\n        self.input_dim = input_shape[0][-1]\n        self.timesteps = input_shape[0][-2]\n        return super(LSTMWithAttention, self).build(input_shape) \n    \n    # This call is invoked with the entire time sequence. The RNN sub-class is responsible\n    # for breaking this up into calls into the cell for each step.\n    # The \"constants\" variable is the key to our implementation. It was specifically added\n    # to Keras to accomodate the \"attention\" mechanism we are implementing.\n    def call(self, x, constants, **kwargs):\n        if isinstance(x, list):\n            self.x_initial = x[0]\n        else:\n            self.x_initial = x\n        \n        # The only difference in the LSTM computational graph really comes from the custom\n        # LSTM Cell that we utilize.\n        self.cell._dropout_mask = None\n        self.cell._recurrent_dropout_mask = None\n        self.cell.setInputSequence(constants[0])\n        return super(LSTMWithAttention, self).call(inputs=x, constants=constants, **kwargs)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# units = 512\n# def create_model():\n#     input_layer = Input(shape=(max_sentence,), dtype='int32')\n\n#     embedding_layer = Embedding(vocabulary_size,\n#                                 embed_dim,\n#                                 weights=[embedding_matrix],\n#                                 input_length=max_sentence,\n#                                 trainable=True)(input_layer)\n#     attenc_outputs, attstate_h, attstate_c = CuDNNLSTM(units=units, return_sequences=True, return_state=True)(embedding_layer)\n#     attenc_states = [attstate_h, attstate_c]\n    \n#     attdec_inputs = Input(shape=(None,))\n#     attdec_emb = Embedding(vocabulary_size,\n#                                 embed_dim,\n#                                 weights=[embedding_matrix],\n#                                 input_length=max_sentence,\n#                                 trainable=True)(attdec_inputs)\n#     attdec_lstm = LSTMWithAttention(units=units, return_sequences=True, return_state=True)\n#     # Note that the only real difference here is that we are feeding attenc_outputs to the decoder now.\n#     attdec_lstm_out, _, _ = attdec_lstm(inputs=attdec_emb, \n#                                         constants=attenc_outputs, \n#                                         initial_state=attenc_states)\n    \n    \n#     attdec_d1 = Dense(128, activation=\"relu\")\n#     attdec_d2 = Dense(4, activation=\"softmax\")\n#     attdec_out = attdec_d2(Dropout(rate=.4)(attdec_d1(Dropout(rate=.4)(attdec_lstm_out))))\n    \n    \n#     attmodel = Model(input_layer, attdec_out)\n#     attmodel.compile(optimizer=tf.train.AdamOptimizer(), loss=\"sparse_categorical_crossentropy\", metrics=['sparse_categorical_accuracy'])\n    \n#     return attmodel\n    \n#     bi_lstm = Bidirectional(LSTM(256))(embedding_layer)\n#     dropout = Dropout(0.4)(bi_lstm)\n#     dense = Dense(128,activation='relu')(dropout)\n#     dropout = Dropout(0.2)(dense)\n#     dense = Dense(64,activation='relu')(dropout)\n#     dropout = Dropout(0.2)(dense)\n#     dense = Dense(4,activation='softmax')(dropout)\n#     adam = optimizers.Adam(lr=0.01)\n#     rmsprop = optimizers.RMSprop(lr=0.005)#, rho=0.9, epsilon=None, decay=0.0)\n#     sgd = optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n\n#     model = Model(inputs=[input_layer], outputs=dense)\n#     model.compile(loss = 'binary_crossentropy', optimizer=rmsprop, metrics = ['accuracy'])\n#     return model","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### [Attention 3d block](https://github.com/philipperemy/keras-attention-mechanism/blob/master/attention_lstm.py)"},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\ndef get_activations(model, inputs, print_shape_only=False, layer_name=None):\n    # Documentation is available online on Github at the address below.\n    # From: https://github.com/philipperemy/keras-visualize-activations\n    print('----- activations -----')\n    activations = []\n    inp = model.input\n    if layer_name is None:\n        outputs = [layer.output for layer in model.layers]\n    else:\n        outputs = [layer.output for layer in model.layers if layer.name == layer_name]  # all layer outputs\n    funcs = [K.function([inp] + [K.learning_phase()], [out]) for out in outputs]  # evaluation functions\n    layer_outputs = [func([inputs, 1.])[0] for func in funcs]\n    for layer_activations in layer_outputs:\n        activations.append(layer_activations)\n        if print_shape_only:\n            print(layer_activations.shape)\n        else:\n            print(layer_activations)\n    return activations\n\n\ndef get_data(n, input_dim, attention_column=1):\n    \"\"\"\n    Data generation. x is purely random except that it's first value equals the target y.\n    In practice, the network should learn that the target = x[attention_column].\n    Therefore, most of its attention should be focused on the value addressed by attention_column.\n    :param n: the number of samples to retrieve.\n    :param input_dim: the number of dimensions of each element in the series.\n    :param attention_column: the column linked to the target. Everything else is purely random.\n    :return: x: model inputs, y: model targets\n    \"\"\"\n    x = np.random.standard_normal(size=(n, input_dim))\n    y = np.random.randint(low=0, high=2, size=(n, 1))\n    x[:, attention_column] = y[:, 0]\n    return x, y\n\n\ndef get_data_recurrent(n, time_steps, input_dim, attention_column=10):\n    \"\"\"\n    Data generation. x is purely random except that it's first value equals the target y.\n    In practice, the network should learn that the target = x[attention_column].\n    Therefore, most of its attention should be focused on the value addressed by attention_column.\n    :param n: the number of samples to retrieve.\n    :param time_steps: the number of time steps of your series.\n    :param input_dim: the number of dimensions of each element in the series.\n    :param attention_column: the column linked to the target. Everything else is purely random.\n    :return: x: model inputs, y: model targets\n    \"\"\"\n    x = np.random.standard_normal(size=(n, time_steps, input_dim))\n    y = np.random.randint(low=0, high=2, size=(n, 1))\n    x[:, attention_column, :] = np.tile(y[:], (1, input_dim))\n    return x, y","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"INPUT_DIM = 2\nTIME_STEPS = max_sentence\n# if True, the attention vector is shared across the input_dimensions where the attention is applied.\nSINGLE_ATTENTION_VECTOR = True\ndef attention_3d_block(inputs):\n    # inputs.shape = (batch_size, time_steps, input_dim)\n    input_dim = int(inputs.shape[2])\n    a = Permute((2, 1))(inputs)\n    a = Dense(TIME_STEPS, activation='softmax')(a)\n    if SINGLE_ATTENTION_VECTOR:\n        a = Lambda(lambda x: K.mean(x, axis=1), name='dim_reduction')(a)\n        a = RepeatVector(input_dim)(a)\n    a_probs = Permute((2, 1), name='attention_vec')(a)\n    output_attention_mul = multiply([inputs, a_probs], name='attention_mul')\n    return output_attention_mul","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lstm_units = 1024\ndef create_model():\n    input_layer = Input(shape=(max_sentence,), dtype='int32')\n    embedding_layer = Embedding(vocabulary_size,\n                                embed_dim,\n                                weights=[embedding_matrix],\n                                input_length=max_sentence,\n                                trainable=True)(input_layer)  \n    \n#     attention_mul = attention_3d_block(embedding_layer)\n#     bi_lstm = Bidirectional(LSTM(256,  return_sequences=False))(embedding_layer)\n    \n    lstm_out = LSTM(lstm_units, return_sequences=True)(embedding_layer)\n    attention_mul = attention_3d_block(lstm_out)\n    attention_mul = Flatten()(attention_mul)\n\n\n    \n#     dropout = Dropout(0.3)(attention_mul)\n\n#     dropout = Dropout(0.3)(attention_mul)\n    dense = Dense(256,activation='relu')(attention_mul)\n    dropout = Dropout(0.2)(dense)\n    dense = Dense(128,activation='relu')(dropout)\n    dropout = Dropout(0.2)(dense)\n    dense = Dense(4, activation='softmax')(dropout)\n\n\n\n    \n#     dense = Dense(4,activation='softmax')(dropout)\n    adam = optimizers.Adam(lr=0.01)\n    rmsprop = optimizers.RMSprop(lr=0.005)#, rho=0.9, epsilon=None, decay=0.0)\n#     sgd = optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n\n    model = Model(inputs=input_layer, outputs=dense)\n    model.compile(loss = 'binary_crossentropy', optimizer=rmsprop, metrics = ['accuracy'])\n    return model  \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = create_model()\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"import keras\nimport IPython\n# SVG(model_to_dot(model).create(prog='dot', format='svg'))\nkeras.utils.plot_model(model, to_file='test_keras_plot_model.png', show_shapes=True,show_layer_names=False)\nIPython.display.Image('test_keras_plot_model.png')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.fit(X_train, Y_train, epochs=50, verbose=1, batch_size=189,\n          validation_data=(X_dev, Y_dev),\n          callbacks=[Metrics(X_test, Y_test, 3),\n                    ])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Attention tanh - [link](https://github.com/keras-team/keras/issues/4962)0.7044"},{"metadata":{},"cell_type":"markdown","source":"* * #############\n* F1 all\n* 0.9045198765656198\n* ***\n* F1 angry: \n* 0.6973500697350068\n* ****\n* F1 happy: \n* 0.6750788643533122\n* ****\n* F1 others: \n* 0.9455380577427821\n* ****\n* F1 sad: \n* 0.7495219885277247\n* ****\n* F1 happy angry sad\n* 0.7043756670224119"},{"metadata":{"trusted":true},"cell_type":"code","source":"units = 512\ndef create_model():\n    input_layer = Input(shape=(max_sentence,), dtype='int32')\n    embedding_layer = Embedding(vocabulary_size,\n                                embed_dim,\n                                weights=[embedding_matrix],\n                                input_length=max_sentence,\n                                trainable=False)(input_layer)  \n#     bi_lstm = Bidirectional(LSTM(256))(embedding_layer)\n    \n    \n    activations = LSTM(units, return_sequences=True)(embedding_layer)\n    \n    attention = TimeDistributed(Dense(1, activation='tanh'))(activations) \n    attention = Dense(1, activation='tanh')(activations) \n    attention = Flatten()(attention)\n    attention = Activation('softmax')(attention)\n    attention = RepeatVector(units)(attention)\n    attention = Permute([2, 1])(attention)\n\n    # apply the attention\n#     sent_representation = multiply([activations, attention])\n#     sent_representation = Lambda(lambda xin: K.sum(xin, axis=0), output_shape=(units, ))(sent_representation)\n#     sent_representation = Flatten()(sent_representation)\n\n\n\n#     sent_representation = multiply([activations, attention])\n    sent_representation = Lambda(lambda xin: K.sum(xin, axis=0), output_shape=(units, ))(attention)\n\n\n    dense = Dense(4, activation='softmax')(sent_representation)\n\n\n\n    \n#     dropout = Dropout(0.4)(bi_lstm)\n#     dense = Dense(128,activation='relu')(dropout)\n#     dropout = Dropout(0.2)(dense)\n#     dense = Dense(64,activation='relu')(dropout)\n#     dropout = Dropout(0.2)(dense)\n#     dense = Dense(4,activation='softmax')(dropout)\n    \n    adam = optimizers.Adam(lr=0.01)\n    rmsprop = optimizers.RMSprop(lr=0.005)#, rho=0.9, epsilon=None, decay=0.0)\n    sgd = optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n\n    model = Model(inputs=input_layer, outputs=dense)\n    model.compile(loss = 'binary_crossentropy', optimizer=rmsprop, metrics = ['accuracy'])\n    return model  \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = create_model()\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import keras\nimport IPython\n# SVG(model_to_dot(model).create(prog='dot', format='svg'))\nkeras.utils.plot_model(model, to_file='test_keras_plot_model.png', show_shapes=True,show_layer_names=False)\nIPython.display.Image('test_keras_plot_model.png')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import keras\nimport IPython\n# SVG(model_to_dot(model).create(prog='dot', format='svg'))\nkeras.utils.plot_model(model, to_file='test_keras_plot_model.png', show_shapes=True,show_layer_names=False)\nIPython.display.Image('test_keras_plot_model.png') # deep lstm","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import keras\nimport IPython\n# SVG(model_to_dot(model).create(prog='dot', format='svg'))\nkeras.utils.plot_model(model, to_file='test_keras_plot_model.png', show_shapes=True,show_layer_names=False)\nIPython.display.Image('test_keras_plot_model.png')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Deep bilstm diagram"},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"import keras\nimport IPython\n# SVG(model_to_dot(model).create(prog='dot', format='svg'))\nkeras.utils.plot_model(model, to_file='test_keras_plot_model.png', show_shapes=True,show_layer_names=False)\nIPython.display.Image('test_keras_plot_model.png') # deep bilstm","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.fit(X_train, Y_train, epochs=50, verbose=1, batch_size=189,\n          validation_data=(X_dev, Y_dev),\n          callbacks=[Metrics(X_test, Y_test, 3),\n                    ])# att + deep lstm ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"import keras\nimport IPython\n# SVG(model_to_dot(model).create(prog='dot', format='svg'))\nkeras.utils.plot_model(model, to_file='test_keras_plot_model.png', show_shapes=True,show_layer_names=False)\nIPython.display.Image('test_keras_plot_model.png') # deep bilstm + single vector","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.fit(X_train, Y_train, epochs=50, verbose=1, batch_size=189,\n          validation_data=(X_dev, Y_dev),\n          callbacks=[Metrics(X_test, Y_test, 3),\n                    ])# deep bilstm + single vector+ rmsprop","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.fit(X_train, Y_train, epochs=50, verbose=1, batch_size=189,\n          validation_data=(X_dev, Y_dev),\n          callbacks=[Metrics(X_test, Y_test, 3),\n                    ])# deep bilstm + single vector+ adam","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.fit(X_train, Y_train, epochs=50, verbose=1, batch_size=189,\n          validation_data=(X_dev, Y_dev),\n          callbacks=[Metrics(X_test, Y_test, 3),\n                    ])# att+deep bilstm","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.fit(X_train, Y_train, epochs=50, verbose=1, batch_size=189,\n          validation_data=(X_dev, Y_dev),\n          callbacks=[Metrics(X_test, Y_test, 3),\n                    ])# deep lstm","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.fit(X_train, Y_train, epochs=50, verbose=1, batch_size=189,\n          validation_data=(X_dev, Y_dev),\n          callbacks=[Metrics(X_test, Y_test, 3),\n                    ])# adam + categorical","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.fit(X_train, Y_train, epochs=50, verbose=1, batch_size=189,\n          validation_data=(X_dev, Y_dev),\n          callbacks=[Metrics(X_test, Y_test, 3),\n                    ])# adam + binary","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.fit(X_train, Y_train, epochs=50, verbose=1, batch_size=189,\n          validation_data=(X_dev, Y_dev),\n          callbacks=[Metrics(X_test, Y_test, 3),\n                    ]) #rmsprop","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}