{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata: ...working... done\n",
      "Solving environment: ...working... done\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: F:\\Anaconda3\n",
      "\n",
      "  added / updated specs:\n",
      "    - tqdm\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    ca-certificates-2019.1.23  |                0         158 KB\n",
      "    certifi-2019.3.9           |           py36_0         156 KB\n",
      "    cryptography-2.6.1         |   py36h7a1dbc1_0         563 KB\n",
      "    kiwisolver-1.0.1           |   py36h6538335_0          61 KB\n",
      "    krb5-1.16.1                |       hc04afaa_7         819 KB\n",
      "    libcurl-7.64.0             |       h2a8f88b_2         283 KB\n",
      "    libpng-1.6.36              |       h2a8f88b_0         550 KB\n",
      "    openssl-1.1.1b             |       he774522_1         5.7 MB\n",
      "    pycurl-7.43.0.2            |   py36h7a1dbc1_0         182 KB\n",
      "    pyqt-5.9.2                 |   py36h6538335_2         4.2 MB\n",
      "    qt-5.9.7                   |   vc14h73c81de_0        92.3 MB\n",
      "    sqlite-3.27.2              |       he774522_0         941 KB\n",
      "    tk-8.6.8                   |       hfa6e2cd_0         3.8 MB\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:       109.6 MB\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "  kiwisolver         pkgs/main/win-64::kiwisolver-1.0.1-py36h6538335_0\n",
      "  krb5               pkgs/main/win-64::krb5-1.16.1-hc04afaa_7\n",
      "  libcurl            pkgs/main/win-64::libcurl-7.64.0-h2a8f88b_2\n",
      "  tqdm               pkgs/main/noarch::tqdm-4.31.1-py_0\n",
      "\n",
      "The following packages will be UPDATED:\n",
      "\n",
      "  ca-certificates                     2017.08.26-h94faf87_0 --> 2019.1.23-0\n",
      "  certifi                        2017.7.27.1-py36h043bc9e_0 --> 2019.3.9-py36_0\n",
      "  cryptography                         2.0.3-py36h123decb_1 --> 2.6.1-py36h7a1dbc1_0\n",
      "  libpng                              1.6.32-vc14h5163883_3 --> 1.6.36-h2a8f88b_0\n",
      "  matplotlib                           2.1.0-py36h11b4b9c_0 --> 2.2.2-py36h153e9ff_1\n",
      "  openssl                             1.0.2l-vc14hcac20b0_2 --> 1.1.1b-he774522_1\n",
      "  pycurl                              7.43.0-py36h086bf4c_3 --> 7.43.0.2-py36h7a1dbc1_0\n",
      "  pyqt                                 5.6.0-py36hb5ed885_5 --> 5.9.2-py36h6538335_2\n",
      "  qt                                  5.6.2-vc14h6f8c307_12 --> 5.9.7-vc14h73c81de_0\n",
      "  sip                                 4.18.1-py36h9c25514_2 --> 4.19.8-py36h6538335_0\n",
      "  sqlite                              3.20.1-vc14h7ce8c62_1 --> 3.27.2-he774522_0\n",
      "  tk                                   8.6.7-vc14hb68737d_1 --> 8.6.8-hfa6e2cd_0\n",
      "\n",
      "\n",
      "\n",
      "Downloading and Extracting Packages\n",
      "\n",
      "ca-certificates-2019 | 158 KB    |            |   0% \n",
      "ca-certificates-2019 | 158 KB    | ########## | 100% \n",
      "\n",
      "krb5-1.16.1          | 819 KB    |            |   0% \n",
      "krb5-1.16.1          | 819 KB    | ######5    |  66% \n",
      "krb5-1.16.1          | 819 KB    | #########9 |  99% \n",
      "krb5-1.16.1          | 819 KB    | ########## | 100% \n",
      "\n",
      "libcurl-7.64.0       | 283 KB    |            |   0% \n",
      "libcurl-7.64.0       | 283 KB    | ########## | 100% \n",
      "\n",
      "kiwisolver-1.0.1     | 61 KB     |            |   0% \n",
      "kiwisolver-1.0.1     | 61 KB     | ########## | 100% \n",
      "\n",
      "pyqt-5.9.2           | 4.2 MB    |            |   0% \n",
      "pyqt-5.9.2           | 4.2 MB    | #2         |  13% \n",
      "pyqt-5.9.2           | 4.2 MB    | ##8        |  28% \n",
      "pyqt-5.9.2           | 4.2 MB    | ####2      |  42% \n",
      "pyqt-5.9.2           | 4.2 MB    | #####9     |  59% \n",
      "pyqt-5.9.2           | 4.2 MB    | #######5   |  75% \n",
      "pyqt-5.9.2           | 4.2 MB    | ########7  |  88% \n",
      "pyqt-5.9.2           | 4.2 MB    | #########7 |  98% \n",
      "pyqt-5.9.2           | 4.2 MB    | ########## | 100% \n",
      "\n",
      "libpng-1.6.36        | 550 KB    |            |   0% \n",
      "libpng-1.6.36        | 550 KB    | #######9   |  79% \n",
      "libpng-1.6.36        | 550 KB    | ########## | 100% \n",
      "\n",
      "openssl-1.1.1b       | 5.7 MB    |            |   0% \n",
      "openssl-1.1.1b       | 5.7 MB    | 8          |   8% \n",
      "openssl-1.1.1b       | 5.7 MB    | #7         |  18% \n",
      "openssl-1.1.1b       | 5.7 MB    | ##9        |  30% \n",
      "openssl-1.1.1b       | 5.7 MB    | ####1      |  41% \n",
      "openssl-1.1.1b       | 5.7 MB    | #####      |  50% \n",
      "openssl-1.1.1b       | 5.7 MB    | ######2    |  62% \n",
      "openssl-1.1.1b       | 5.7 MB    | #######2   |  72% \n",
      "openssl-1.1.1b       | 5.7 MB    | ########1  |  82% \n",
      "openssl-1.1.1b       | 5.7 MB    | ########9  |  90% \n",
      "openssl-1.1.1b       | 5.7 MB    | #########6 |  96% \n",
      "openssl-1.1.1b       | 5.7 MB    | ########## | 100% \n",
      "\n",
      "certifi-2019.3.9     | 156 KB    |            |   0% \n",
      "certifi-2019.3.9     | 156 KB    | ########## | 100% \n",
      "\n",
      "cryptography-2.6.1   | 563 KB    |            |   0% \n",
      "cryptography-2.6.1   | 563 KB    | #9         |  19% \n",
      "cryptography-2.6.1   | 563 KB    | ########4  |  85% \n",
      "cryptography-2.6.1   | 563 KB    | ########## | 100% \n",
      "\n",
      "sqlite-3.27.2        | 941 KB    |            |   0% \n",
      "sqlite-3.27.2        | 941 KB    | ####5      |  46% \n",
      "sqlite-3.27.2        | 941 KB    | ########8  |  89% \n",
      "sqlite-3.27.2        | 941 KB    | ########## | 100% \n",
      "\n",
      "tk-8.6.8             | 3.8 MB    |            |   0% \n",
      "tk-8.6.8             | 3.8 MB    | 8          |   9% \n",
      "tk-8.6.8             | 3.8 MB    | ##6        |  27% \n",
      "tk-8.6.8             | 3.8 MB    | ####5      |  45% \n",
      "tk-8.6.8             | 3.8 MB    | #####9     |  60% \n",
      "tk-8.6.8             | 3.8 MB    | #######3   |  74% \n",
      "tk-8.6.8             | 3.8 MB    | ########5  |  86% \n",
      "tk-8.6.8             | 3.8 MB    | #########5 |  96% \n",
      "tk-8.6.8             | 3.8 MB    | ########## | 100% \n",
      "\n",
      "pycurl-7.43.0.2      | 182 KB    |            |   0% \n",
      "pycurl-7.43.0.2      | 182 KB    | ########## | 100% \n",
      "\n",
      "qt-5.9.7             | 92.3 MB   |            |   0% \n",
      "qt-5.9.7             | 92.3 MB   |            |   0% \n",
      "qt-5.9.7             | 92.3 MB   | 1          |   1% \n",
      "qt-5.9.7             | 92.3 MB   | 1          |   2% \n",
      "qt-5.9.7             | 92.3 MB   | 2          |   3% \n",
      "qt-5.9.7             | 92.3 MB   | 3          |   4% \n",
      "qt-5.9.7             | 92.3 MB   | 4          |   4% \n",
      "qt-5.9.7             | 92.3 MB   | 5          |   5% \n",
      "qt-5.9.7             | 92.3 MB   | 5          |   6% \n",
      "qt-5.9.7             | 92.3 MB   | 6          |   7% \n",
      "qt-5.9.7             | 92.3 MB   | 7          |   7% \n",
      "qt-5.9.7             | 92.3 MB   | 8          |   8% \n",
      "qt-5.9.7             | 92.3 MB   | 8          |   9% \n",
      "qt-5.9.7             | 92.3 MB   | 9          |  10% \n",
      "qt-5.9.7             | 92.3 MB   | #          |  11% \n",
      "qt-5.9.7             | 92.3 MB   | #1         |  11% \n",
      "qt-5.9.7             | 92.3 MB   | #2         |  12% \n",
      "qt-5.9.7             | 92.3 MB   | #3         |  13% \n",
      "qt-5.9.7             | 92.3 MB   | #3         |  14% \n",
      "qt-5.9.7             | 92.3 MB   | #4         |  15% \n",
      "qt-5.9.7             | 92.3 MB   | #5         |  16% \n",
      "qt-5.9.7             | 92.3 MB   | #6         |  16% \n",
      "qt-5.9.7             | 92.3 MB   | #7         |  17% \n",
      "qt-5.9.7             | 92.3 MB   | #8         |  18% \n",
      "qt-5.9.7             | 92.3 MB   | #8         |  19% \n",
      "qt-5.9.7             | 92.3 MB   | #9         |  20% \n",
      "qt-5.9.7             | 92.3 MB   | ##         |  20% \n",
      "qt-5.9.7             | 92.3 MB   | ##1        |  21% \n",
      "qt-5.9.7             | 92.3 MB   | ##1        |  22% \n",
      "qt-5.9.7             | 92.3 MB   | ##2        |  23% \n",
      "qt-5.9.7             | 92.3 MB   | ##3        |  24% \n",
      "qt-5.9.7             | 92.3 MB   | ##4        |  25% \n",
      "qt-5.9.7             | 92.3 MB   | ##5        |  26% \n",
      "qt-5.9.7             | 92.3 MB   | ##6        |  27% \n",
      "qt-5.9.7             | 92.3 MB   | ##7        |  27% \n",
      "qt-5.9.7             | 92.3 MB   | ##8        |  28% \n",
      "qt-5.9.7             | 92.3 MB   | ##9        |  29% \n",
      "qt-5.9.7             | 92.3 MB   | ###        |  30% \n",
      "qt-5.9.7             | 92.3 MB   | ###        |  31% \n",
      "qt-5.9.7             | 92.3 MB   | ###1       |  32% \n",
      "qt-5.9.7             | 92.3 MB   | ###2       |  32% \n",
      "qt-5.9.7             | 92.3 MB   | ###3       |  33% \n",
      "qt-5.9.7             | 92.3 MB   | ###3       |  34% \n",
      "qt-5.9.7             | 92.3 MB   | ###4       |  35% \n",
      "qt-5.9.7             | 92.3 MB   | ###5       |  35% \n",
      "qt-5.9.7             | 92.3 MB   | ###6       |  36% \n",
      "qt-5.9.7             | 92.3 MB   | ###6       |  37% \n",
      "qt-5.9.7             | 92.3 MB   | ###7       |  38% \n",
      "qt-5.9.7             | 92.3 MB   | ###8       |  38% \n",
      "qt-5.9.7             | 92.3 MB   | ###9       |  39% \n",
      "qt-5.9.7             | 92.3 MB   | ####       |  40% \n",
      "qt-5.9.7             | 92.3 MB   | ####       |  41% \n",
      "qt-5.9.7             | 92.3 MB   | ####1      |  42% \n",
      "qt-5.9.7             | 92.3 MB   | ####2      |  43% \n",
      "qt-5.9.7             | 92.3 MB   | ####3      |  44% \n",
      "qt-5.9.7             | 92.3 MB   | ####4      |  45% \n",
      "qt-5.9.7             | 92.3 MB   | ####5      |  45% \n",
      "qt-5.9.7             | 92.3 MB   | ####6      |  46% \n",
      "qt-5.9.7             | 92.3 MB   | ####7      |  47% \n",
      "qt-5.9.7             | 92.3 MB   | ####8      |  48% \n",
      "qt-5.9.7             | 92.3 MB   | ####8      |  49% \n",
      "qt-5.9.7             | 92.3 MB   | ####9      |  50% \n",
      "qt-5.9.7             | 92.3 MB   | #####      |  50% \n",
      "qt-5.9.7             | 92.3 MB   | #####1     |  51% \n",
      "qt-5.9.7             | 92.3 MB   | #####2     |  52% \n",
      "qt-5.9.7             | 92.3 MB   | #####2     |  53% \n",
      "qt-5.9.7             | 92.3 MB   | #####3     |  54% \n",
      "qt-5.9.7             | 92.3 MB   | #####4     |  55% \n",
      "qt-5.9.7             | 92.3 MB   | #####5     |  55% \n",
      "qt-5.9.7             | 92.3 MB   | #####6     |  56% \n",
      "qt-5.9.7             | 92.3 MB   | #####7     |  57% \n",
      "qt-5.9.7             | 92.3 MB   | #####8     |  58% \n",
      "qt-5.9.7             | 92.3 MB   | #####8     |  59% \n",
      "qt-5.9.7             | 92.3 MB   | #####9     |  60% \n",
      "qt-5.9.7             | 92.3 MB   | ######     |  61% \n",
      "qt-5.9.7             | 92.3 MB   | ######1    |  61% \n",
      "qt-5.9.7             | 92.3 MB   | ######2    |  62% \n",
      "qt-5.9.7             | 92.3 MB   | ######2    |  63% \n",
      "qt-5.9.7             | 92.3 MB   | ######3    |  64% \n",
      "qt-5.9.7             | 92.3 MB   | ######4    |  65% \n",
      "qt-5.9.7             | 92.3 MB   | ######5    |  66% \n",
      "qt-5.9.7             | 92.3 MB   | ######6    |  66% \n",
      "qt-5.9.7             | 92.3 MB   | ######7    |  67% \n",
      "qt-5.9.7             | 92.3 MB   | ######8    |  68% \n",
      "qt-5.9.7             | 92.3 MB   | ######8    |  69% \n",
      "qt-5.9.7             | 92.3 MB   | ######9    |  70% \n",
      "qt-5.9.7             | 92.3 MB   | #######    |  71% \n",
      "qt-5.9.7             | 92.3 MB   | #######1   |  71% \n",
      "qt-5.9.7             | 92.3 MB   | #######2   |  72% \n",
      "qt-5.9.7             | 92.3 MB   | #######2   |  73% \n",
      "qt-5.9.7             | 92.3 MB   | #######3   |  74% \n",
      "qt-5.9.7             | 92.3 MB   | #######4   |  74% \n",
      "qt-5.9.7             | 92.3 MB   | #######5   |  75% \n",
      "qt-5.9.7             | 92.3 MB   | #######5   |  76% \n",
      "qt-5.9.7             | 92.3 MB   | #######6   |  76% \n",
      "qt-5.9.7             | 92.3 MB   | #######6   |  77% \n",
      "qt-5.9.7             | 92.3 MB   | #######6   |  77% \n",
      "qt-5.9.7             | 92.3 MB   | #######7   |  77% \n",
      "qt-5.9.7             | 92.3 MB   | #######7   |  77% \n",
      "qt-5.9.7             | 92.3 MB   | #######7   |  78% \n",
      "qt-5.9.7             | 92.3 MB   | #######7   |  78% \n",
      "qt-5.9.7             | 92.3 MB   | #######8   |  78% \n",
      "qt-5.9.7             | 92.3 MB   | #######8   |  78% \n",
      "qt-5.9.7             | 92.3 MB   | #######8   |  79% \n",
      "qt-5.9.7             | 92.3 MB   | #######9   |  79% \n",
      "qt-5.9.7             | 92.3 MB   | #######9   |  79% \n",
      "qt-5.9.7             | 92.3 MB   | #######9   |  80% \n",
      "qt-5.9.7             | 92.3 MB   | #######9   |  80% \n",
      "qt-5.9.7             | 92.3 MB   | ########   |  80% \n",
      "qt-5.9.7             | 92.3 MB   | ########   |  81% \n",
      "qt-5.9.7             | 92.3 MB   | ########   |  81% \n",
      "qt-5.9.7             | 92.3 MB   | ########1  |  81% \n",
      "qt-5.9.7             | 92.3 MB   | ########1  |  81% \n",
      "qt-5.9.7             | 92.3 MB   | ########1  |  82% \n",
      "qt-5.9.7             | 92.3 MB   | ########1  |  82% \n",
      "qt-5.9.7             | 92.3 MB   | ########2  |  82% \n",
      "qt-5.9.7             | 92.3 MB   | ########2  |  83% \n",
      "qt-5.9.7             | 92.3 MB   | ########2  |  83% \n",
      "qt-5.9.7             | 92.3 MB   | ########3  |  83% \n",
      "qt-5.9.7             | 92.3 MB   | ########3  |  84% \n",
      "qt-5.9.7             | 92.3 MB   | ########3  |  84% \n",
      "qt-5.9.7             | 92.3 MB   | ########4  |  84% \n",
      "qt-5.9.7             | 92.3 MB   | ########4  |  85% \n",
      "qt-5.9.7             | 92.3 MB   | ########4  |  85% \n",
      "qt-5.9.7             | 92.3 MB   | ########5  |  85% \n",
      "qt-5.9.7             | 92.3 MB   | ########5  |  85% \n",
      "qt-5.9.7             | 92.3 MB   | ########5  |  86% \n",
      "qt-5.9.7             | 92.3 MB   | ########5  |  86% \n",
      "qt-5.9.7             | 92.3 MB   | ########6  |  86% \n",
      "qt-5.9.7             | 92.3 MB   | ########6  |  87% \n",
      "qt-5.9.7             | 92.3 MB   | ########6  |  87% \n",
      "qt-5.9.7             | 92.3 MB   | ########7  |  87% \n",
      "qt-5.9.7             | 92.3 MB   | ########7  |  87% \n",
      "qt-5.9.7             | 92.3 MB   | ########7  |  88% \n",
      "qt-5.9.7             | 92.3 MB   | ########7  |  88% \n",
      "qt-5.9.7             | 92.3 MB   | ########8  |  88% \n",
      "qt-5.9.7             | 92.3 MB   | ########8  |  89% \n",
      "qt-5.9.7             | 92.3 MB   | ########8  |  89% \n",
      "qt-5.9.7             | 92.3 MB   | ########9  |  89% \n",
      "qt-5.9.7             | 92.3 MB   | ########9  |  89% \n",
      "qt-5.9.7             | 92.3 MB   | ########9  |  90% \n",
      "qt-5.9.7             | 92.3 MB   | ########9  |  90% \n",
      "qt-5.9.7             | 92.3 MB   | #########  |  90% \n",
      "qt-5.9.7             | 92.3 MB   | #########  |  91% \n",
      "qt-5.9.7             | 92.3 MB   | #########  |  91% \n",
      "qt-5.9.7             | 92.3 MB   | #########1 |  91% \n",
      "qt-5.9.7             | 92.3 MB   | #########1 |  91% \n",
      "qt-5.9.7             | 92.3 MB   | #########1 |  92% \n",
      "qt-5.9.7             | 92.3 MB   | #########1 |  92% \n",
      "qt-5.9.7             | 92.3 MB   | #########2 |  92% \n",
      "qt-5.9.7             | 92.3 MB   | #########2 |  92% \n",
      "qt-5.9.7             | 92.3 MB   | #########2 |  93% \n",
      "qt-5.9.7             | 92.3 MB   | #########3 |  93% \n",
      "qt-5.9.7             | 92.3 MB   | #########3 |  93% \n",
      "qt-5.9.7             | 92.3 MB   | #########3 |  94% \n",
      "qt-5.9.7             | 92.3 MB   | #########4 |  94% \n",
      "qt-5.9.7             | 92.3 MB   | #########4 |  94% \n",
      "qt-5.9.7             | 92.3 MB   | #########4 |  95% \n",
      "qt-5.9.7             | 92.3 MB   | #########4 |  95% \n",
      "qt-5.9.7             | 92.3 MB   | #########5 |  95% \n",
      "qt-5.9.7             | 92.3 MB   | #########5 |  95% \n",
      "qt-5.9.7             | 92.3 MB   | #########5 |  95% \n",
      "qt-5.9.7             | 92.3 MB   | #########5 |  96% \n",
      "qt-5.9.7             | 92.3 MB   | #########5 |  96% \n",
      "qt-5.9.7             | 92.3 MB   | #########6 |  96% \n",
      "qt-5.9.7             | 92.3 MB   | #########6 |  96% \n",
      "qt-5.9.7             | 92.3 MB   | #########6 |  97% \n",
      "qt-5.9.7             | 92.3 MB   | #########7 |  97% \n",
      "qt-5.9.7             | 92.3 MB   | #########7 |  97% \n",
      "qt-5.9.7             | 92.3 MB   | #########7 |  98% \n",
      "qt-5.9.7             | 92.3 MB   | #########7 |  98% \n",
      "qt-5.9.7             | 92.3 MB   | #########8 |  98% \n",
      "qt-5.9.7             | 92.3 MB   | #########8 |  99% \n",
      "qt-5.9.7             | 92.3 MB   | #########8 |  99% \n",
      "qt-5.9.7             | 92.3 MB   | #########9 |  99% \n",
      "qt-5.9.7             | 92.3 MB   | #########9 |  99% \n",
      "qt-5.9.7             | 92.3 MB   | #########9 | 100% \n",
      "qt-5.9.7             | 92.3 MB   | #########9 | 100% \n",
      "qt-5.9.7             | 92.3 MB   | #########9 | 100% \n",
      "qt-5.9.7             | 92.3 MB   | ########## | 100% \n",
      "Preparing transaction: ...working... done\n",
      "Verifying transaction: ...working... done\n",
      "Executing transaction: ...working... done\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata: ...working... done\n",
      "Solving environment: ...working... done\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# import sys\n",
    "# !conda install --yes --prefix {sys.prefix} tqdm\n",
    "# !conda install --yes --prefix {sys.prefix} tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'bar_length' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-ca29aa12029d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mprogress\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[0mprogress\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m \u001b[0mblock\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mround\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbar_length\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mprogress\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[0mclear_output\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwait\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"Progress: [{0}] {1:.1f}%\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m \u001b[1;34m\"#\"\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mblock\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\"-\"\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mbar_length\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mblock\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprogress\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'bar_length' is not defined"
     ]
    }
   ],
   "source": [
    "import time, sys\n",
    "from IPython.display import clear_output\n",
    "def update_progress(progress):\n",
    "    bar_length = 20\n",
    "    if isinstance(progress, int):\n",
    "        progress = float(progress)\n",
    "    if not isinstance(progress, float):\n",
    "        progress = 0\n",
    "    if progress < 0:\n",
    "        progress = 0\n",
    "    if progress >= 1:\n",
    "        progress = 1\n",
    "bar_length  = 50\n",
    "block = int(round(bar_length * progress))\n",
    "clear_output(wait = True)\n",
    "text = \"Progress: [{0}] {1:.1f}%\".format( \"#\" * block + \"-\" * (bar_length - block), progress * 100)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: tensorflow in f:\\anaconda3\\lib\\site-packages (1.13.1)\n",
      "Requirement already satisfied, skipping upgrade: protobuf>=3.6.1 in f:\\anaconda3\\lib\\site-packages (from tensorflow) (3.6.1)\n",
      "Requirement already satisfied, skipping upgrade: astor>=0.6.0 in f:\\anaconda3\\lib\\site-packages (from tensorflow) (0.7.1)\n",
      "Requirement already satisfied, skipping upgrade: keras-applications>=1.0.6 in f:\\anaconda3\\lib\\site-packages (from tensorflow) (1.0.7)\n",
      "Requirement already satisfied, skipping upgrade: grpcio>=1.8.6 in f:\\anaconda3\\lib\\site-packages (from tensorflow) (1.12.1)\n",
      "Requirement already satisfied, skipping upgrade: wheel>=0.26 in f:\\anaconda3\\lib\\site-packages (from tensorflow) (0.29.0)\n",
      "Requirement already satisfied, skipping upgrade: termcolor>=1.1.0 in f:\\anaconda3\\lib\\site-packages (from tensorflow) (1.1.0)\n",
      "Requirement already satisfied, skipping upgrade: tensorflow-estimator<1.14.0rc0,>=1.13.0 in f:\\anaconda3\\lib\\site-packages (from tensorflow) (1.13.0)\n",
      "Requirement already satisfied, skipping upgrade: keras-preprocessing>=1.0.5 in f:\\anaconda3\\lib\\site-packages (from tensorflow) (1.0.9)\n",
      "Requirement already satisfied, skipping upgrade: absl-py>=0.1.6 in f:\\anaconda3\\lib\\site-packages (from tensorflow) (0.7.0)\n",
      "Requirement already satisfied, skipping upgrade: gast>=0.2.0 in f:\\anaconda3\\lib\\site-packages (from tensorflow) (0.2.2)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.13.3 in f:\\anaconda3\\lib\\site-packages (from tensorflow) (1.13.3)\n",
      "Requirement already satisfied, skipping upgrade: six>=1.10.0 in f:\\anaconda3\\lib\\site-packages (from tensorflow) (1.12.0)\n",
      "Requirement already satisfied, skipping upgrade: tensorboard<1.14.0,>=1.13.0 in f:\\anaconda3\\lib\\site-packages (from tensorflow) (1.13.1)\n",
      "Requirement already satisfied, skipping upgrade: setuptools in f:\\anaconda3\\lib\\site-packages (from protobuf>=3.6.1->tensorflow) (36.5.0.post20170921)\n",
      "Requirement already satisfied, skipping upgrade: h5py in f:\\anaconda3\\lib\\site-packages (from keras-applications>=1.0.6->tensorflow) (2.7.0)\n",
      "Requirement already satisfied, skipping upgrade: markdown>=2.6.8 in f:\\anaconda3\\lib\\site-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow) (3.0.1)\n",
      "Requirement already satisfied, skipping upgrade: werkzeug>=0.11.15 in f:\\anaconda3\\lib\\site-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow) (0.12.2)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "# !{sys.executable} -m pip install --upgrade pip\n",
    "# !{sys.executable} -m pip install ipdb\n",
    "# !{sys.executable} -m pip install tensorflow_hub\n",
    "!{sys.executable} -m pip install --upgrade tensorflow "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow-gpu\n",
      "  Downloading https://files.pythonhosted.org/packages/6d/c4/d5b81c2d40be9219a23852f630df9e1ed88a22f88da774e1bf69261f9b32/tensorflow_gpu-1.13.1-cp36-cp36m-win_amd64.whl (259.7MB)\n",
      "Requirement already satisfied: astor>=0.6.0 in f:\\anaconda3\\lib\\site-packages (from tensorflow-gpu) (0.7.1)\n",
      "Requirement already satisfied: six>=1.10.0 in f:\\anaconda3\\lib\\site-packages (from tensorflow-gpu) (1.12.0)\n",
      "Requirement already satisfied: absl-py>=0.1.6 in f:\\anaconda3\\lib\\site-packages (from tensorflow-gpu) (0.7.0)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in f:\\anaconda3\\lib\\site-packages (from tensorflow-gpu) (1.12.1)\n",
      "Requirement already satisfied: tensorflow-estimator<1.14.0rc0,>=1.13.0 in f:\\anaconda3\\lib\\site-packages (from tensorflow-gpu) (1.13.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in f:\\anaconda3\\lib\\site-packages (from tensorflow-gpu) (1.1.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in f:\\anaconda3\\lib\\site-packages (from tensorflow-gpu) (1.0.9)\n",
      "Requirement already satisfied: numpy>=1.13.3 in f:\\anaconda3\\lib\\site-packages (from tensorflow-gpu) (1.13.3)\n",
      "Requirement already satisfied: wheel>=0.26 in f:\\anaconda3\\lib\\site-packages (from tensorflow-gpu) (0.29.0)\n",
      "Requirement already satisfied: protobuf>=3.6.1 in f:\\anaconda3\\lib\\site-packages (from tensorflow-gpu) (3.6.1)\n",
      "Requirement already satisfied: tensorboard<1.14.0,>=1.13.0 in f:\\anaconda3\\lib\\site-packages (from tensorflow-gpu) (1.13.1)\n",
      "Requirement already satisfied: keras-applications>=1.0.6 in f:\\anaconda3\\lib\\site-packages (from tensorflow-gpu) (1.0.7)\n",
      "Requirement already satisfied: gast>=0.2.0 in f:\\anaconda3\\lib\\site-packages (from tensorflow-gpu) (0.2.2)\n",
      "Requirement already satisfied: setuptools in f:\\anaconda3\\lib\\site-packages (from protobuf>=3.6.1->tensorflow-gpu) (36.5.0.post20170921)\n",
      "Requirement already satisfied: markdown>=2.6.8 in f:\\anaconda3\\lib\\site-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow-gpu) (3.0.1)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in f:\\anaconda3\\lib\\site-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow-gpu) (0.12.2)\n",
      "Requirement already satisfied: h5py in f:\\anaconda3\\lib\\site-packages (from keras-applications>=1.0.6->tensorflow-gpu) (2.7.0)\n",
      "Installing collected packages: tensorflow-gpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not install packages due to an EnvironmentError: [WinError 5] Access is denied: 'F:\\\\Anaconda3\\\\Lib\\\\site-packages\\\\tensorflow\\\\python\\\\_pywrap_tensorflow_internal.pyd'\n",
      "Consider using the `--user` option or check the permissions.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!{sys.executable} -m pip install tensorflow-gpu "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import urllib\n",
    "import sys\n",
    "import os\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download Glove word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "glove_zip_file = \"data/glove.6B.zip\"\n",
    "glove_vectors_file = \"data/glove.6B.50d.txt\"\n",
    "import zipfile, urllib.request, shutil, os\n",
    "    \n",
    "#large file - 862 MB\n",
    "if (not os.path.isfile(glove_zip_file) and\n",
    "    not os.path.isfile(glove_vectors_file)):\n",
    "    with urllib.request.urlopen(\"http://nlp.stanford.edu/data/glove.6B.zip\") as response, open(glove_zip_file, 'wb') as out_file:\n",
    "        shutil.copyfileobj(response, out_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unzip word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def unzip_single_file(zip_file_name, output_file_name):\n",
    "    \"\"\"\n",
    "        If the outFile is already created, don't recreate\n",
    "        If the outFile does not exist, create it from the zipFile\n",
    "    \"\"\"\n",
    "    if not os.path.isfile(output_file_name):\n",
    "        with open(output_file_name, 'wb') as out_file:\n",
    "            with zipfile.ZipFile(zip_file_name) as zipped:\n",
    "                for info in zipped.infolist():\n",
    "                    if output_file_name in info.filename:\n",
    "                        with zipped.open(info) as requested_file:\n",
    "                            out_file.write(requested_file.read())\n",
    "                            return\n",
    "\n",
    "unzip_single_file(glove_zip_file, glove_vectors_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "glove_wordmap = {}\n",
    "with open(glove_vectors_file, \"r\", encoding=\"utf8\") as glove:\n",
    "    for line in glove:\n",
    "        name, vector = tuple(line.split(\" \", 1))\n",
    "        glove_wordmap[name] = np.fromstring(vector, sep=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 0.7557078  -0.29991025  0.74354523 ...,  0.26082098  0.32645333\n",
      "    0.48831999]\n",
      "  [ 0.33039808  0.22212878  0.71501005 ...,  0.39743879  0.19633918\n",
      "    0.45981079]]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "elmo = hub.Module(\"https://tfhub.dev/google/elmo/2\", trainable=True)\n",
    "\n",
    "def sentence2sequence(sentence):\n",
    "    \"\"\"\n",
    "     \n",
    "    - Turns an input sentence into an (n,d) matrix, \n",
    "        where n is the number of tokens in the sentence\n",
    "        and d is the number of dimensions each word vector has.\n",
    "    \n",
    "      Tensorflow doesn't need to be used here, as simply\n",
    "      turning the sentence into a sequence based off our \n",
    "      mapping does not need the computational power that\n",
    "      Tensorflow provides. Normal Python suffices for this task.\n",
    "    \"\"\"\n",
    "    # GLOVE\n",
    "#     tokens = sentence.lower().split(\" \")\n",
    "#     rows = []\n",
    "#     words = []\n",
    "#     #Greedy search for tokens\n",
    "#     for token in tokens:\n",
    "#         i = len(token)\n",
    "#         while len(token) > 0 and i > 0:\n",
    "#             word = token[:i]\n",
    "#             if word in glove_wordmap:\n",
    "#                 rows.append(glove_wordmap[word])\n",
    "#                 words.append(word)\n",
    "#                 token = token[i:]\n",
    "#                 i = len(token)\n",
    "#             else:\n",
    "#                 i = i-1\n",
    "#     return rows, words\n",
    "\n",
    "# ELMO\n",
    "\n",
    "#     with tf.Session() as sess:\n",
    "    emb = elmo(sentence,\n",
    "    signature=\"default\",\n",
    "    as_dict=True)[\"elmo\"]\n",
    "#         sess.run(tf.global_variables_initializer())\n",
    "#         embeddings = sess.run(emb)\n",
    "    return embeddings\n",
    "print(sentence2sequence([\"apple carrots\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Constants setup\n",
    "max_hypothesis_length, max_evidence_length = 30, 30\n",
    "batch_size, vector_size, hidden_size = 128, 50, 64\n",
    "\n",
    "lstm_size = hidden_size\n",
    "\n",
    "weight_decay = 0.0001\n",
    "\n",
    "learning_rate = 1\n",
    "\n",
    "input_p, output_p = 0.5, 0.5\n",
    "\n",
    "training_iterations_count = 100000\n",
    "\n",
    "display_step = 10\n",
    "\n",
    "def score_setup(row):\n",
    "    convert_dict = {\n",
    "      'ENTAILMENT': 0,\n",
    "      'NEUTRAL': 1,\n",
    "      'CONTRADICTION': 2\n",
    "    }\n",
    "    score = np.zeros((3,))\n",
    "    tag = row[\"entailment_judgment\"]\n",
    "    score[convert_dict[tag]] += 1\n",
    "    return score\n",
    "\n",
    "def fit_to_size(matrix, shape):\n",
    "    res = np.zeros(shape)\n",
    "    slices = [slice(0,min(dim,shape[e])) for e, dim in enumerate(matrix.shape)]\n",
    "    res[slices] = matrix[slices]\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Training data](http://www.site.uottawa.ca/~diana/csi5386/A2_2019/SICK_train.txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hyp_sentences_embedded = \"data/elmo_hyp_sent_emb_%s.txt\"\n",
    "evi_sentences_embedded = \"data/elmo_evi_sent_emb_%s.txt\"\n",
    "import os, pickle\n",
    "def save_elmo_emb(hyp_sentences, evi_sentences, stage=\"\"):    \n",
    "    global hyp_sentences_embedded\n",
    "    global evi_sentences_embedded\n",
    "    hyp_file = hyp_sentences_embedded % stage\n",
    "    evi_file = evi_sentences_embedded % stage\n",
    "    if not os.path.isfile(hyp_file):\n",
    "        print(\"writing %s\" % hyp_file)\n",
    "        with open(hyp_file, \"wb\") as f:\n",
    "#             pickler = pickle.Pickler(f)\n",
    "            pickle.dump(hyp_sentences, f)\n",
    "    if not os.path.isfile(evi_file):\n",
    "        print(\"writing %s\" % evi_file)\n",
    "        with open(evi_file, \"wb\") as f:\n",
    "#             pickler = pickle.Pickler(f)\n",
    "            pickle.dump(evi_sentences, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "def load_elmo_emb_hyp():\n",
    "    if os.path.isfile(hyp_sentences_embedded):\n",
    "        with open(hyp_sentences_embedded, \"rb\") as f:\n",
    "#             unpickler = pickle.Unpickler(f)\n",
    "#             hyp_sentences = unpickler.load()\n",
    "            hyp_sentences = pickle.load()\n",
    "            return hyp_sentences\n",
    "def load_elmo_emb_evi():\n",
    "    if os.path.isfile(evi_sentences_embedded):\n",
    "        with open(evi_sentences_embedded, \"rb\") as f:\n",
    "#             unpickler = pickle.Unpickler(f)\n",
    "#             evi_sentences = unpickler.load()\n",
    "            evi_sentences = pickle.load()\n",
    "\n",
    "            return evi_sentences\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'update_progress' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-acbb6d3e2e20>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhyp_sentences\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevi_sentences\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 68\u001b[1;33m \u001b[0mupdate_progress\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     69\u001b[0m \u001b[0mdata_feature_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcorrect_values\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcorrect_scores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msplit_data_into_scores\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'update_progress' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import time, datetime\n",
    "import tensorflow as tf\n",
    "\n",
    "def embed_elmo(train, start_stage=0):\n",
    "    started = time.time()\n",
    "    last_stage = time.time()\n",
    "\n",
    "    hyp_sentences, evi_sentences = [], []\n",
    "    count = 0\n",
    "    number_of_elements = 10000\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        for row in train:\n",
    "            count += 1\n",
    "            if count <= start_stage:\n",
    "                continue\n",
    "            if (count-start_stage) % 10 == 0:\n",
    "                print(\"Stage %s. Time elapsed so far: %s\" % (str(count), str(time.time()-started)))\n",
    "                stage = str(count)\n",
    "                save_elmo_emb(hyp_sentences, evi_sentences, stage)\n",
    "                hyp_sentences, evi_sentences = [], []\n",
    "\n",
    "            print(\"Stage %s. Time elapsed last stage: %s\" % (str(count), str(time.time()-last_stage)))\n",
    "            last_stage = time.time()\n",
    "\n",
    "            hyp_sentences.append(#np.vstack(\n",
    "                    sess.run(sentence2sequence([row[\"sentence_A\"].lower()[0]])))\n",
    "    #             passed = time.time()\n",
    "    #             print(\"Emb %s S1: %s\" % (str(count), str(passed-started)))\n",
    "            evi_sentences.append(#np.vstack(\n",
    "                    sess.run(sentence2sequence([row[\"sentence_B\"].lower()[0]])))\n",
    "    #             started = time.time()\n",
    "    #             print(\"Emb %s S2: %s\" % (str(count), str(started-passed)))\n",
    "    #         update_progress(count / number_of_elements)\n",
    "    print(\"Total exe time: %s\" % (str(time.time()-started)))\n",
    "    return hyp_sentences, evi_sentences, labels, scores\n",
    "\n",
    "def split_data_into_scores():\n",
    "    print(\"started\")\n",
    "    import csv\n",
    "    with open(\"data/training.txt\",\"r\") as data:\n",
    "        print(\"opened training file\")\n",
    "        train = csv.DictReader(data , delimiter='\\t')\n",
    "        evi_sentences = load_elmo_emb_hyp()\n",
    "        hyp_sentences = load_elmo_emb_evi()\n",
    "        if evi_sentences is None or hyp_sentences is None:\n",
    "            hyp_sentences, evi_sentences = embed_elmo(train)\n",
    "        hyp_sentences = np.stack([fit_to_size(x, (max_hypothesis_length, vector_size))\n",
    "                          for x in hyp_sentences])\n",
    "        evi_sentences = np.stack([fit_to_size(x, (max_evidence_length, vector_size))\n",
    "                          for x in evi_sentences])\n",
    "        save_elmo_emb(hyp, evi)\n",
    "        \n",
    "        labels = []\n",
    "        scores = []\n",
    "        for row in train:\n",
    "            if count % 10==0:\n",
    "                print(\"Stage %s. Time elapsed so far: %s\" % (str(count), str(time.time()-started)))\n",
    "            count+=1\n",
    "            labels.append(row[\"entailment_judgment\"])\n",
    "            scores.append(score_setup(row))\n",
    "        \n",
    "        print(\"Split took: %s\" %(str(datetime.timedelta(seconds=time.time()-started))))    \n",
    "\n",
    "        return (hyp_sentences, evi_sentences), labels, np.array(scores)\n",
    "update_progress(1)\n",
    "data_feature_list, correct_values, correct_scores = split_data_into_scores()\n",
    "\n",
    "l_h, l_e = max_hypothesis_length, max_evidence_length\n",
    "N, D, H = batch_size, vector_size, hidden_size\n",
    "l_seq = l_h + l_e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lstm = tf.contrib.rnn.BasicLSTMCell(lstm_size)\n",
    "lstm_drop =  tf.contrib.rnn.DropoutWrapper(lstm, input_p, output_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# N: The number of elements in each of our batches, \n",
    "#   which we use to train subsets of data for efficiency's sake.\n",
    "# l_h: The maximum length of a hypothesis, or the second sentence.  This is\n",
    "#   used because training an RNN is extraordinarily difficult without \n",
    "#   rolling it out to a fixed length.\n",
    "# l_e: The maximum length of evidence, the first sentence.  This is used\n",
    "#   because training an RNN is extraordinarily difficult without \n",
    "#   rolling it out to a fixed length.\n",
    "# D: The size of our used GloVe or other vectors.\n",
    "hyp = tf.placeholder(tf.float32, [N, l_h, D], 'hypothesis')\n",
    "evi = tf.placeholder(tf.float32, [N, l_e, D], 'evidence')\n",
    "y = tf.placeholder(tf.float32, [N, 3], 'label')\n",
    "# hyp: Where the hypotheses will be stored during training.\n",
    "# evi: Where the evidences will be stored during training.\n",
    "# y: Where correct scores will be stored during training.\n",
    "\n",
    "# lstm_size: the size of the gates in the LSTM, \n",
    "#    as in the first LSTM layer's initialization.\n",
    "lstm_back = tf.contrib.rnn.BasicLSTMCell(lstm_size)\n",
    "# lstm_back:  The LSTM used for looking backwards \n",
    "#   through the sentences, similar to lstm.\n",
    "\n",
    "# input_p: the probability that inputs to the LSTM will be retained at each\n",
    "#   iteration of dropout.\n",
    "# output_p: the probability that outputs from the LSTM will be retained at \n",
    "#   each iteration of dropout.\n",
    "lstm_drop_back = tf.contrib.rnn.DropoutWrapper(lstm_back, input_p, output_p)\n",
    "# lstm_drop_back:  A dropout wrapper for lstm_back, like lstm_drop.\n",
    "\n",
    "\n",
    "fc_initializer = tf.random_normal_initializer(stddev=0.1) \n",
    "# fc_initializer: initial values for the fully connected layer's weights.\n",
    "# hidden_size: the size of the outputs from each lstm layer.  \n",
    "#   Multiplied by 2 to account for the two LSTMs.\n",
    "fc_weight = tf.get_variable('fc_weight', [2*hidden_size, 3], \n",
    "                            initializer = fc_initializer)\n",
    "# fc_weight: Storage for the fully connected layer's weights.\n",
    "fc_bias = tf.get_variable('bias', [3])\n",
    "# fc_bias: Storage for the fully connected layer's bias.\n",
    "\n",
    "# tf.GraphKeys.REGULARIZATION_LOSSES:  A key to a collection in the graph\n",
    "#   designated for losses due to regularization.\n",
    "#   In this case, this portion of loss is regularization on the weights\n",
    "#   for the fully connected layer.\n",
    "tf.add_to_collection(tf.GraphKeys.REGULARIZATION_LOSSES, \n",
    "                     tf.nn.l2_loss(fc_weight)) \n",
    "\n",
    "x = tf.concat([hyp, evi], 1) # N, (Lh+Le), d\n",
    "# Permuting batch_size and n_steps\n",
    "x = tf.transpose(x, [1, 0, 2]) # (Le+Lh), N, d\n",
    "# Reshaping to (n_steps*batch_size, n_input)\n",
    "x = tf.reshape(x, [-1, vector_size]) # (Le+Lh)*N, d\n",
    "# Split to get a list of 'n_steps' tensors of shape (batch_size, n_input)\n",
    "x = tf.split(x, l_seq,)\n",
    "\n",
    "# x: the inputs to the bidirectional_rnn\n",
    "\n",
    "\n",
    "# tf.contrib.rnn.static_bidirectional_rnn: Runs the input through\n",
    "#   two recurrent networks, one that runs the inputs forward and one\n",
    "#   that runs the inputs in reversed order, combining the outputs.\n",
    "rnn_outputs, _, _ = tf.contrib.rnn.static_bidirectional_rnn(lstm, lstm_back,\n",
    "                                                            x, dtype=tf.float32)\n",
    "# rnn_outputs: the list of LSTM outputs, as a list. \n",
    "#   What we want is the latest output, rnn_outputs[-1]\n",
    "\n",
    "classification_scores = tf.matmul(rnn_outputs[-1], fc_weight) + fc_bias\n",
    "# The scores are relative certainties for how likely the output matches\n",
    "#   a certain entailment: \n",
    "#     0: Positive entailment\n",
    "#     1: Neutral entailment\n",
    "#     2: Negative entailment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-14-0b21781cbf35>:10: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with tf.variable_scope('Accuracy'):\n",
    "    predicts = tf.cast(tf.argmax(classification_scores, 1), 'int32')\n",
    "    y_label = tf.cast(tf.argmax(y, 1), 'int32')\n",
    "    corrects = tf.equal(predicts, y_label)\n",
    "    num_corrects = tf.reduce_sum(tf.cast(corrects, tf.float32))\n",
    "    accuracy = tf.reduce_mean(tf.cast(corrects, tf.float32))\n",
    "\n",
    "with tf.variable_scope(\"loss\"):\n",
    "    cross_entropy = tf.nn.softmax_cross_entropy_with_logits(\n",
    "        logits = classification_scores, labels = y)\n",
    "    loss = tf.reduce_mean(cross_entropy)\n",
    "    total_loss = loss + weight_decay * tf.add_n(\n",
    "        tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES))\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "\n",
    "opt_op = optimizer.minimize(total_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                                                                                                                   | 0/782 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 0.0, Minibatch Loss= 1.088545, Training Accuracy= 0.54688\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|█▉                                                                                                                                                                         | 9/782 [00:03<18:19,  1.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 10.0, Minibatch Loss= 0.955285, Training Accuracy= 0.54688\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|███▋                                                                                                                                                                      | 17/782 [00:03<09:04,  1.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 20.0, Minibatch Loss= 0.954662, Training Accuracy= 0.54688\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|██████▎                                                                                                                                                                   | 29/782 [00:03<03:17,  3.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 30.0, Minibatch Loss= 0.953685, Training Accuracy= 0.54688\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|████████                                                                                                                                                                  | 37/782 [00:03<01:46,  7.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 40.0, Minibatch Loss= 0.949344, Training Accuracy= 0.54688\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|██████████▋                                                                                                                                                               | 49/782 [00:04<00:48, 14.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 50.0, Minibatch Loss= 0.949705, Training Accuracy= 0.54688\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|████████████▌                                                                                                                                                             | 58/782 [00:04<00:34, 21.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 60.0, Minibatch Loss= 0.954604, Training Accuracy= 0.54688\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|██████████████▎                                                                                                                                                           | 66/782 [00:04<00:26, 26.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 70.0, Minibatch Loss= 0.953389, Training Accuracy= 0.54688\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█████████████████▏                                                                                                                                                        | 79/782 [00:05<00:21, 33.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 80.0, Minibatch Loss= 0.949505, Training Accuracy= 0.54688\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|██████████████████▉                                                                                                                                                       | 87/782 [00:05<00:20, 33.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 90.0, Minibatch Loss= 0.946607, Training Accuracy= 0.54688\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█████████████████████▌                                                                                                                                                    | 99/782 [00:05<00:19, 35.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 100.0, Minibatch Loss= 0.954881, Training Accuracy= 0.54688\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|███████████████████████                                                                                                                                                  | 107/782 [00:05<00:18, 35.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 110.0, Minibatch Loss= 0.938280, Training Accuracy= 0.54688\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█████████████████████████▉                                                                                                                                               | 120/782 [00:06<00:17, 37.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 120.0, Minibatch Loss= 0.934608, Training Accuracy= 0.54688\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|███████████████████████████▋                                                                                                                                             | 128/782 [00:06<00:18, 36.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 130.0, Minibatch Loss= 0.975279, Training Accuracy= 0.58594\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|██████████████████████████████▎                                                                                                                                          | 140/782 [00:06<00:17, 36.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 140.0, Minibatch Loss= 0.941669, Training Accuracy= 0.58594\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|███████████████████████████████▉                                                                                                                                         | 148/782 [00:06<00:17, 36.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 150.0, Minibatch Loss= 0.945555, Training Accuracy= 0.58594\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██████████████████████████████████▌                                                                                                                                      | 160/782 [00:07<00:18, 32.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 160.0, Minibatch Loss= 0.976508, Training Accuracy= 0.58594\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|████████████████████████████████████▎                                                                                                                                    | 168/782 [00:07<00:18, 33.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 170.0, Minibatch Loss= 0.966716, Training Accuracy= 0.58594\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██████████████████████████████████████▉                                                                                                                                  | 180/782 [00:07<00:17, 34.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 180.0, Minibatch Loss= 0.949962, Training Accuracy= 0.58594\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|████████████████████████████████████████▋                                                                                                                                | 188/782 [00:08<00:17, 33.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 190.0, Minibatch Loss= 0.944000, Training Accuracy= 0.58594\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|███████████████████████████████████████████▏                                                                                                                             | 200/782 [00:08<00:16, 35.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 200.0, Minibatch Loss= 0.938348, Training Accuracy= 0.58594\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|████████████████████████████████████████████▉                                                                                                                            | 208/782 [00:08<00:16, 35.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 210.0, Minibatch Loss= 0.931351, Training Accuracy= 0.58594\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|███████████████████████████████████████████████▌                                                                                                                         | 220/782 [00:09<00:15, 36.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 220.0, Minibatch Loss= 0.931949, Training Accuracy= 0.58594\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|█████████████████████████████████████████████████▍                                                                                                                       | 229/782 [00:09<00:15, 36.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 230.0, Minibatch Loss= 0.941348, Training Accuracy= 0.58594\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███████████████████████████████████████████████████▏                                                                                                                     | 237/782 [00:09<00:14, 36.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 240.0, Minibatch Loss= 0.935957, Training Accuracy= 0.58594\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|█████████████████████████████████████████████████████▊                                                                                                                   | 249/782 [00:09<00:14, 36.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 250.0, Minibatch Loss= 0.930860, Training Accuracy= 0.60938\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███████████████████████████████████████████████████████▌                                                                                                                 | 257/782 [00:10<00:14, 36.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 260.0, Minibatch Loss= 0.930871, Training Accuracy= 0.60938\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|██████████████████████████████████████████████████████████▏                                                                                                              | 269/782 [00:10<00:16, 31.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 270.0, Minibatch Loss= 0.897945, Training Accuracy= 0.60938\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███████████████████████████████████████████████████████████▊                                                                                                             | 277/782 [00:10<00:15, 33.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 280.0, Minibatch Loss= 0.902510, Training Accuracy= 0.60938\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|██████████████████████████████████████████████████████████████▍                                                                                                          | 289/782 [00:11<00:13, 35.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 290.0, Minibatch Loss= 0.887287, Training Accuracy= 0.60938\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|████████████████████████████████████████████████████████████████▍                                                                                                        | 298/782 [00:11<00:13, 35.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 300.0, Minibatch Loss= 0.932270, Training Accuracy= 0.60938\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|██████████████████████████████████████████████████████████████████▏                                                                                                      | 306/782 [00:11<00:13, 35.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 310.0, Minibatch Loss= 0.883820, Training Accuracy= 0.60938\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|█████████████████████████████████████████████████████████████████████▏                                                                                                   | 320/782 [00:12<00:12, 37.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 320.0, Minibatch Loss= 0.900218, Training Accuracy= 0.60938\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|██████████████████████████████████████████████████████████████████████▉                                                                                                  | 328/782 [00:12<00:13, 33.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 330.0, Minibatch Loss= 0.864138, Training Accuracy= 0.60938\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████████████████████████████████████████████████████████████████████████▊                                                                                                | 337/782 [00:12<00:12, 34.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 340.0, Minibatch Loss= 0.915513, Training Accuracy= 0.60938\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|███████████████████████████████████████████████████████████████████████████▍                                                                                             | 349/782 [00:12<00:12, 35.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 350.0, Minibatch Loss= 0.875103, Training Accuracy= 0.60938\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|█████████████████████████████████████████████████████████████████████████████▏                                                                                           | 357/782 [00:13<00:12, 35.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 360.0, Minibatch Loss= 0.949105, Training Accuracy= 0.60938\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|███████████████████████████████████████████████████████████████████████████████▋                                                                                         | 369/782 [00:13<00:12, 33.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 370.0, Minibatch Loss= 0.909757, Training Accuracy= 0.62500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|█████████████████████████████████████████████████████████████████████████████████▋                                                                                       | 378/782 [00:13<00:11, 34.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 380.0, Minibatch Loss= 0.875509, Training Accuracy= 0.58594\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|████████████████████████████████████████████████████████████████████████████████████▎                                                                                    | 390/782 [00:14<00:11, 34.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 390.0, Minibatch Loss= 0.867362, Training Accuracy= 0.58594\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|██████████████████████████████████████████████████████████████████████████████████████                                                                                   | 398/782 [00:14<00:10, 34.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 400.0, Minibatch Loss= 0.838592, Training Accuracy= 0.58594\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|███████████████████████████████████████████████████████████████████████████████████████▋                                                                                 | 406/782 [00:14<00:10, 35.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 410.0, Minibatch Loss= 0.800593, Training Accuracy= 0.63281\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|██████████████████████████████████████████████████████████████████████████████████████████▊                                                                              | 420/782 [00:14<00:09, 37.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 420.0, Minibatch Loss= 0.967913, Training Accuracy= 0.53906\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|████████████████████████████████████████████████████████████████████████████████████████████▍                                                                            | 428/782 [00:15<00:09, 36.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 430.0, Minibatch Loss= 0.873160, Training Accuracy= 0.53125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|███████████████████████████████████████████████████████████████████████████████████████████████                                                                          | 440/782 [00:15<00:10, 33.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 440.0, Minibatch Loss= 0.752973, Training Accuracy= 0.61719\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|████████████████████████████████████████████████████████████████████████████████████████████████▊                                                                        | 448/782 [00:15<00:09, 34.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 450.0, Minibatch Loss= 0.950051, Training Accuracy= 0.57812\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|███████████████████████████████████████████████████████████████████████████████████████████████████▍                                                                     | 460/782 [00:16<00:09, 35.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 460.0, Minibatch Loss= 0.916224, Training Accuracy= 0.58594\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|█████████████████████████████████████████████████████████████████████████████████████████████████████▏                                                                   | 468/782 [00:16<00:08, 35.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 470.0, Minibatch Loss= 0.893310, Training Accuracy= 0.58594\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|███████████████████████████████████████████████████████████████████████████████████████████████████████▋                                                                 | 480/782 [00:16<00:08, 36.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 480.0, Minibatch Loss= 0.870162, Training Accuracy= 0.58594\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|█████████████████████████████████████████████████████████████████████████████████████████████████████████▍                                                               | 488/782 [00:16<00:08, 35.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 490.0, Minibatch Loss= 0.832232, Training Accuracy= 0.59375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                             | 500/782 [00:17<00:08, 33.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 500.0, Minibatch Loss= 0.995679, Training Accuracy= 0.50000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                                           | 508/782 [00:17<00:08, 33.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 510.0, Minibatch Loss= 0.969486, Training Accuracy= 0.50781\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                                                        | 520/782 [00:17<00:07, 34.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 520.0, Minibatch Loss= 0.941515, Training Accuracy= 0.50781\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                       | 528/782 [00:18<00:07, 34.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 530.0, Minibatch Loss= 0.935399, Training Accuracy= 0.52344\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                                    | 540/782 [00:18<00:07, 33.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 540.0, Minibatch Loss= 0.924663, Training Accuracy= 0.53125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                                                  | 548/782 [00:18<00:07, 32.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 550.0, Minibatch Loss= 0.961541, Training Accuracy= 0.53906\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                | 560/782 [00:19<00:06, 34.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 560.0, Minibatch Loss= 0.870920, Training Accuracy= 0.57812\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                              | 568/782 [00:19<00:06, 34.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 570.0, Minibatch Loss= 0.954794, Training Accuracy= 0.52344\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                                           | 580/782 [00:19<00:05, 34.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 580.0, Minibatch Loss= 0.926407, Training Accuracy= 0.53125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                          | 588/782 [00:19<00:05, 32.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 590.0, Minibatch Loss= 0.928199, Training Accuracy= 0.52344\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                       | 600/782 [00:20<00:05, 34.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 600.0, Minibatch Loss= 0.900524, Training Accuracy= 0.55469\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                                     | 608/782 [00:20<00:05, 34.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 610.0, Minibatch Loss= 0.956419, Training Accuracy= 0.52344\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                                   | 620/782 [00:20<00:04, 34.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 620.0, Minibatch Loss= 0.841862, Training Accuracy= 0.62500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                 | 628/782 [00:21<00:04, 33.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 630.0, Minibatch Loss= 0.879170, Training Accuracy= 0.58594\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                              | 640/782 [00:21<00:04, 34.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 640.0, Minibatch Loss= 0.859413, Training Accuracy= 0.58594\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                             | 648/782 [00:21<00:03, 34.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 650.0, Minibatch Loss= 1.021953, Training Accuracy= 0.58594\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                          | 660/782 [00:22<00:03, 34.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 660.0, Minibatch Loss= 0.815212, Training Accuracy= 0.57031\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                        | 668/782 [00:22<00:03, 33.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 670.0, Minibatch Loss= 0.895409, Training Accuracy= 0.57031\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                      | 680/782 [00:22<00:03, 33.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 680.0, Minibatch Loss= 0.882989, Training Accuracy= 0.58594\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                    | 688/782 [00:22<00:02, 33.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 690.0, Minibatch Loss= 0.823505, Training Accuracy= 0.58594\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                 | 700/782 [00:23<00:02, 35.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 700.0, Minibatch Loss= 0.796352, Training Accuracy= 0.63281\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                | 708/782 [00:23<00:02, 33.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 710.0, Minibatch Loss= 0.840799, Training Accuracy= 0.58594\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌             | 720/782 [00:23<00:01, 34.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 720.0, Minibatch Loss= 0.860268, Training Accuracy= 0.60938\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎           | 728/782 [00:24<00:01, 34.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 730.0, Minibatch Loss= 0.842868, Training Accuracy= 0.56250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉         | 740/782 [00:24<00:01, 35.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 740.0, Minibatch Loss= 0.744859, Training Accuracy= 0.61719\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋       | 748/782 [00:24<00:01, 32.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 750.0, Minibatch Loss= 0.856104, Training Accuracy= 0.64844\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏    | 760/782 [00:25<00:00, 33.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 760.0, Minibatch Loss= 0.763196, Training Accuracy= 0.64844\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉   | 768/782 [00:25<00:00, 34.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 770.0, Minibatch Loss= 0.828612, Training Accuracy= 0.64844\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌| 780/782 [00:25<00:00, 33.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 780.0, Minibatch Loss= 0.708883, Training Accuracy= 0.66406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 782/782 [00:25<00:00, 30.39it/s]\n"
     ]
    }
   ],
   "source": [
    "# Initialize variables\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Use TQDM if installed\n",
    "tqdm_installed = False\n",
    "try:\n",
    "    from tqdm import tqdm\n",
    "    tqdm_installed = True\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Launch the Tensorflow session\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "# training_iterations_count: The number of data pieces to train on in total\n",
    "# batch_size: The number of data pieces per batch\n",
    "training_iterations = range(0,training_iterations_count,batch_size)\n",
    "if tqdm_installed:\n",
    "    # Add a progress bar if TQDM is installed\n",
    "    training_iterations = tqdm(training_iterations)\n",
    "\n",
    "for i in training_iterations:\n",
    "    if i % 1000 == 0:\n",
    "    # Select indices for a random data subset\n",
    "        batch = np.random.randint(data_feature_list[0].shape[0], size=batch_size)\n",
    "    \n",
    "    # Use the selected subset indices to initialize the graph's \n",
    "    #   placeholder values\n",
    "    hyps, evis, ys = (data_feature_list[0][batch,:],\n",
    "                      data_feature_list[1][batch,:],\n",
    "                      correct_scores[batch])\n",
    "    \n",
    "    # Run the optimization with these initialized values\n",
    "    sess.run([opt_op], feed_dict={hyp: hyps, evi: evis, y: ys})\n",
    "    # display_step: how often the accuracy and loss should \n",
    "    #   be tested and displayed.\n",
    "    if (i/batch_size) % display_step == 0:\n",
    "        # Calculate batch accuracy\n",
    "        acc = sess.run(accuracy, feed_dict={hyp: hyps, evi: evis, y: ys})\n",
    "        # Calculate batch loss\n",
    "        tmp_loss = sess.run(loss, feed_dict={hyp: hyps, evi: evis, y: ys})\n",
    "        # Display results\n",
    "        print(\"Iter \" + str(i/batch_size) + \", Minibatch Loss= \" + \\\n",
    "              \"{:.6f}\".format(tmp_loss) + \", Training Accuracy= \" + \\\n",
    "              \"{:.5f}\".format(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive / Positive\n",
      "Positive / Neutral\n",
      "Positive / Negative\n",
      "Positive / Positive\n",
      "Positive / Neutral\n",
      "Positive / Neutral\n",
      "Positive / Neutral\n",
      "Positive / Positive\n",
      "Positive / Neutral\n",
      "Positive / Neutral\n",
      "Positive / Neutral\n",
      "Positive / Negative\n",
      "Positive / Neutral\n",
      "Positive / Neutral\n",
      "Positive / Neutral\n",
      "Positive / Negative\n",
      "Positive / Negative\n",
      "Positive / Positive\n",
      "Positive / Positive\n",
      "Positive / Neutral\n",
      "Positive / Neutral\n",
      "Positive / Negative\n",
      "Positive / Negative\n",
      "Positive / Positive\n",
      "Positive / Positive\n",
      "Positive / Neutral\n",
      "Positive / Neutral\n",
      "Positive / Positive\n",
      "Positive / Neutral\n",
      "Positive / Neutral\n",
      "Positive / Neutral\n",
      "Positive / Positive\n",
      "Positive / Positive\n",
      "Positive / Positive\n",
      "Positive / Negative\n",
      "Positive / Neutral\n",
      "Positive / Positive\n",
      "Positive / Neutral\n",
      "Positive / Neutral\n",
      "Positive / Negative\n",
      "Positive / Positive\n",
      "Positive / Neutral\n",
      "Positive / Neutral\n",
      "Positive / Positive\n",
      "Positive / Positive\n",
      "Positive / Neutral\n",
      "Positive / Negative\n",
      "Positive / Neutral\n",
      "Positive / Neutral\n",
      "Positive / Positive\n",
      "Positive / Positive\n",
      "Positive / Positive\n",
      "Positive / Neutral\n",
      "Positive / Positive\n",
      "Positive / Positive\n",
      "Positive / Neutral\n",
      "Positive / Neutral\n",
      "Positive / Neutral\n",
      "Positive / Positive\n",
      "Positive / Neutral\n",
      "Positive / Positive\n",
      "Positive / Positive\n",
      "Positive / Neutral\n",
      "Positive / Neutral\n",
      "Positive / Neutral\n",
      "Positive / Neutral\n",
      "Positive / Neutral\n",
      "Positive / Neutral\n",
      "Positive / Neutral\n",
      "Positive / Positive\n",
      "Positive / Neutral\n",
      "Positive / Neutral\n",
      "Positive / Positive\n",
      "Positive / Positive\n",
      "Positive / Neutral\n",
      "Positive / Neutral\n",
      "Positive / Neutral\n",
      "Positive / Neutral\n",
      "Positive / Positive\n",
      "Positive / Neutral\n",
      "Positive / Negative\n",
      "Positive / Neutral\n",
      "Positive / Positive\n",
      "Positive / Neutral\n",
      "Positive / Positive\n",
      "Positive / Neutral\n",
      "Positive / Negative\n",
      "Positive / Neutral\n",
      "Positive / Neutral\n",
      "Positive / Neutral\n",
      "Positive / Neutral\n",
      "Positive / Positive\n",
      "Positive / Positive\n",
      "Positive / Positive\n",
      "Positive / Positive\n",
      "Positive / Neutral\n",
      "Positive / Positive\n",
      "Positive / Positive\n",
      "Positive / Neutral\n",
      "Positive / Positive\n",
      "Positive / Neutral\n",
      "Positive / Negative\n",
      "Positive / Neutral\n",
      "Positive / Neutral\n",
      "Positive / Negative\n",
      "Positive / Neutral\n",
      "Positive / Positive\n",
      "Positive / Neutral\n",
      "Positive / Neutral\n",
      "Positive / Neutral\n",
      "Positive / Neutral\n",
      "Positive / Positive\n",
      "Positive / Neutral\n",
      "Positive / Negative\n",
      "Positive / Neutral\n",
      "Positive / Neutral\n",
      "Positive / Positive\n",
      "Positive / Neutral\n",
      "Positive / Neutral\n",
      "Positive / Neutral\n",
      "Positive / Neutral\n",
      "Positive / Neutral\n",
      "Positive / Neutral\n",
      "Positive / Neutral\n",
      "Positive / Neutral\n",
      "Positive / Neutral\n",
      "Positive / Neutral\n",
      "Positive / Positive\n",
      "Acc: 31.25\n"
     ]
    }
   ],
   "source": [
    "evidences = [\"People wearing costumes are gathering in a forest and are looking in the same direction\"]\n",
    "hypotheses = [\"Masked people are looking in the same direction in a forest\"\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "sentence1 = [fit_to_size(np.vstack(sentence2sequence(evidence)[0]),\n",
    "                         (30, 50)) for evidence in evidences]\n",
    "\n",
    "sentence2 = [fit_to_size(np.vstack(sentence2sequence(hypothesis)[0]),\n",
    "                         (30,50)) for hypothesis in hypotheses]\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def split_data_into_scores():\n",
    "    import csv\n",
    "    with open(\"data/test_labeled.txt\",\"r\") as data:\n",
    "        train = csv.DictReader(data , delimiter='\\t')\n",
    "        evi_sentences = []\n",
    "        hyp_sentences = []\n",
    "        labels = []\n",
    "        scores = []\n",
    "        count = 1\n",
    "        for row in train:\n",
    "            hyp_sentences.append(np.vstack(\n",
    "                    sentence2sequence(row[\"sentence_A\"].lower())[0]))\n",
    "            evi_sentences.append(np.vstack(\n",
    "                    sentence2sequence(row[\"sentence_B\"].lower())[0]))\n",
    "            labels.append(row[\"entailment_judgment\"])\n",
    "            scores.append(score_setup(row))\n",
    "        \n",
    "        hyp_sentences = np.stack([fit_to_size(x, (max_hypothesis_length, vector_size))\n",
    "                          for x in hyp_sentences])\n",
    "        evi_sentences = np.stack([fit_to_size(x, (max_evidence_length, vector_size))\n",
    "                          for x in evi_sentences])\n",
    "                                 \n",
    "        return (hyp_sentences, evi_sentences), labels, np.array(scores)\n",
    "data_feature_list, correct_values, correct_scores = split_data_into_scores()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "hyps, evis, ys = (data_feature_list[0][batch,:],\n",
    "                  data_feature_list[1][batch,:],\n",
    "                  correct_scores[batch])\n",
    "predictions = sess.run(classification_scores, feed_dict={hyp: hyps, evi: evis, y: ys})\n",
    "\n",
    "acc = sess.run(accuracy, feed_dict={hyp: hyps, evi: evis, y: ys})\n",
    "print(acc)\n",
    "\n",
    "total = len(predictions)\n",
    "correct_predictions = 0\n",
    "for i,prediction in enumerate(predictions):\n",
    "    if np.argmax(prediction[0])==np.argmax(ys[i]):\n",
    "        correct_predictions += 1\n",
    "#     print([\"Positive\", \"Neutral\", \"Negative\"][np.argmax(prediction[0])]+\n",
    "#       \" / \" + [\"Positive\", \"Neutral\", \"Negative\"][np.argmax(ys[i])])\n",
    "    \n",
    "print(\"Acc: %s\" % str(correct_predictions*100/total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Can't convert 'text': Shape TensorShape([]) is incompatible with TensorShape([Dimension(None)])",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-43b16676688c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m \u001b[0mvisualize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"The quick brown fox jumped over the lazy dog.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[0mvisualize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"The pretty flowers shone in the sunlight.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-22-43b16676688c>\u001b[0m in \u001b[0;36mvisualize\u001b[1;34m(sentence)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mvisualize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mrows\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwords\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msentence2sequence\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0mmat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mfig\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-21-3c9c3cfd2d90>\u001b[0m in \u001b[0;36msentence2sequence\u001b[1;34m(sentence)\u001b[0m\n\u001b[0;32m     35\u001b[0m     embeddings = elmo(sentence,\n\u001b[0;32m     36\u001b[0m     \u001b[0msignature\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"default\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m     as_dict=True)[\"elmo\"]\n\u001b[0m\u001b[0;32m     38\u001b[0m \u001b[1;31m#     print(embeddings)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0membeddings\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\Anaconda3\\lib\\site-packages\\tensorflow_hub\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs, _sentinel, signature, as_dict)\u001b[0m\n\u001b[0;32m    243\u001b[0m     dict_inputs = _convert_dict_inputs(\n\u001b[0;32m    244\u001b[0m         inputs, self._spec.get_input_info_dict(signature=signature,\n\u001b[1;32m--> 245\u001b[1;33m                                                tags=self._tags))\n\u001b[0m\u001b[0;32m    246\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    247\u001b[0m     dict_outputs = self._impl.create_apply_graph(\n",
      "\u001b[1;32mF:\\Anaconda3\\lib\\site-packages\\tensorflow_hub\\module.py\u001b[0m in \u001b[0;36m_convert_dict_inputs\u001b[1;34m(inputs, tensor_info_map)\u001b[0m\n\u001b[0;32m    445\u001b[0m   \u001b[0mdict_inputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_prepare_dict_inputs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensor_info_map\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    446\u001b[0m   return tensor_info.convert_dict_to_compatible_tensor(dict_inputs,\n\u001b[1;32m--> 447\u001b[1;33m                                                        tensor_info_map)\n\u001b[0m\u001b[0;32m    448\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    449\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\Anaconda3\\lib\\site-packages\\tensorflow_hub\\tensor_info.py\u001b[0m in \u001b[0;36mconvert_dict_to_compatible_tensor\u001b[1;34m(values, targets)\u001b[0m\n\u001b[0;32m    148\u001b[0m   \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m     result[key] = _convert_to_compatible_tensor(\n\u001b[1;32m--> 150\u001b[1;33m         value, targets[key], error_prefix=\"Can't convert %r\" % key)\n\u001b[0m\u001b[0;32m    151\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\Anaconda3\\lib\\site-packages\\tensorflow_hub\\tensor_info.py\u001b[0m in \u001b[0;36m_convert_to_compatible_tensor\u001b[1;34m(value, target, error_prefix)\u001b[0m\n\u001b[0;32m    127\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mtensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_compatible_with\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    128\u001b[0m     raise TypeError(\"%s: Shape %r is incompatible with %r\" %\n\u001b[1;32m--> 129\u001b[1;33m                     (error_prefix, tensor.get_shape(), target.get_shape()))\n\u001b[0m\u001b[0;32m    130\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mtensor\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    131\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: Can't convert 'text': Shape TensorShape([]) is incompatible with TensorShape([Dimension(None)])"
     ]
    }
   ],
   "source": [
    "# def visualize(sentence):\n",
    "#     rows, words = sentence2sequence(sentence)\n",
    "#     mat = np.vstack(rows)\n",
    "    \n",
    "#     fig = plt.figure()\n",
    "#     ax = fig.add_subplot(111)\n",
    "#     shown = ax.matshow(mat, aspect=\"auto\")\n",
    "#     ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "#     fig.colorbar(shown)\n",
    "    \n",
    "#     ax.set_yticklabels([\"\"]+words)\n",
    "#     plt.show()\n",
    "    \n",
    "# visualize(\"The quick brown fox jumped over the lazy dog.\")\n",
    "# visualize(\"The pretty flowers shone in the sunlight.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
