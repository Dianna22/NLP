{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [SICK Training data](http://www.site.uottawa.ca/~diana/csi5386/A2_2019/SICK_train.txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "##### max length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max_hyp 28\n",
      "Max_evi 32\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import csv\n",
    "file_name=\"data/training.txt\"\n",
    "with open(file_name,\"r\") as data:\n",
    "    train = csv.DictReader(data , delimiter='\\t')\n",
    "    max_evi, max_hyp = 0, 0 \n",
    "    count = 1\n",
    "    for row in train:\n",
    "        hyp = len(row[\"sentence_A\"].split())\n",
    "        if hyp > max_hyp:\n",
    "            max_hyp = hyp\n",
    "        evi = len(row[\"sentence_B\"].split())\n",
    "        if evi > max_evi:\n",
    "            max_evi = evi\n",
    "    print(\"Max_hyp %s\" % str(max_hyp))        \n",
    "    print(\"Max_evi %s\" % str(max_evi))            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "##### Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "glove_zip_file = \"data/glove.6B.zip\"\n",
    "glove_vectors_file = \"data/glove.6B.300d.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import zipfile, urllib.request, shutil, os\n",
    "    \n",
    "#large file - 862 MB\n",
    "if (not os.path.isfile(glove_zip_file) and\n",
    "    not os.path.isfile(glove_vectors_file)):\n",
    "    with urllib.request.urlopen(\"http://nlp.stanford.edu/data/glove.6B.zip\") as response, open(glove_zip_file, 'wb') as out_file:\n",
    "        shutil.copyfileobj(response, out_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import zipfile\n",
    "def unzip_single_file(zip_file_name, output_file_name):\n",
    "    \"\"\"\n",
    "        If the outFile is already created, don't recreate\n",
    "        If the outFile does not exist, create it from the zipFile\n",
    "    \"\"\"\n",
    "    if not os.path.isfile(output_file_name):\n",
    "        with open(output_file_name, 'wb') as out_file:\n",
    "            with zipfile.ZipFile(zip_file_name) as zipped:\n",
    "                for info in zipped.infolist():\n",
    "                    if output_file_name in info.filename:\n",
    "                        with zipped.open(info) as requested_file:\n",
    "                            out_file.write(requested_file.read())\n",
    "                            return\n",
    "\n",
    "unzip_single_file(glove_zip_file, glove_vectors_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "glove_wordmap = {}\n",
    "with open(glove_vectors_file, \"r\", encoding=\"utf8\") as glove:\n",
    "    for line in glove:\n",
    "        name, vector = tuple(line.split(\" \", 1))\n",
    "        glove_wordmap[name] = np.fromstring(vector, sep=\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.random.seed(1337)  # for reproducibility\n",
    "import os\n",
    "from keras.regularizers import l2\n",
    "from keras.callbacks import *\n",
    "# from visualizer import *\n",
    "from keras.models import *\n",
    "from keras.optimizers import *\n",
    "from keras import utils\n",
    "from keras.utils.np_utils import *\n",
    "from keras.layers.core import *\n",
    "from keras.layers import Input, Embedding, LSTM, Dense, merge, TimeDistributed\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "# from keras.utils.visualize_util import plot  # THIS IS BAD\n",
    "# from data_reader import *\n",
    "import logging\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AccCallBack(Callback):\n",
    "    def __init__(self, xtrain, ytrain, xdev, ydev, xtest, ytest, vocab, opts):\n",
    "        self.xtrain = xtrain\n",
    "        self.ytrain = ytrain\n",
    "        self.xdev = xdev\n",
    "        self.ydev = ydev\n",
    "        self.xtest = xtest\n",
    "        self.ytest = ytest\n",
    "        self.vocab = vocab\n",
    "        self.opts = opts\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        train_acc = compute_acc(self.xtrain, self.ytrain, self.vocab, self.model, self.opts)\n",
    "        dev_acc = compute_acc(self.xdev, self.ydev, self.vocab, self.model, self.opts)\n",
    "        test_acc = compute_acc(self.xtest, self.ytest, self.vocab, self.model, self.opts)\n",
    "        logging.info('----------------------------------')\n",
    "        logging.info('Epoch ' + str(epoch) + ' train loss:' + str(logs.get('loss')) + ' - Validation loss: ' + str(\n",
    "            logs.get('val_loss')) + ' train acc: ' + str(train_acc[0]) + '/' + str(train_acc[1]) + ' dev acc: ' + str(\n",
    "            dev_acc[0]) + '/' + str(dev_acc[1]) + ' test acc: ' + str(test_acc[0]) + '/' + str(test_acc[1]))\n",
    "        logging.info('----------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_acc(X, Y, vocab, model, opts):\n",
    "    scores = model.predict(X, batch_size=opts['batch_size'])\n",
    "    prediction = np.zeros(scores.shape)\n",
    "    for i in range(scores.shape[0]):\n",
    "        l = np.argmax(scores[i])\n",
    "        prediction[i][l] = 1.0\n",
    "    assert np.array_equal(np.ones(prediction.shape[0]), np.sum(prediction, axis=1))\n",
    "    plabels = np.argmax(prediction, axis=1)\n",
    "    tlabels = np.argmax(Y, axis=1)\n",
    "    acc = sum([1 if x==y else 0 for x,y in list(zip(tlabels, plabels))])/len(tlabels)\n",
    "    return acc, acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_H_n(X):\n",
    "    ans = X[:, -1, :]  # get last element from time dim\n",
    "    return ans\n",
    "\n",
    "\n",
    "def get_Y(X, xmaxlen):\n",
    "    return X[:, :xmaxlen, :]  # get first xmaxlen elem from time dim\n",
    "\n",
    "\n",
    "def get_R(X):\n",
    "    Y, alpha = X[0], X[1]\n",
    "    ans = K.T.batched_dot(Y, alpha)\n",
    "    return ans\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(opts):\n",
    "    k = 2 * opts['lstm_units']  # 200\n",
    "    L = opts['xmaxlen']  # 35\n",
    "    N = opts['xmaxlen'] + opts['ymaxlen']\n",
    "    \n",
    "    main_input = Input(shape=(N,), dtype='int32', name='main_input') #(N,70)\n",
    "    x = Embedding(output_dim=opts['emb'], input_dim=len(VOCABULARY.keys())+1, input_length=N, name='x')(main_input)\n",
    "    drop_out = Dropout(0.1, name='dropout')(x) # 70,50\n",
    "    lstm_fwd = LSTM(opts['lstm_units'], return_sequences=True, name='lstm_fwd')(drop_out)\n",
    "    lstm_bwd = LSTM(opts['lstm_units'], return_sequences=True, go_backwards=True, name='lstm_bwd')(drop_out)\n",
    "    #70,100\n",
    "    bilstm = merge([lstm_fwd, lstm_bwd], name='bilstm', mode='concat')\n",
    "    #70,200\n",
    "    drop_out = Dropout(0.1, name=\"d_bilstm\")(bilstm)\n",
    "    h_n = Lambda(get_H_n, output_shape=(k,), name=\"h_n\")(drop_out)\n",
    "    #200\n",
    "    \n",
    "    \n",
    "    Y = Lambda(get_Y, arguments={\"xmaxlen\": L}, name=\"Y\", output_shape=(L, k))(drop_out)\n",
    "    #35,200\n",
    "    Whn = Dense(k, W_regularizer=l2(0.01), name=\"Wh_n\")(h_n) #200\n",
    "    Whn_x_e = RepeatVector(L, name=\"Wh_n_x_e\")(Whn)#35,200\n",
    "    \n",
    "\n",
    "    \n",
    "    WY = TimeDistributed(Dense(k, W_regularizer=l2(0.01)), name=\"WY\")(Y)#35,200\n",
    "    merged = merge([Whn_x_e, WY], name=\"merged\", mode='sum')\n",
    "    M = Activation('tanh', name=\"M\")(merged)\n",
    "    #35,200\n",
    "\n",
    "    alpha_ = TimeDistributed(Dense(k, activation='linear'), name=\"alpha_\")(M)\n",
    "\n",
    "    flat_alpha = Flatten(name=\"flat_alpha\")(alpha_)\n",
    "    alpha = Dense(L, activation='softmax', name=\"alpha\")(flat_alpha) #35,200 Dense_33\n",
    "    \n",
    "    alpha = RepeatVector(k, name=\"alpha_rep\")(alpha)\n",
    "    \n",
    "    \n",
    "    Y_trans = Permute((2, 1), name=\"y_trans\")(Y)  # of shape (None,200,35)\n",
    "    \n",
    "    r = merge([Y_trans, alpha], output_shape=(k, 1), name=\"r_\")#, mode=get_R)\n",
    "    #200,35\n",
    "\n",
    "#     r = Reshape((k,), name=\"r\")(r_)\n",
    "    Wr = Dense(L, W_regularizer=l2(0.01), name=\"Dense_Wr\")(r) #200,35\n",
    "    Wh = Dense(k, W_regularizer=l2(0.01), name=\"Dense_Wh\")(Whn_x_e)\n",
    "    Wh = Permute((2, 1), name=\"Wh_trans\")(Wh)\n",
    "    \n",
    "    merged = merge([Wr, Wh], mode='sum')\n",
    "    h_star = Activation('tanh')(merged)\n",
    "    \n",
    "    flat_h_star = Flatten(name=\"flat_h_star\")(h_star)\n",
    "    out = Dense(3, activation='softmax')(flat_h_star)\n",
    "    \n",
    "    \n",
    "    output = out\n",
    "    model = Model(input=[main_input], output=output)\n",
    "    model.summary()\n",
    "    # plot(model, 'model.png')\n",
    "    # # model.compile(loss={'output':'binary_crossentropy'}, optimizer=Adam())\n",
    "    # model.compile(loss={'output':'categorical_crossentropy'}, optimizer=Adam(options.lr))\n",
    "    model.compile(loss='categorical_crossentropy',optimizer=Adam(opts['lr']))\n",
    "    return model\n",
    "\n",
    "# build_model(opts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Model - Load & save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def save_model(model, wtpath, archpath, mode='yaml'):\n",
    "    if mode == 'yaml':\n",
    "        yaml_string = model.to_yaml()\n",
    "        open(archpath, 'w').write(yaml_string)\n",
    "    else:\n",
    "        with open(archpath, 'w') as f:\n",
    "            f.write(model.to_json())\n",
    "    model.save_weights(wtpath)\n",
    "\n",
    "\n",
    "def load_model(wtpath, archpath, mode='yaml'):\n",
    "    if mode == 'yaml':\n",
    "        model = model_from_yaml(open(archpath).read())  # ,custom_objects={\"MyEmbedding\": MyEmbedding})\n",
    "    else:\n",
    "        with open(archpath) as f:\n",
    "            model = model_from_json(f.read())  # , custom_objects={\"MyEmbedding\": MyEmbedding})\n",
    "    model.load_weights(wtpath)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# one hot encoding\n",
    "def score_setup(row):\n",
    "    convert_dict = {\n",
    "      'ENTAILMENT': 0,\n",
    "      'NEUTRAL': 1,\n",
    "      'CONTRADICTION': 2\n",
    "    }\n",
    "    score = np.zeros((3,))\n",
    "    tag = row[\"entailment_judgment\"]\n",
    "    score[convert_dict[tag]] += 1\n",
    "    return score\n",
    "#     return convert_dict[row[\"entailment_judgment\"]]\n",
    "\n",
    "VOCABULARY = {'unk':0}\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "tokenizer = Tokenizer()\n",
    "\n",
    "\n",
    "def split_data_into_scores(max_hyp, max_evi, file_name=\"data/training.txt\"):\n",
    "\n",
    "    global VOCABULARY, tokenizer\n",
    "    import csv\n",
    "    with open(file_name,\"r\") as data:\n",
    "        train = csv.DictReader(data , delimiter='\\t')\n",
    "        evi_sentences = np.empty((0,max_evi))\n",
    "        hyp_sentences = np.empty((0,max_hyp))\n",
    "        labels = []\n",
    "        scores = []\n",
    "        count = 1\n",
    "        for row in train:\n",
    "            hyp = row[\"sentence_A\"].lower()\n",
    "            evi = row[\"sentence_B\"].lower()\n",
    "            tokenizer.fit_on_texts([hyp])\n",
    "            tokenizer.fit_on_texts([evi])\n",
    "            hyp_seq = np.array(tokenizer.texts_to_sequences([hyp])[0])\n",
    "            \n",
    "            padded_hyp = np.pad(hyp_seq,\n",
    "                                (max_hyp-np.shape(hyp_seq)[0],0),\n",
    "                                       'constant',\n",
    "                                       constant_values=(0,))\n",
    "            hyp_sentences = np.append(hyp_sentences, [padded_hyp], axis=0)\n",
    "            count += 1\n",
    "            \n",
    "            evi_seq = np.array(tokenizer.texts_to_sequences([evi])[0])\n",
    "            padded_evi = np.pad(evi_seq,\n",
    "                               (max_evi-np.shape(evi_seq)[0],0),\n",
    "                                'constant',\n",
    "                                constant_values=(0,))\n",
    "            evi_sentences = np.append(evi_sentences, [padded_evi], axis=0)\n",
    "            labels.append(row[\"entailment_judgment\"])\n",
    "            scores.append(score_setup(row))\n",
    "        print(\"Vocabulary size: %s\" % str(len(tokenizer.word_counts.keys())+1))\n",
    "        VOCABULARY = tokenizer.word_index\n",
    "        VOCABULARY['unk'] = 0\n",
    "        print(np.shape(hyp_sentences))\n",
    "        print(np.shape(evi_sentences))\n",
    "        return hyp_sentences, evi_sentences, np.array(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### tokenizer scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# from keras.preprocessing.text import Tokenizer\n",
    "# t = Tokenizer()\n",
    "# fit_text = [\"The earth is an awesome place live\"]\n",
    "# t.fit_on_texts(fit_text)\n",
    "# print(t.word_index)\n",
    "# fit_text = [\"Ana has apples\"]\n",
    "# t.fit_on_texts(fit_text)\n",
    "# print(t.texts_to_sequences([\"Ana is an awesome apple\"]))\n",
    "# # \n",
    "# print(len(t.word_index.keys()))\n",
    "# # from keras.utils.np_utils import to_categorical\n",
    "# # print(to_categorical([1], num_classes=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# # from keras.preprocessing.sequence import TimeseriesGenerator, pad_sequences\n",
    "# import numpy as np\n",
    "# a=np.empty((0,))\n",
    "# for i in range(0,10):\n",
    "#     a = np.append(a, [1,2,3], axis=0)\n",
    "# print(a)\n",
    "\n",
    "# print(np.pad(a, (0,0), 'constant', constant_values=(0,)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model + options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "opts = {\n",
    "    'lstm_units': 150,\n",
    "    'xmaxlen': 35,\n",
    "    'ymaxlen': 35,\n",
    "    'emb': 100, #dimension of the embedding\n",
    "    'max_features': len(VOCABULARY.keys())+1, #vocabulary dim+1\n",
    "    'batch_size': 128,\n",
    "    'lr': 0.005,\n",
    "    'epochs': 100\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\python 36 64\\lib\\site-packages\\ipykernel_launcher.py:12: UserWarning: The `merge` function is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "  if sys.path[0] == '':\n",
      "f:\\python 36 64\\lib\\site-packages\\ipykernel_launcher.py:21: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(200, name=\"Wh_n\", kernel_regularizer=<keras.reg...)`\n",
      "f:\\python 36 64\\lib\\site-packages\\ipykernel_launcher.py:26: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(200, kernel_regularizer=<keras.reg...)`\n",
      "f:\\python 36 64\\lib\\site-packages\\ipykernel_launcher.py:27: UserWarning: The `merge` function is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "f:\\python 36 64\\lib\\site-packages\\ipykernel_launcher.py:41: UserWarning: The `merge` function is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "f:\\python 36 64\\lib\\site-packages\\ipykernel_launcher.py:45: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(35, name=\"Dense_Wr\", kernel_regularizer=<keras.reg...)`\n",
      "f:\\python 36 64\\lib\\site-packages\\ipykernel_launcher.py:46: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(200, name=\"Dense_Wh\", kernel_regularizer=<keras.reg...)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "main_input (InputLayer)          (None, 70)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "x (Embedding)                    (None, 70, 50)        116350      main_input[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "dropout (Dropout)                (None, 70, 50)        0           x[0][0]                          \n",
      "____________________________________________________________________________________________________\n",
      "lstm_fwd (LSTM)                  (None, 70, 100)       60400       dropout[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "lstm_bwd (LSTM)                  (None, 70, 100)       60400       dropout[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "bilstm (Merge)                   (None, 70, 200)       0           lstm_fwd[0][0]                   \n",
      "                                                                   lstm_bwd[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "d_bilstm (Dropout)               (None, 70, 200)       0           bilstm[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "h_n (Lambda)                     (None, 200)           0           d_bilstm[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "Y (Lambda)                       (None, 35, 200)       0           d_bilstm[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "Wh_n (Dense)                     (None, 200)           40200       h_n[0][0]                        \n",
      "____________________________________________________________________________________________________\n",
      "Wh_n_x_e (RepeatVector)          (None, 35, 200)       0           Wh_n[0][0]                       \n",
      "____________________________________________________________________________________________________\n",
      "WY (TimeDistributed)             (None, 35, 200)       40200       Y[0][0]                          \n",
      "____________________________________________________________________________________________________\n",
      "merged (Merge)                   (None, 35, 200)       0           Wh_n_x_e[0][0]                   \n",
      "                                                                   WY[0][0]                         \n",
      "____________________________________________________________________________________________________\n",
      "M (Activation)                   (None, 35, 200)       0           merged[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "alpha_ (TimeDistributed)         (None, 35, 200)       40200       M[0][0]                          \n",
      "____________________________________________________________________________________________________\n",
      "flat_alpha (Flatten)             (None, 7000)          0           alpha_[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "alpha (Dense)                    (None, 35)            245035      flat_alpha[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "y_trans (Permute)                (None, 200, 35)       0           Y[0][0]                          \n",
      "____________________________________________________________________________________________________\n",
      "alpha_rep (RepeatVector)         (None, 200, 35)       0           alpha[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "r_ (Merge)                       (None, 200, 35)       0           y_trans[0][0]                    \n",
      "                                                                   alpha_rep[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "Dense_Wh (Dense)                 (None, 35, 200)       40200       Wh_n_x_e[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "Dense_Wr (Dense)                 (None, 200, 35)       1260        r_[0][0]                         \n",
      "____________________________________________________________________________________________________\n",
      "Wh_trans (Permute)               (None, 200, 35)       0           Dense_Wh[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "merge_2 (Merge)                  (None, 200, 35)       0           Dense_Wr[0][0]                   \n",
      "                                                                   Wh_trans[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "activation_2 (Activation)        (None, 200, 35)       0           merge_2[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "flat_h_star (Flatten)            (None, 7000)          0           activation_2[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "dense_6 (Dense)                  (None, 3)             21003       flat_h_star[0][0]                \n",
      "====================================================================================================\n",
      "Total params: 665,248\n",
      "Trainable params: 665,248\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\python 36 64\\lib\\site-packages\\ipykernel_launcher.py:49: UserWarning: The `merge` function is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "f:\\python 36 64\\lib\\site-packages\\ipykernel_launcher.py:57: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=Tensor(\"de...)`\n"
     ]
    }
   ],
   "source": [
    "model = build_model(opts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Data manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 2185\n",
      "(4500, 35)\n",
      "(4500, 35)\n",
      "(4500, 35)\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.\n",
      "   4.  5. 13.  2.  6.  3.  1.  7.  8. 14. 15.  9.  2. 10.  3. 11. 12.]]\n",
      "out\n",
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.\n",
      "  4.  5. 13.  2.  6.  3.  1.  7.  8. 14. 15.  9.  2. 10.  3. 11. 12.]\n",
      "***\n",
      "(4500, 70)\n",
      "***\n",
      "Vocabulary size: 2217\n",
      "(500, 35)\n",
      "(500, 35)\n",
      "***\n",
      "(500, 70)\n",
      "***\n",
      "Vocabulary size: 2326\n",
      "(4927, 35)\n",
      "(4927, 35)\n"
     ]
    }
   ],
   "source": [
    "X_train,Y_train,Z_train=split_data_into_scores(opts['xmaxlen'], opts['ymaxlen'], \"data/training.txt\")\n",
    "print(np.shape(X_train))\n",
    "print(X_train[:1])\n",
    "\n",
    "print(\"out\")\n",
    "print(X_train[0])\n",
    "xy_train = np.concatenate((X_train, Y_train), axis=1)\n",
    "print(\"***\")\n",
    "print(np.shape(xy_train))\n",
    "print(\"***\")\n",
    "\n",
    "X_dev,Y_dev,Z_dev=split_data_into_scores(opts['xmaxlen'], opts['ymaxlen'], \"data/dev.txt\")\n",
    "xy_dev = np.concatenate((X_dev, Y_dev), axis=1)\n",
    "print(\"***\")\n",
    "print(np.shape(xy_dev))\n",
    "print(\"***\")\n",
    "\n",
    "X_test,Y_test,Z_test=split_data_into_scores(opts['xmaxlen'], opts['ymaxlen'], \"data/test_labeled.txt\")\n",
    "xy_test = np.concatenate((X_test, Y_test), axis=1)\n",
    "\n",
    "train_dict = {'input': xy_train, 'output': Z_train}\n",
    "dev_dict = {'input': xy_dev, 'output': Z_dev}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "2/2 [==============================] - 1s - loss: 7.4762\n"
     ]
    }
   ],
   "source": [
    "# history = model.fit(xy_train[:2],Z_train[:2])#,\n",
    "#                             batch_size=opts['batch_size'])\n",
    "#                             epochs=opts['epochs'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/1\n",
      "4500/4500 [==============================] - 46s - loss: 7.0350 - val_loss: 7.0278\n",
      "(0.5635555555555556, 0.5635555555555556)\n",
      "(0.564, 0.564)\n",
      "(0.5668763953724376, 0.5668763953724376)\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(xy_train,Z_train,\n",
    "                            batch_size=opts['batch_size'],\n",
    "                            epochs=opts['epochs'],\n",
    "                            validation_data=(xy_dev, Z_dev),\n",
    "                            callbacks=[\n",
    "                                AccCallBack(xy_train, Z_train, xy_dev, Z_dev, xy_test, Z_test, VOCABULARY, opts),\n",
    "                                EarlyStopping()\n",
    "                            ]\n",
    "                            )\n",
    "\n",
    "train_acc = compute_acc(xy_train, Z_train, VOCABULARY, model, opts)\n",
    "dev_acc = compute_acc(xy_dev, Z_dev, VOCABULARY, model, opts)\n",
    "test_acc = compute_acc(xy_test, Z_test, VOCABULARY, model, opts)\n",
    "print(train_acc)\n",
    "print(dev_acc)\n",
    "print(test_acc)\n",
    "\n",
    "opts_name = \"opts-1\"\n",
    "save_model(model, 'model_weights-%s-%s.weights' % (str(opts_name), str(test_acc[0])),\n",
    "           'model_arch_att-%s-%s.yaml' % (str(opts_name), str(test_acc[0])))\n",
    "with open(opts_name, \"w\") as f:\n",
    "    f.write(str(opts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.5635555555555556, 0.5635555555555556)\n",
      "(0.564, 0.564)\n",
      "(0.5668763953724376, 0.5668763953724376)\n"
     ]
    }
   ],
   "source": [
    "# load_model('model_weights.weights', 'model_arch_att.yaml')\n",
    "train_acc = compute_acc(xy_train, Z_train, VOCABULARY, model, opts)\n",
    "dev_acc = compute_acc(xy_dev, Z_dev, VOCABULARY, model, opts)\n",
    "test_acc = compute_acc(xy_test, Z_test, VOCABULARY, model, opts)\n",
    "\n",
    "print(train_acc)\n",
    "print(dev_acc)\n",
    "print(test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
