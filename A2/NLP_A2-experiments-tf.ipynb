{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### installs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata: ...working... done\n",
      "Solving environment: ...working... done\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: F:\\Anaconda3\n",
      "\n",
      "  added / updated specs:\n",
      "    - tqdm\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    ca-certificates-2019.1.23  |                0         158 KB\n",
      "    certifi-2019.3.9           |           py36_0         156 KB\n",
      "    cryptography-2.6.1         |   py36h7a1dbc1_0         563 KB\n",
      "    kiwisolver-1.0.1           |   py36h6538335_0          61 KB\n",
      "    krb5-1.16.1                |       hc04afaa_7         819 KB\n",
      "    libcurl-7.64.0             |       h2a8f88b_2         283 KB\n",
      "    libpng-1.6.36              |       h2a8f88b_0         550 KB\n",
      "    openssl-1.1.1b             |       he774522_1         5.7 MB\n",
      "    pycurl-7.43.0.2            |   py36h7a1dbc1_0         182 KB\n",
      "    pyqt-5.9.2                 |   py36h6538335_2         4.2 MB\n",
      "    qt-5.9.7                   |   vc14h73c81de_0        92.3 MB\n",
      "    sqlite-3.27.2              |       he774522_0         941 KB\n",
      "    tk-8.6.8                   |       hfa6e2cd_0         3.8 MB\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:       109.6 MB\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "  kiwisolver         pkgs/main/win-64::kiwisolver-1.0.1-py36h6538335_0\n",
      "  krb5               pkgs/main/win-64::krb5-1.16.1-hc04afaa_7\n",
      "  libcurl            pkgs/main/win-64::libcurl-7.64.0-h2a8f88b_2\n",
      "  tqdm               pkgs/main/noarch::tqdm-4.31.1-py_0\n",
      "\n",
      "The following packages will be UPDATED:\n",
      "\n",
      "  ca-certificates                     2017.08.26-h94faf87_0 --> 2019.1.23-0\n",
      "  certifi                        2017.7.27.1-py36h043bc9e_0 --> 2019.3.9-py36_0\n",
      "  cryptography                         2.0.3-py36h123decb_1 --> 2.6.1-py36h7a1dbc1_0\n",
      "  libpng                              1.6.32-vc14h5163883_3 --> 1.6.36-h2a8f88b_0\n",
      "  matplotlib                           2.1.0-py36h11b4b9c_0 --> 2.2.2-py36h153e9ff_1\n",
      "  openssl                             1.0.2l-vc14hcac20b0_2 --> 1.1.1b-he774522_1\n",
      "  pycurl                              7.43.0-py36h086bf4c_3 --> 7.43.0.2-py36h7a1dbc1_0\n",
      "  pyqt                                 5.6.0-py36hb5ed885_5 --> 5.9.2-py36h6538335_2\n",
      "  qt                                  5.6.2-vc14h6f8c307_12 --> 5.9.7-vc14h73c81de_0\n",
      "  sip                                 4.18.1-py36h9c25514_2 --> 4.19.8-py36h6538335_0\n",
      "  sqlite                              3.20.1-vc14h7ce8c62_1 --> 3.27.2-he774522_0\n",
      "  tk                                   8.6.7-vc14hb68737d_1 --> 8.6.8-hfa6e2cd_0\n",
      "\n",
      "\n",
      "\n",
      "Downloading and Extracting Packages\n",
      "\n",
      "ca-certificates-2019 | 158 KB    |            |   0% \n",
      "ca-certificates-2019 | 158 KB    | ########## | 100% \n",
      "\n",
      "krb5-1.16.1          | 819 KB    |            |   0% \n",
      "krb5-1.16.1          | 819 KB    | ######5    |  66% \n",
      "krb5-1.16.1          | 819 KB    | #########9 |  99% \n",
      "krb5-1.16.1          | 819 KB    | ########## | 100% \n",
      "\n",
      "libcurl-7.64.0       | 283 KB    |            |   0% \n",
      "libcurl-7.64.0       | 283 KB    | ########## | 100% \n",
      "\n",
      "kiwisolver-1.0.1     | 61 KB     |            |   0% \n",
      "kiwisolver-1.0.1     | 61 KB     | ########## | 100% \n",
      "\n",
      "pyqt-5.9.2           | 4.2 MB    |            |   0% \n",
      "pyqt-5.9.2           | 4.2 MB    | #2         |  13% \n",
      "pyqt-5.9.2           | 4.2 MB    | ##8        |  28% \n",
      "pyqt-5.9.2           | 4.2 MB    | ####2      |  42% \n",
      "pyqt-5.9.2           | 4.2 MB    | #####9     |  59% \n",
      "pyqt-5.9.2           | 4.2 MB    | #######5   |  75% \n",
      "pyqt-5.9.2           | 4.2 MB    | ########7  |  88% \n",
      "pyqt-5.9.2           | 4.2 MB    | #########7 |  98% \n",
      "pyqt-5.9.2           | 4.2 MB    | ########## | 100% \n",
      "\n",
      "libpng-1.6.36        | 550 KB    |            |   0% \n",
      "libpng-1.6.36        | 550 KB    | #######9   |  79% \n",
      "libpng-1.6.36        | 550 KB    | ########## | 100% \n",
      "\n",
      "openssl-1.1.1b       | 5.7 MB    |            |   0% \n",
      "openssl-1.1.1b       | 5.7 MB    | 8          |   8% \n",
      "openssl-1.1.1b       | 5.7 MB    | #7         |  18% \n",
      "openssl-1.1.1b       | 5.7 MB    | ##9        |  30% \n",
      "openssl-1.1.1b       | 5.7 MB    | ####1      |  41% \n",
      "openssl-1.1.1b       | 5.7 MB    | #####      |  50% \n",
      "openssl-1.1.1b       | 5.7 MB    | ######2    |  62% \n",
      "openssl-1.1.1b       | 5.7 MB    | #######2   |  72% \n",
      "openssl-1.1.1b       | 5.7 MB    | ########1  |  82% \n",
      "openssl-1.1.1b       | 5.7 MB    | ########9  |  90% \n",
      "openssl-1.1.1b       | 5.7 MB    | #########6 |  96% \n",
      "openssl-1.1.1b       | 5.7 MB    | ########## | 100% \n",
      "\n",
      "certifi-2019.3.9     | 156 KB    |            |   0% \n",
      "certifi-2019.3.9     | 156 KB    | ########## | 100% \n",
      "\n",
      "cryptography-2.6.1   | 563 KB    |            |   0% \n",
      "cryptography-2.6.1   | 563 KB    | #9         |  19% \n",
      "cryptography-2.6.1   | 563 KB    | ########4  |  85% \n",
      "cryptography-2.6.1   | 563 KB    | ########## | 100% \n",
      "\n",
      "sqlite-3.27.2        | 941 KB    |            |   0% \n",
      "sqlite-3.27.2        | 941 KB    | ####5      |  46% \n",
      "sqlite-3.27.2        | 941 KB    | ########8  |  89% \n",
      "sqlite-3.27.2        | 941 KB    | ########## | 100% \n",
      "\n",
      "tk-8.6.8             | 3.8 MB    |            |   0% \n",
      "tk-8.6.8             | 3.8 MB    | 8          |   9% \n",
      "tk-8.6.8             | 3.8 MB    | ##6        |  27% \n",
      "tk-8.6.8             | 3.8 MB    | ####5      |  45% \n",
      "tk-8.6.8             | 3.8 MB    | #####9     |  60% \n",
      "tk-8.6.8             | 3.8 MB    | #######3   |  74% \n",
      "tk-8.6.8             | 3.8 MB    | ########5  |  86% \n",
      "tk-8.6.8             | 3.8 MB    | #########5 |  96% \n",
      "tk-8.6.8             | 3.8 MB    | ########## | 100% \n",
      "\n",
      "pycurl-7.43.0.2      | 182 KB    |            |   0% \n",
      "pycurl-7.43.0.2      | 182 KB    | ########## | 100% \n",
      "\n",
      "qt-5.9.7             | 92.3 MB   |            |   0% \n",
      "qt-5.9.7             | 92.3 MB   |            |   0% \n",
      "qt-5.9.7             | 92.3 MB   | 1          |   1% \n",
      "qt-5.9.7             | 92.3 MB   | 1          |   2% \n",
      "qt-5.9.7             | 92.3 MB   | 2          |   3% \n",
      "qt-5.9.7             | 92.3 MB   | 3          |   4% \n",
      "qt-5.9.7             | 92.3 MB   | 4          |   4% \n",
      "qt-5.9.7             | 92.3 MB   | 5          |   5% \n",
      "qt-5.9.7             | 92.3 MB   | 5          |   6% \n",
      "qt-5.9.7             | 92.3 MB   | 6          |   7% \n",
      "qt-5.9.7             | 92.3 MB   | 7          |   7% \n",
      "qt-5.9.7             | 92.3 MB   | 8          |   8% \n",
      "qt-5.9.7             | 92.3 MB   | 8          |   9% \n",
      "qt-5.9.7             | 92.3 MB   | 9          |  10% \n",
      "qt-5.9.7             | 92.3 MB   | #          |  11% \n",
      "qt-5.9.7             | 92.3 MB   | #1         |  11% \n",
      "qt-5.9.7             | 92.3 MB   | #2         |  12% \n",
      "qt-5.9.7             | 92.3 MB   | #3         |  13% \n",
      "qt-5.9.7             | 92.3 MB   | #3         |  14% \n",
      "qt-5.9.7             | 92.3 MB   | #4         |  15% \n",
      "qt-5.9.7             | 92.3 MB   | #5         |  16% \n",
      "qt-5.9.7             | 92.3 MB   | #6         |  16% \n",
      "qt-5.9.7             | 92.3 MB   | #7         |  17% \n",
      "qt-5.9.7             | 92.3 MB   | #8         |  18% \n",
      "qt-5.9.7             | 92.3 MB   | #8         |  19% \n",
      "qt-5.9.7             | 92.3 MB   | #9         |  20% \n",
      "qt-5.9.7             | 92.3 MB   | ##         |  20% \n",
      "qt-5.9.7             | 92.3 MB   | ##1        |  21% \n",
      "qt-5.9.7             | 92.3 MB   | ##1        |  22% \n",
      "qt-5.9.7             | 92.3 MB   | ##2        |  23% \n",
      "qt-5.9.7             | 92.3 MB   | ##3        |  24% \n",
      "qt-5.9.7             | 92.3 MB   | ##4        |  25% \n",
      "qt-5.9.7             | 92.3 MB   | ##5        |  26% \n",
      "qt-5.9.7             | 92.3 MB   | ##6        |  27% \n",
      "qt-5.9.7             | 92.3 MB   | ##7        |  27% \n",
      "qt-5.9.7             | 92.3 MB   | ##8        |  28% \n",
      "qt-5.9.7             | 92.3 MB   | ##9        |  29% \n",
      "qt-5.9.7             | 92.3 MB   | ###        |  30% \n",
      "qt-5.9.7             | 92.3 MB   | ###        |  31% \n",
      "qt-5.9.7             | 92.3 MB   | ###1       |  32% \n",
      "qt-5.9.7             | 92.3 MB   | ###2       |  32% \n",
      "qt-5.9.7             | 92.3 MB   | ###3       |  33% \n",
      "qt-5.9.7             | 92.3 MB   | ###3       |  34% \n",
      "qt-5.9.7             | 92.3 MB   | ###4       |  35% \n",
      "qt-5.9.7             | 92.3 MB   | ###5       |  35% \n",
      "qt-5.9.7             | 92.3 MB   | ###6       |  36% \n",
      "qt-5.9.7             | 92.3 MB   | ###6       |  37% \n",
      "qt-5.9.7             | 92.3 MB   | ###7       |  38% \n",
      "qt-5.9.7             | 92.3 MB   | ###8       |  38% \n",
      "qt-5.9.7             | 92.3 MB   | ###9       |  39% \n",
      "qt-5.9.7             | 92.3 MB   | ####       |  40% \n",
      "qt-5.9.7             | 92.3 MB   | ####       |  41% \n",
      "qt-5.9.7             | 92.3 MB   | ####1      |  42% \n",
      "qt-5.9.7             | 92.3 MB   | ####2      |  43% \n",
      "qt-5.9.7             | 92.3 MB   | ####3      |  44% \n",
      "qt-5.9.7             | 92.3 MB   | ####4      |  45% \n",
      "qt-5.9.7             | 92.3 MB   | ####5      |  45% \n",
      "qt-5.9.7             | 92.3 MB   | ####6      |  46% \n",
      "qt-5.9.7             | 92.3 MB   | ####7      |  47% \n",
      "qt-5.9.7             | 92.3 MB   | ####8      |  48% \n",
      "qt-5.9.7             | 92.3 MB   | ####8      |  49% \n",
      "qt-5.9.7             | 92.3 MB   | ####9      |  50% \n",
      "qt-5.9.7             | 92.3 MB   | #####      |  50% \n",
      "qt-5.9.7             | 92.3 MB   | #####1     |  51% \n",
      "qt-5.9.7             | 92.3 MB   | #####2     |  52% \n",
      "qt-5.9.7             | 92.3 MB   | #####2     |  53% \n",
      "qt-5.9.7             | 92.3 MB   | #####3     |  54% \n",
      "qt-5.9.7             | 92.3 MB   | #####4     |  55% \n",
      "qt-5.9.7             | 92.3 MB   | #####5     |  55% \n",
      "qt-5.9.7             | 92.3 MB   | #####6     |  56% \n",
      "qt-5.9.7             | 92.3 MB   | #####7     |  57% \n",
      "qt-5.9.7             | 92.3 MB   | #####8     |  58% \n",
      "qt-5.9.7             | 92.3 MB   | #####8     |  59% \n",
      "qt-5.9.7             | 92.3 MB   | #####9     |  60% \n",
      "qt-5.9.7             | 92.3 MB   | ######     |  61% \n",
      "qt-5.9.7             | 92.3 MB   | ######1    |  61% \n",
      "qt-5.9.7             | 92.3 MB   | ######2    |  62% \n",
      "qt-5.9.7             | 92.3 MB   | ######2    |  63% \n",
      "qt-5.9.7             | 92.3 MB   | ######3    |  64% \n",
      "qt-5.9.7             | 92.3 MB   | ######4    |  65% \n",
      "qt-5.9.7             | 92.3 MB   | ######5    |  66% \n",
      "qt-5.9.7             | 92.3 MB   | ######6    |  66% \n",
      "qt-5.9.7             | 92.3 MB   | ######7    |  67% \n",
      "qt-5.9.7             | 92.3 MB   | ######8    |  68% \n",
      "qt-5.9.7             | 92.3 MB   | ######8    |  69% \n",
      "qt-5.9.7             | 92.3 MB   | ######9    |  70% \n",
      "qt-5.9.7             | 92.3 MB   | #######    |  71% \n",
      "qt-5.9.7             | 92.3 MB   | #######1   |  71% \n",
      "qt-5.9.7             | 92.3 MB   | #######2   |  72% \n",
      "qt-5.9.7             | 92.3 MB   | #######2   |  73% \n",
      "qt-5.9.7             | 92.3 MB   | #######3   |  74% \n",
      "qt-5.9.7             | 92.3 MB   | #######4   |  74% \n",
      "qt-5.9.7             | 92.3 MB   | #######5   |  75% \n",
      "qt-5.9.7             | 92.3 MB   | #######5   |  76% \n",
      "qt-5.9.7             | 92.3 MB   | #######6   |  76% \n",
      "qt-5.9.7             | 92.3 MB   | #######6   |  77% \n",
      "qt-5.9.7             | 92.3 MB   | #######6   |  77% \n",
      "qt-5.9.7             | 92.3 MB   | #######7   |  77% \n",
      "qt-5.9.7             | 92.3 MB   | #######7   |  77% \n",
      "qt-5.9.7             | 92.3 MB   | #######7   |  78% \n",
      "qt-5.9.7             | 92.3 MB   | #######7   |  78% \n",
      "qt-5.9.7             | 92.3 MB   | #######8   |  78% \n",
      "qt-5.9.7             | 92.3 MB   | #######8   |  78% \n",
      "qt-5.9.7             | 92.3 MB   | #######8   |  79% \n",
      "qt-5.9.7             | 92.3 MB   | #######9   |  79% \n",
      "qt-5.9.7             | 92.3 MB   | #######9   |  79% \n",
      "qt-5.9.7             | 92.3 MB   | #######9   |  80% \n",
      "qt-5.9.7             | 92.3 MB   | #######9   |  80% \n",
      "qt-5.9.7             | 92.3 MB   | ########   |  80% \n",
      "qt-5.9.7             | 92.3 MB   | ########   |  81% \n",
      "qt-5.9.7             | 92.3 MB   | ########   |  81% \n",
      "qt-5.9.7             | 92.3 MB   | ########1  |  81% \n",
      "qt-5.9.7             | 92.3 MB   | ########1  |  81% \n",
      "qt-5.9.7             | 92.3 MB   | ########1  |  82% \n",
      "qt-5.9.7             | 92.3 MB   | ########1  |  82% \n",
      "qt-5.9.7             | 92.3 MB   | ########2  |  82% \n",
      "qt-5.9.7             | 92.3 MB   | ########2  |  83% \n",
      "qt-5.9.7             | 92.3 MB   | ########2  |  83% \n",
      "qt-5.9.7             | 92.3 MB   | ########3  |  83% \n",
      "qt-5.9.7             | 92.3 MB   | ########3  |  84% \n",
      "qt-5.9.7             | 92.3 MB   | ########3  |  84% \n",
      "qt-5.9.7             | 92.3 MB   | ########4  |  84% \n",
      "qt-5.9.7             | 92.3 MB   | ########4  |  85% \n",
      "qt-5.9.7             | 92.3 MB   | ########4  |  85% \n",
      "qt-5.9.7             | 92.3 MB   | ########5  |  85% \n",
      "qt-5.9.7             | 92.3 MB   | ########5  |  85% \n",
      "qt-5.9.7             | 92.3 MB   | ########5  |  86% \n",
      "qt-5.9.7             | 92.3 MB   | ########5  |  86% \n",
      "qt-5.9.7             | 92.3 MB   | ########6  |  86% \n",
      "qt-5.9.7             | 92.3 MB   | ########6  |  87% \n",
      "qt-5.9.7             | 92.3 MB   | ########6  |  87% \n",
      "qt-5.9.7             | 92.3 MB   | ########7  |  87% \n",
      "qt-5.9.7             | 92.3 MB   | ########7  |  87% \n",
      "qt-5.9.7             | 92.3 MB   | ########7  |  88% \n",
      "qt-5.9.7             | 92.3 MB   | ########7  |  88% \n",
      "qt-5.9.7             | 92.3 MB   | ########8  |  88% \n",
      "qt-5.9.7             | 92.3 MB   | ########8  |  89% \n",
      "qt-5.9.7             | 92.3 MB   | ########8  |  89% \n",
      "qt-5.9.7             | 92.3 MB   | ########9  |  89% \n",
      "qt-5.9.7             | 92.3 MB   | ########9  |  89% \n",
      "qt-5.9.7             | 92.3 MB   | ########9  |  90% \n",
      "qt-5.9.7             | 92.3 MB   | ########9  |  90% \n",
      "qt-5.9.7             | 92.3 MB   | #########  |  90% \n",
      "qt-5.9.7             | 92.3 MB   | #########  |  91% \n",
      "qt-5.9.7             | 92.3 MB   | #########  |  91% \n",
      "qt-5.9.7             | 92.3 MB   | #########1 |  91% \n",
      "qt-5.9.7             | 92.3 MB   | #########1 |  91% \n",
      "qt-5.9.7             | 92.3 MB   | #########1 |  92% \n",
      "qt-5.9.7             | 92.3 MB   | #########1 |  92% \n",
      "qt-5.9.7             | 92.3 MB   | #########2 |  92% \n",
      "qt-5.9.7             | 92.3 MB   | #########2 |  92% \n",
      "qt-5.9.7             | 92.3 MB   | #########2 |  93% \n",
      "qt-5.9.7             | 92.3 MB   | #########3 |  93% \n",
      "qt-5.9.7             | 92.3 MB   | #########3 |  93% \n",
      "qt-5.9.7             | 92.3 MB   | #########3 |  94% \n",
      "qt-5.9.7             | 92.3 MB   | #########4 |  94% \n",
      "qt-5.9.7             | 92.3 MB   | #########4 |  94% \n",
      "qt-5.9.7             | 92.3 MB   | #########4 |  95% \n",
      "qt-5.9.7             | 92.3 MB   | #########4 |  95% \n",
      "qt-5.9.7             | 92.3 MB   | #########5 |  95% \n",
      "qt-5.9.7             | 92.3 MB   | #########5 |  95% \n",
      "qt-5.9.7             | 92.3 MB   | #########5 |  95% \n",
      "qt-5.9.7             | 92.3 MB   | #########5 |  96% \n",
      "qt-5.9.7             | 92.3 MB   | #########5 |  96% \n",
      "qt-5.9.7             | 92.3 MB   | #########6 |  96% \n",
      "qt-5.9.7             | 92.3 MB   | #########6 |  96% \n",
      "qt-5.9.7             | 92.3 MB   | #########6 |  97% \n",
      "qt-5.9.7             | 92.3 MB   | #########7 |  97% \n",
      "qt-5.9.7             | 92.3 MB   | #########7 |  97% \n",
      "qt-5.9.7             | 92.3 MB   | #########7 |  98% \n",
      "qt-5.9.7             | 92.3 MB   | #########7 |  98% \n",
      "qt-5.9.7             | 92.3 MB   | #########8 |  98% \n",
      "qt-5.9.7             | 92.3 MB   | #########8 |  99% \n",
      "qt-5.9.7             | 92.3 MB   | #########8 |  99% \n",
      "qt-5.9.7             | 92.3 MB   | #########9 |  99% \n",
      "qt-5.9.7             | 92.3 MB   | #########9 |  99% \n",
      "qt-5.9.7             | 92.3 MB   | #########9 | 100% \n",
      "qt-5.9.7             | 92.3 MB   | #########9 | 100% \n",
      "qt-5.9.7             | 92.3 MB   | #########9 | 100% \n",
      "qt-5.9.7             | 92.3 MB   | ########## | 100% \n",
      "Preparing transaction: ...working... done\n",
      "Verifying transaction: ...working... done\n",
      "Executing transaction: ...working... done\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata: ...working... done\n",
      "Solving environment: ...working... done\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# import sys\n",
    "# !conda install --yes --prefix {sys.prefix} tqdm\n",
    "# !conda install --yes --prefix {sys.prefix} tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# import sys\n",
    "# !conda install --yes --prefix {sys.prefix} ipdb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import urllib\n",
    "import sys\n",
    "import os\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download Glove word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_zip_file = \"data/glove.6B.zip\"\n",
    "glove_vectors_file = \"data/glove.6B.100d.txt\"\n",
    "import zipfile, urllib.request, shutil, os\n",
    "    \n",
    "# #large file - 862 MB\n",
    "# if (not os.path.isfile(glove_zip_file) and\n",
    "#     not os.path.isfile(glove_vectors_file)):\n",
    "#     with urllib.request.urlopen(\"http://nlp.stanford.edu/data/glove.6B.zip\") as response, open(glove_zip_file, 'wb') as out_file:\n",
    "#         shutil.copyfileobj(response, out_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unzip word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unzip_single_file(zip_file_name, output_file_name):\n",
    "    \"\"\"\n",
    "        If the outFile is already created, don't recreate\n",
    "        If the outFile does not exist, create it from the zipFile\n",
    "    \"\"\"\n",
    "    if not os.path.isfile(output_file_name):\n",
    "        with open(output_file_name, 'wb') as out_file:\n",
    "            with zipfile.ZipFile(zip_file_name) as zipped:\n",
    "                for info in zipped.infolist():\n",
    "                    if output_file_name in info.filename:\n",
    "                        with zipped.open(info) as requested_file:\n",
    "                            out_file.write(requested_file.read())\n",
    "                            return\n",
    "\n",
    "unzip_single_file(glove_zip_file, glove_vectors_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_wordmap = {}\n",
    "with open(glove_vectors_file, \"r\", encoding=\"utf8\") as glove:\n",
    "    for line in glove:\n",
    "        name, vector = tuple(line.split(\" \", 1))\n",
    "        glove_wordmap[name] = np.fromstring(vector, sep=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n"
     ]
    }
   ],
   "source": [
    "print(glove_wordmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence2sequence(sentence):\n",
    "    \"\"\"\n",
    "     \n",
    "    - Turns an input sentence into an (n,d) matrix, \n",
    "        where n is the number of tokens in the sentence\n",
    "        and d is the number of dimensions each word vector has.\n",
    "    \n",
    "      Tensorflow doesn't need to be used here, as simply\n",
    "      turning the sentence into a sequence based off our \n",
    "      mapping does not need the computational power that\n",
    "      Tensorflow provides. Normal Python suffices for this task.\n",
    "    \"\"\"\n",
    "    tokens = sentence.lower().split(\" \")\n",
    "    rows = []\n",
    "    words = []\n",
    "    #Greedy search for tokens\n",
    "    for token in tokens:\n",
    "        i = len(token)\n",
    "        while len(token) > 0 and i > 0:\n",
    "            word = token[:i]\n",
    "            if word in glove_wordmap:\n",
    "                rows.append(glove_wordmap[word])\n",
    "                words.append(word)\n",
    "                token = token[i:]\n",
    "                i = len(token)\n",
    "            else:\n",
    "                i = i-1\n",
    "    return rows, words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXgAAAD8CAYAAAB9y7/cAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nFeUAb2ZWUf8fFE7QkIne2f0AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nWcU50JuZVZwDvZlZxTnQm5lV3P8HUZAkzQS1u/EAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def visualize(sentence):\n",
    "    rows, words = sentence2sequence(sentence)\n",
    "    mat = np.vstack(rows)\n",
    "    \n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    shown = ax.matshow(mat, aspect=\"auto\")\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    fig.colorbar(shown)\n",
    "    \n",
    "    ax.set_yticklabels([\"\"]+words)\n",
    "    plt.show()\n",
    "    \n",
    "visualize(\"The quick brown fox jumped over the lazy dog.\")\n",
    "visualize(\"The pretty flowers shone in the sunlight.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "display_step = 10\n",
    "\n",
    "def score_setup(row):\n",
    "    convert_dict = {\n",
    "      'ENTAILMENT': 0,\n",
    "      'NEUTRAL': 1,\n",
    "      'CONTRADICTION': 2\n",
    "    }\n",
    "    score = np.zeros((3,))\n",
    "    tag = row[\"entailment_judgment\"]\n",
    "    score[convert_dict[tag]] += 1\n",
    "    return score\n",
    "\n",
    "def fit_to_size(matrix, shape):\n",
    "    res = np.zeros(shape)\n",
    "    slices = [slice(0,min(dim,shape[e])) for e, dim in enumerate(matrix.shape)]\n",
    "    res[slices] = matrix[slices]\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Constants setup\n",
    "max_hypothesis_length, max_evidence_length = 30, 30\n",
    "batch_size, vector_size, hidden_size = 128, 100, 64\n",
    "\n",
    "training_iterations_count = 100000\n",
    "\n",
    "lstm_size = hidden_size\n",
    "\n",
    "weight_decay = 0.0005\n",
    "\n",
    "learning_rate = 1\n",
    "\n",
    "input_p, output_p = 0.6, 0.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Training data](http://www.site.uottawa.ca/~diana/csi5386/A2_2019/SICK_train.txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\python 36 64\\lib\\site-packages\\ipykernel_launcher.py:20: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def split_data_into_scores():\n",
    "    import csv\n",
    "    with open(\"data/training.txt\",\"r\") as data:\n",
    "        train = csv.DictReader(data , delimiter='\\t')\n",
    "        evi_sentences = []\n",
    "        hyp_sentences = []\n",
    "        labels = []\n",
    "        scores = []\n",
    "        count = 1\n",
    "        for row in train:\n",
    "            hyp_sentences.append(np.vstack(\n",
    "                    sentence2sequence(row[\"sentence_A\"].lower())[0]))\n",
    "            evi_sentences.append(np.vstack(\n",
    "                    sentence2sequence(row[\"sentence_B\"].lower())[0]))\n",
    "            labels.append(row[\"entailment_judgment\"])\n",
    "            scores.append(score_setup(row))\n",
    "        \n",
    "        hyp_sentences = np.stack([fit_to_size(x, (max_hypothesis_length, vector_size))\n",
    "                          for x in hyp_sentences])\n",
    "        evi_sentences = np.stack([fit_to_size(x, (max_evidence_length, vector_size))\n",
    "                          for x in evi_sentences])\n",
    "                                 \n",
    "        return (hyp_sentences, evi_sentences), labels, np.array(scores)\n",
    "data_feature_list, correct_values, correct_scores = split_data_into_scores()\n",
    "\n",
    "l_h, l_e = max_hypothesis_length, max_evidence_length\n",
    "N, D, H = batch_size, vector_size, hidden_size\n",
    "l_seq = l_h + l_e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Cell used for development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    tf.reset_default_graph() \n",
    "    data_feature_list, correct_values, correct_scores = split_data_into_scores()\n",
    "\n",
    "    l_h, l_e = max_hypothesis_length, max_evidence_length\n",
    "    N, D, H = batch_size, vector_size, hidden_size\n",
    "    l_seq = l_h + l_e\n",
    "    lstm = tf.contrib.rnn.BasicLSTMCell(lstm_size)\n",
    "    lstm_drop =  tf.contrib.rnn.DropoutWrapper(lstm, input_p, output_p)\n",
    "    hyp = tf.placeholder(tf.float32, [N, l_h, D], 'hypothesis')\n",
    "    evi = tf.placeholder(tf.float32, [N, l_e, D], 'evidence')\n",
    "    y = tf.placeholder(tf.float32, [N, 3], 'label')\n",
    "    lstm_back = tf.contrib.rnn.BasicLSTMCell(lstm_size)\n",
    "\n",
    "    lstm_drop_back = tf.contrib.rnn.DropoutWrapper(lstm_back, input_p, output_p)\n",
    "\n",
    "    fc_initializer = tf.random_normal_initializer(stddev=0.1) \n",
    "    fc_weight = tf.get_variable('fc_weight', [2*hidden_size, 3], \n",
    "                            initializer = fc_initializer)\n",
    "    fc_bias = tf.get_variable('bias', [3])\n",
    "    tf.add_to_collection(tf.GraphKeys.REGULARIZATION_LOSSES, \n",
    "                     tf.nn.l2_loss(fc_weight)) \n",
    "\n",
    "    x = tf.concat([hyp, evi], 1) # N, (Lh+Le), d\n",
    "    x = tf.transpose(x, [1, 0, 2]) # (Le+Lh), N, d\n",
    "    x = tf.reshape(x, [-1, vector_size]) # (Le+Lh)*N, d\n",
    "    x = tf.split(x, l_seq,)\n",
    "    rnn_outputs, _, _ = tf.contrib.rnn.static_bidirectional_rnn(lstm, lstm_back,\n",
    "                                                            x, dtype=tf.float32)\n",
    "\n",
    "    classification_scores = tf.matmul(rnn_outputs[-1], fc_weight) + fc_bias\n",
    "\n",
    "    with tf.variable_scope('Accuracy'):\n",
    "        predicts = tf.cast(tf.argmax(classification_scores, 1), 'int32')\n",
    "        y_label = tf.cast(tf.argmax(y, 1), 'int32')\n",
    "        corrects = tf.equal(predicts, y_label)\n",
    "        num_corrects = tf.reduce_sum(tf.cast(corrects, tf.float32))\n",
    "        accuracy = tf.reduce_mean(tf.cast(corrects, tf.float32))\n",
    "\n",
    "    with tf.variable_scope(\"loss\"):\n",
    "        cross_entropy = tf.nn.softmax_cross_entropy_with_logits(\n",
    "            logits = classification_scores, labels = y)\n",
    "        loss = tf.reduce_mean(cross_entropy)\n",
    "        total_loss = loss + weight_decay * tf.add_n(\n",
    "            tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES))\n",
    "\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "\n",
    "    opt_op = optimizer.minimize(total_loss)\n",
    "    # Initialize variables\n",
    "    init = tf.global_variables_initializer()\n",
    "\n",
    "    # Use TQDM if installed\n",
    "    tqdm_installed = False\n",
    "    try:\n",
    "        from tqdm import tqdm\n",
    "        tqdm_installed = True\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    # Launch the Tensorflow session\n",
    "    sess = tf.Session()\n",
    "    sess.run(init)\n",
    "\n",
    "    # training_iterations_count: The number of data pieces to train on in total\n",
    "    # batch_size: The number of data pieces per batch\n",
    "    training_iterations = range(0,training_iterations_count,batch_size)\n",
    "    if tqdm_installed:\n",
    "        # Add a progress bar if TQDM is installed\n",
    "        training_iterations = tqdm(training_iterations)\n",
    "\n",
    "    for i in training_iterations:\n",
    "        if i % 1000 == 0:\n",
    "        # Select indices for a random data subset\n",
    "            batch = np.random.randint(data_feature_list[0].shape[0], size=batch_size)\n",
    "\n",
    "        # Use the selected subset indices to initialize the graph's \n",
    "        #   placeholder values\n",
    "        hyps, evis, ys = (data_feature_list[0][batch,:],\n",
    "                          data_feature_list[1][batch,:],\n",
    "                          correct_scores[batch])\n",
    "\n",
    "        # Run the optimization with these initialized values\n",
    "        sess.run([opt_op], feed_dict={hyp: hyps, evi: evis, y: ys})\n",
    "        # display_step: how often the accuracy and loss should \n",
    "        #   be tested and displayed.\n",
    "        if (i/batch_size) % display_step == 0:\n",
    "            # Calculate batch accuracy\n",
    "            acc = sess.run(accuracy, feed_dict={hyp: hyps, evi: evis, y: ys})\n",
    "            # Calculate batch loss\n",
    "            tmp_loss = sess.run(loss, feed_dict={hyp: hyps, evi: evis, y: ys})\n",
    "            # Display results\n",
    "            print(\"Iter \" + str(i/batch_size) + \", Minibatch Loss= \" + \\\n",
    "                  \"{:.6f}\".format(tmp_loss) + \", Training Accuracy= \" + \\\n",
    "                  \"{:.5f}\".format(acc))\n",
    "\n",
    "    data_feature_list, correct_values, correct_scores = split_test_data_into_scores(\"data/dev.txt\")\n",
    "\n",
    "\n",
    "    hyps, evis, ys = (data_feature_list[0][:],\n",
    "                      data_feature_list[1][:],\n",
    "                      correct_scores)\n",
    "    predictions = sess.run(classification_scores, feed_dict={hyp: hyps, evi: evis, y: ys})\n",
    "    total = len(predictions)\n",
    "    correct_predictions = 0\n",
    "    for i,prediction in enumerate(predictions):\n",
    "        if np.argmax(prediction[0])==np.argmax(ys[i]):\n",
    "            correct_predictions += 1\n",
    "    #     print([\"Positive\", \"Neutral\", \"Negative\"][np.argmax(prediction[0])]+\n",
    "    #       \" / \" + [\"Positive\", \"Neutral\", \"Negative\"][np.argmax(ys[i])])\n",
    "    acc = correct_predictions*100/total\n",
    "    print(\"Acc: %s\" % str(acc))\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "need at least one array to concatenate",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-8dcecbc16f2a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mbatches\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mbatch_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0maccs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcreate_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maccs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-12-2af7800b45f4>\u001b[0m in \u001b[0;36mcreate_model\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mcreate_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_default_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mdata_feature_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcorrect_values\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcorrect_scores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msplit_data_into_scores\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0ml_h\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ml_e\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmax_hypothesis_length\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_evidence_length\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-11-3a23a9106649>\u001b[0m in \u001b[0;36msplit_data_into_scores\u001b[1;34m()\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m             hyp_sentences.append(np.vstack(\n\u001b[1;32m---> 14\u001b[1;33m                     sentence2sequence(row[\"sentence_A\"].lower())[0]))\n\u001b[0m\u001b[0;32m     15\u001b[0m             evi_sentences.append(np.vstack(\n\u001b[0;32m     16\u001b[0m                     sentence2sequence(row[\"sentence_B\"].lower())[0]))\n",
      "\u001b[1;32mf:\\python 36 64\\lib\\site-packages\\numpy\\core\\shape_base.py\u001b[0m in \u001b[0;36mvstack\u001b[1;34m(tup)\u001b[0m\n\u001b[0;32m    281\u001b[0m     \"\"\"\n\u001b[0;32m    282\u001b[0m     \u001b[0m_warn_for_nonsequence\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 283\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_nx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0matleast_2d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_m\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0m_m\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtup\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    284\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    285\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: need at least one array to concatenate"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "accs = [] # 128\n",
    "batches = [128, 64, 32, 1, 256]\n",
    "for batch in batches:\n",
    "    batch_size = batch\n",
    "    accs.append(create_model())\n",
    "\n",
    "print(accs)\n",
    "print(\"Best batch size %s\" % str(batches[np.argmax(accs)]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm = tf.contrib.rnn.BasicLSTMCell(lstm_size)\n",
    "lstm_drop =  tf.contrib.rnn.DropoutWrapper(lstm, input_p, output_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# N: The number of elements in each of our batches, \n",
    "#   which we use to train subsets of data for efficiency's sake.\n",
    "# l_h: The maximum length of a hypothesis, or the second sentence.  This is\n",
    "#   used because training an RNN is extraordinarily difficult without \n",
    "#   rolling it out to a fixed length.\n",
    "# l_e: The maximum length of evidence, the first sentence.  This is used\n",
    "#   because training an RNN is extraordinarily difficult without \n",
    "#   rolling it out to a fixed length.\n",
    "# D: The size of our used GloVe or other vectors.\n",
    "hyp = tf.placeholder(tf.float32, [N, l_h, D], 'hypothesis')\n",
    "evi = tf.placeholder(tf.float32, [N, l_e, D], 'evidence')\n",
    "y = tf.placeholder(tf.float32, [N, 3], 'label')\n",
    "# hyp: Where the hypotheses will be stored during training.\n",
    "# evi: Where the evidences will be stored during training.\n",
    "# y: Where correct scores will be stored during training.\n",
    "\n",
    "# lstm_size: the size of the gates in the LSTM, \n",
    "#    as in the first LSTM layer's initialization.\n",
    "# lstm_back = tf.contrib.rnn.BasicLSTMCell(lstm_size)\n",
    "lstm_back = tf.contrib.rnn.BasicLSTMCell(lstm_size)\n",
    "\n",
    "# lstm_back:  The LSTM used for looking backwards \n",
    "#   through the sentences, similar to lstm.\n",
    "\n",
    "# input_p: the probability that inputs to the LSTM will be retained at each\n",
    "#   iteration of dropout.\n",
    "# output_p: the probability that outputs from the LSTM will be retained at \n",
    "#   each iteration of dropout.\n",
    "lstm_drop_back = tf.contrib.rnn.DropoutWrapper(lstm_back, input_p, output_p)\n",
    "# lstm_drop_back:  A dropout wrapper for lstm_back, like lstm_drop.\n",
    "\n",
    "\n",
    "fc_initializer = tf.random_normal_initializer(stddev=0.1) \n",
    "# fc_initializer: initial values for the fully connected layer's weights.\n",
    "# hidden_size: the size of the outputs from each lstm layer.  \n",
    "#   Multiplied by 2 to account for the two LSTMs.\n",
    "fc_weight = tf.get_variable('fc_weight', [2*hidden_size, 3], \n",
    "                            initializer = fc_initializer)\n",
    "# fc_weight: Storage for the fully connected layer's weights.\n",
    "fc_bias = tf.get_variable('bias', [3])\n",
    "# fc_bias: Storage for the fully connected layer's bias.\n",
    "\n",
    "# tf.GraphKeys.REGULARIZATION_LOSSES:  A key to a collection in the graph\n",
    "#   designated for losses due to regularization.\n",
    "#   In this case, this portion of loss is regularization on the weights\n",
    "#   for the fully connected layer.\n",
    "tf.add_to_collection(tf.GraphKeys.REGULARIZATION_LOSSES, \n",
    "                     tf.nn.l2_loss(fc_weight)) \n",
    "\n",
    "x = tf.concat([hyp, evi], 1) # N, (Lh+Le), d\n",
    "# Permuting batch_size and n_steps\n",
    "x = tf.transpose(x, [1, 0, 2]) # (Le+Lh), N, d\n",
    "# Reshaping to (n_steps*batch_size, n_input)\n",
    "x = tf.reshape(x, [-1, vector_size]) # (Le+Lh)*N, d\n",
    "# Split to get a list of 'n_steps' tensors of shape (batch_size, n_input)\n",
    "x = tf.split(x, l_seq,)\n",
    "\n",
    "# x: the inputs to the bidirectional_rnn\n",
    "\n",
    "\n",
    "# tf.contrib.rnn.static_bidirectional_rnn: Runs the input through\n",
    "#   two recurrent networks, one that runs the inputs forward and one\n",
    "#   that runs the inputs in reversed order, combining the outputs.\n",
    "rnn_outputs, _, _ = tf.contrib.rnn.static_bidirectional_rnn(lstm, lstm_back,\n",
    "                                                            x, dtype=tf.float32)\n",
    "# rnn_outputs: the list of LSTM outputs, as a list. \n",
    "#   What we want is the latest output, rnn_outputs[-1]\n",
    "\n",
    "classification_scores = tf.matmul(rnn_outputs[-1], fc_weight) + fc_bias\n",
    "# The scores are relative certainties for how likely the output matches\n",
    "#   a certain entailment: \n",
    "#     0: Positive entailment\n",
    "#     1: Neutral entailment\n",
    "#     2: Negative entailment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope('Accuracy'):\n",
    "    predicts = tf.cast(tf.argmax(classification_scores, 1), 'int32')\n",
    "    y_label = tf.cast(tf.argmax(y, 1), 'int32')\n",
    "    corrects = tf.equal(predicts, y_label)\n",
    "    num_corrects = tf.reduce_sum(tf.cast(corrects, tf.float32))\n",
    "    accuracy = tf.reduce_mean(tf.cast(corrects, tf.float32))\n",
    "\n",
    "with tf.variable_scope(\"loss\"):\n",
    "    cross_entropy = tf.nn.softmax_cross_entropy_with_logits(\n",
    "        logits = classification_scores, labels = y)\n",
    "    loss = tf.reduce_mean(cross_entropy)\n",
    "    total_loss = loss + weight_decay * tf.add_n(\n",
    "        tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES))\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "\n",
    "opt_op = optimizer.minimize(total_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                          | 0/782 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 0.0, Minibatch Loss= 0.953089, Training Accuracy= 0.62500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▊                                                                | 10/782 [00:01<02:42,  4.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 10.0, Minibatch Loss= 0.869778, Training Accuracy= 0.62500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|█▋                                                               | 20/782 [00:02<01:14, 10.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 20.0, Minibatch Loss= 0.870624, Training Accuracy= 0.62500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|██▍                                                              | 30/782 [00:03<00:58, 12.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 30.0, Minibatch Loss= 0.881937, Training Accuracy= 0.62500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|███▎                                                             | 40/782 [00:04<00:57, 12.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 40.0, Minibatch Loss= 0.864157, Training Accuracy= 0.62500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|████▏                                                            | 50/782 [00:05<00:57, 12.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 50.0, Minibatch Loss= 0.855668, Training Accuracy= 0.62500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|████▉                                                            | 60/782 [00:05<00:54, 13.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 60.0, Minibatch Loss= 0.860911, Training Accuracy= 0.62500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|█████▊                                                           | 70/782 [00:06<01:14,  9.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 70.0, Minibatch Loss= 0.810160, Training Accuracy= 0.62500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|██████▋                                                          | 80/782 [00:08<01:19,  8.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 80.0, Minibatch Loss= 0.869567, Training Accuracy= 0.62500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|███████▍                                                         | 90/782 [00:09<01:36,  7.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 90.0, Minibatch Loss= 0.866427, Training Accuracy= 0.62500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|████████▏                                                        | 99/782 [00:10<01:04, 10.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 100.0, Minibatch Loss= 0.860988, Training Accuracy= 0.62500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|████████▉                                                       | 109/782 [00:11<00:52, 12.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 110.0, Minibatch Loss= 0.851916, Training Accuracy= 0.62500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█████████▋                                                      | 119/782 [00:11<00:49, 13.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 120.0, Minibatch Loss= 1.020243, Training Accuracy= 0.62500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|██████████▌                                                     | 129/782 [00:12<00:47, 13.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 130.0, Minibatch Loss= 0.986630, Training Accuracy= 0.50000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|███████████▍                                                    | 139/782 [00:13<00:46, 13.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 140.0, Minibatch Loss= 0.979619, Training Accuracy= 0.50000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|████████████▏                                                   | 149/782 [00:14<00:47, 13.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 150.0, Minibatch Loss= 0.973268, Training Accuracy= 0.50000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|█████████████                                                   | 160/782 [00:15<01:20,  7.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 160.0, Minibatch Loss= 0.967191, Training Accuracy= 0.50781\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|█████████████▉                                                  | 170/782 [00:16<01:22,  7.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 170.0, Minibatch Loss= 0.972102, Training Accuracy= 0.50000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██████████████▋                                                 | 179/782 [00:17<01:04,  9.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 180.0, Minibatch Loss= 1.170082, Training Accuracy= 0.50000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|███████████████▌                                                | 190/782 [00:19<01:30,  6.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 190.0, Minibatch Loss= 0.946140, Training Accuracy= 0.50000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|████████████████▎                                               | 200/782 [00:21<01:42,  5.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 200.0, Minibatch Loss= 0.969867, Training Accuracy= 0.50000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|█████████████████▏                                              | 210/782 [00:22<01:01,  9.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 210.0, Minibatch Loss= 0.994749, Training Accuracy= 0.50000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██████████████████                                              | 220/782 [00:23<01:07,  8.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 220.0, Minibatch Loss= 0.977621, Training Accuracy= 0.50000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██████████████████▊                                             | 230/782 [00:24<00:53, 10.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 230.0, Minibatch Loss= 0.943307, Training Accuracy= 0.50781\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███████████████████▋                                            | 240/782 [00:25<01:25,  6.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 240.0, Minibatch Loss= 0.989862, Training Accuracy= 0.50000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|████████████████████▍                                           | 250/782 [00:27<01:51,  4.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 250.0, Minibatch Loss= 0.952276, Training Accuracy= 0.58594\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|█████████████████████▎                                          | 260/782 [00:29<01:44,  4.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 260.0, Minibatch Loss= 0.943423, Training Accuracy= 0.58594\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|██████████████████████                                          | 270/782 [00:31<01:19,  6.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 270.0, Minibatch Loss= 0.941632, Training Accuracy= 0.58594\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|██████████████████████▉                                         | 280/782 [00:33<01:25,  5.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 280.0, Minibatch Loss= 0.938469, Training Accuracy= 0.58594\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███████████████████████▋                                        | 290/782 [00:35<01:50,  4.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 290.0, Minibatch Loss= 0.931999, Training Accuracy= 0.58594\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|████████████████████████▌                                       | 300/782 [00:36<01:12,  6.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 300.0, Minibatch Loss= 0.926593, Training Accuracy= 0.58594\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|█████████████████████████▎                                      | 310/782 [00:38<00:48,  9.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 310.0, Minibatch Loss= 0.925156, Training Accuracy= 0.58594\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|██████████████████████████▏                                     | 320/782 [00:39<00:43, 10.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 320.0, Minibatch Loss= 0.920445, Training Accuracy= 0.58594\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|███████████████████████████                                     | 330/782 [00:40<00:42, 10.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 330.0, Minibatch Loss= 0.912083, Training Accuracy= 0.58594\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|███████████████████████████▊                                    | 340/782 [00:41<00:37, 11.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 340.0, Minibatch Loss= 0.967439, Training Accuracy= 0.58594\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████████████████████████████▋                                   | 350/782 [00:41<00:32, 13.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 350.0, Minibatch Loss= 0.929947, Training Accuracy= 0.58594\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|█████████████████████████████▍                                  | 360/782 [00:42<00:38, 10.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 360.0, Minibatch Loss= 0.914748, Training Accuracy= 0.58594\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|██████████████████████████████▎                                 | 370/782 [00:44<01:03,  6.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 370.0, Minibatch Loss= 0.897903, Training Accuracy= 0.58594\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|███████████████████████████████                                 | 380/782 [00:45<00:50,  7.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 380.0, Minibatch Loss= 0.973282, Training Accuracy= 0.52344\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|███████████████████████████████▉                                | 390/782 [00:47<00:46,  8.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 390.0, Minibatch Loss= 0.962831, Training Accuracy= 0.52344\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|████████████████████████████████▋                               | 400/782 [00:48<00:53,  7.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 400.0, Minibatch Loss= 0.991482, Training Accuracy= 0.52344\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████████████████████████████████▌                              | 410/782 [00:50<00:54,  6.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 410.0, Minibatch Loss= 0.964698, Training Accuracy= 0.52344\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|██████████████████████████████████▎                             | 420/782 [00:51<00:50,  7.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 420.0, Minibatch Loss= 0.959742, Training Accuracy= 0.53906\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|███████████████████████████████████▏                            | 430/782 [00:53<00:47,  7.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 430.0, Minibatch Loss= 0.974693, Training Accuracy= 0.52344\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|████████████████████████████████████                            | 440/782 [00:54<00:53,  6.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 440.0, Minibatch Loss= 0.948343, Training Accuracy= 0.53125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|████████████████████████████████████▊                           | 450/782 [00:56<00:49,  6.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 450.0, Minibatch Loss= 0.972716, Training Accuracy= 0.52344\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|█████████████████████████████████████▋                          | 460/782 [00:57<00:41,  7.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 460.0, Minibatch Loss= 0.942318, Training Accuracy= 0.53125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████████████████████████████████████▍                         | 470/782 [00:59<00:48,  6.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 470.0, Minibatch Loss= 0.970029, Training Accuracy= 0.53906\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|███████████████████████████████████████▎                        | 480/782 [01:01<01:07,  4.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 480.0, Minibatch Loss= 0.905508, Training Accuracy= 0.55469\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|████████████████████████████████████████                        | 490/782 [01:03<00:41,  7.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 490.0, Minibatch Loss= 0.967807, Training Accuracy= 0.53125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|████████████████████████████████████████▉                       | 500/782 [01:04<00:35,  7.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 500.0, Minibatch Loss= 0.834887, Training Accuracy= 0.67188\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|█████████████████████████████████████████▋                      | 510/782 [01:06<00:36,  7.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 510.0, Minibatch Loss= 0.795980, Training Accuracy= 0.69531\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|██████████████████████████████████████████▌                     | 520/782 [01:07<00:35,  7.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 520.0, Minibatch Loss= 0.844264, Training Accuracy= 0.69531\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|███████████████████████████████████████████▍                    | 530/782 [01:09<00:38,  6.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 530.0, Minibatch Loss= 0.728521, Training Accuracy= 0.68750\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|████████████████████████████████████████████▏                   | 540/782 [01:10<00:35,  6.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 540.0, Minibatch Loss= 0.799570, Training Accuracy= 0.69531\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|█████████████████████████████████████████████                   | 550/782 [01:12<00:30,  7.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 550.0, Minibatch Loss= 0.894853, Training Accuracy= 0.69531\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|█████████████████████████████████████████████▊                  | 560/782 [01:13<00:26,  8.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 560.0, Minibatch Loss= 0.869831, Training Accuracy= 0.69531\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|██████████████████████████████████████████████▋                 | 570/782 [01:14<00:27,  7.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 570.0, Minibatch Loss= 0.829611, Training Accuracy= 0.57812\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|███████████████████████████████████████████████▍                | 580/782 [01:16<00:25,  7.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 580.0, Minibatch Loss= 0.724442, Training Accuracy= 0.69531\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|████████████████████████████████████████████████▎               | 590/782 [01:17<00:23,  8.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 590.0, Minibatch Loss= 0.783291, Training Accuracy= 0.69531\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|█████████████████████████████████████████████████               | 600/782 [01:18<00:22,  8.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 600.0, Minibatch Loss= 0.816589, Training Accuracy= 0.69531\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|█████████████████████████████████████████████████▉              | 610/782 [01:20<00:22,  7.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 610.0, Minibatch Loss= 0.906984, Training Accuracy= 0.69531\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|██████████████████████████████████████████████████▋             | 620/782 [01:21<00:23,  6.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 620.0, Minibatch Loss= 0.851249, Training Accuracy= 0.69531\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|███████████████████████████████████████████████████▌            | 630/782 [01:23<00:19,  7.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 630.0, Minibatch Loss= 0.883435, Training Accuracy= 0.57812\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████████████████████████████████████████████████▍           | 640/782 [01:24<00:17,  8.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 640.0, Minibatch Loss= 0.879895, Training Accuracy= 0.57812\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|█████████████████████████████████████████████████████▏          | 650/782 [01:26<00:18,  6.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 650.0, Minibatch Loss= 0.877085, Training Accuracy= 0.57812\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|██████████████████████████████████████████████████████          | 660/782 [01:27<00:17,  7.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 660.0, Minibatch Loss= 0.872239, Training Accuracy= 0.57812\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|██████████████████████████████████████████████████████▊         | 670/782 [01:29<00:22,  5.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 670.0, Minibatch Loss= 0.861971, Training Accuracy= 0.57812\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|███████████████████████████████████████████████████████▋        | 680/782 [01:31<00:15,  6.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 680.0, Minibatch Loss= 0.834377, Training Accuracy= 0.57812\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████████████████████████████████████████████████████▍       | 690/782 [01:32<00:11,  7.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 690.0, Minibatch Loss= 0.821736, Training Accuracy= 0.57812\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████████████████████████████████████████████████████▎      | 700/782 [01:34<00:12,  6.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 700.0, Minibatch Loss= 0.879906, Training Accuracy= 0.57812\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|██████████████████████████████████████████████████████████      | 710/782 [01:35<00:11,  6.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 710.0, Minibatch Loss= 0.877059, Training Accuracy= 0.57812\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|██████████████████████████████████████████████████████████▉     | 720/782 [01:37<00:10,  5.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 720.0, Minibatch Loss= 0.867354, Training Accuracy= 0.57812\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|███████████████████████████████████████████████████████████▋    | 730/782 [01:39<00:08,  6.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 730.0, Minibatch Loss= 0.813593, Training Accuracy= 0.57812\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|████████████████████████████████████████████████████████████▌   | 740/782 [01:41<00:08,  5.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 740.0, Minibatch Loss= 0.803179, Training Accuracy= 0.58594\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████████████████████████████████████████████████████████▍  | 750/782 [01:42<00:05,  5.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 750.0, Minibatch Loss= 0.927214, Training Accuracy= 0.62500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|██████████████████████████████████████████████████████████████▏ | 760/782 [01:44<00:03,  6.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 760.0, Minibatch Loss= 0.887888, Training Accuracy= 0.62500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|███████████████████████████████████████████████████████████████ | 770/782 [01:46<00:02,  4.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 770.0, Minibatch Loss= 0.901604, Training Accuracy= 0.62500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████▊| 780/782 [01:48<00:00,  5.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 780.0, Minibatch Loss= 0.861893, Training Accuracy= 0.62500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████| 782/782 [01:48<00:00,  5.00it/s]\n"
     ]
    }
   ],
   "source": [
    "# Initialize variables\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Use TQDM if installed\n",
    "# tqdm_installed = False\n",
    "# try:\n",
    "#     from tqdm import tqdm\n",
    "#     tqdm_installed = True\n",
    "# except:\n",
    "#     pass\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Launch the Tensorflow session\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "# training_iterations_count: The number of data pieces to train on in total\n",
    "# batch_size: The number of data pieces per batch\n",
    "training_iterations = range(0,training_iterations_count,batch_size)\n",
    "# if tqdm_installed:\n",
    "    # Add a progress bar if TQDM is installed\n",
    "training_iterations = tqdm(training_iterations)\n",
    "\n",
    "for i in training_iterations:\n",
    "    if i % 1000 == 0:\n",
    "    # Select indices for a random data subset\n",
    "        batch = np.random.randint(data_feature_list[0].shape[0], size=batch_size)\n",
    "    \n",
    "    # Use the selected subset indices to initialize the graph's \n",
    "    #   placeholder values\n",
    "    hyps, evis, ys = (data_feature_list[0][batch,:],\n",
    "                      data_feature_list[1][batch,:],\n",
    "                      correct_scores[batch])\n",
    "    \n",
    "    # Run the optimization with these initialized values\n",
    "    sess.run([opt_op], feed_dict={hyp: hyps, evi: evis, y: ys})\n",
    "    # display_step: how often the accuracy and loss should \n",
    "    #   be tested and displayed.\n",
    "    if (i/batch_size) % display_step == 0:\n",
    "        # Calculate batch accuracy\n",
    "        acc = sess.run(accuracy, feed_dict={hyp: hyps, evi: evis, y: ys})\n",
    "        # Calculate batch loss\n",
    "        tmp_loss = sess.run(loss, feed_dict={hyp: hyps, evi: evis, y: ys})\n",
    "        # Display results\n",
    "        print(\"Iter \" + str(i/batch_size) + \", Minibatch Loss= \" + \\\n",
    "              \"{:.6f}\".format(tmp_loss) + \", Training Accuracy= \" + \\\n",
    "              \"{:.5f}\".format(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def split_test_data_into_scores(file_path):\n",
    "    import csv\n",
    "    with open(file_path,\"r\") as data:\n",
    "        train = csv.DictReader(data , delimiter='\\t')\n",
    "        evi_sentences = []\n",
    "        hyp_sentences = []\n",
    "        labels = []\n",
    "        scores = []\n",
    "        count = 1\n",
    "        for row in train:\n",
    "            hyp_sentences.append(np.vstack(\n",
    "                    sentence2sequence(row[\"sentence_A\"].lower())[0]))\n",
    "            evi_sentences.append(np.vstack(\n",
    "                    sentence2sequence(row[\"sentence_B\"].lower())[0]))\n",
    "            labels.append(row[\"entailment_judgment\"])\n",
    "            scores.append(score_setup(row))\n",
    "        \n",
    "        hyp_sentences = np.stack([fit_to_size(x, (max_hypothesis_length, vector_size))\n",
    "                          for x in hyp_sentences])\n",
    "        evi_sentences = np.stack([fit_to_size(x, (max_evidence_length, vector_size))\n",
    "                          for x in evi_sentences])\n",
    "                                 \n",
    "        return (hyp_sentences, evi_sentences), labels, np.array(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\python 36 64\\lib\\site-packages\\ipykernel_launcher.py:20: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Cannot feed value of shape (4927, 30, 100) for Tensor 'hypothesis:0', which has shape '(128, 30, 100)'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-df57abc7fe46>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     21\u001b[0m                   \u001b[0mdata_feature_list\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m                   correct_scores)\n\u001b[1;32m---> 23\u001b[1;33m \u001b[0mpredictions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclassification_scores\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mhyp\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mhyps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevi\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mevis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mys\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m \u001b[0mtotal\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[0mcorrect_predictions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mf:\\python 36 64\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    887\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 889\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    890\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mf:\\python 36 64\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1094\u001b[0m                 \u001b[1;34m'Cannot feed value of shape %r for Tensor %r, '\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1095\u001b[0m                 \u001b[1;34m'which has shape %r'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1096\u001b[1;33m                 % (np_val.shape, subfeed_t.name, str(subfeed_t.get_shape())))\n\u001b[0m\u001b[0;32m   1097\u001b[0m           \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_feedable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msubfeed_t\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1098\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Tensor %s may not be fed.'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0msubfeed_t\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Cannot feed value of shape (4927, 30, 100) for Tensor 'hypothesis:0', which has shape '(128, 30, 100)'"
     ]
    }
   ],
   "source": [
    "evidences = [\"People wearing costumes are gathering in a forest and are looking in the same direction\"]\n",
    "hypotheses = [\"Masked people are looking in the same direction in a forest\"\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "sentence1 = [fit_to_size(np.vstack(sentence2sequence(evidence)[0]),\n",
    "                         (30, 50)) for evidence in evidences]\n",
    "\n",
    "sentence2 = [fit_to_size(np.vstack(sentence2sequence(hypothesis)[0]),\n",
    "                         (30,50)) for hypothesis in hypotheses]\n",
    "\n",
    "\n",
    "\n",
    "data_feature_list, correct_values, correct_scores = split_test_data_into_scores(\"data/test_labeled.txt\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "hyps, evis, ys = (data_feature_list[0],\n",
    "                  data_feature_list[1],\n",
    "                  correct_scores)\n",
    "predictions = sess.run(classification_scores, feed_dict={hyp: hyps, evi: evis, y: ys})\n",
    "total = len(predictions)\n",
    "correct_predictions = 0\n",
    "for i,prediction in enumerate(predictions):\n",
    "    if np.argmax(prediction[0])==np.argmax(ys[i]):\n",
    "        correct_predictions += 1\n",
    "#     print([\"Positive\", \"Neutral\", \"Negative\"][np.argmax(prediction[0])]+\n",
    "#       \" / \" + [\"Positive\", \"Neutral\", \"Negative\"][np.argmax(ys[i])])\n",
    "    \n",
    "print(\"Acc: %s\" % str(correct_predictions*100/total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
