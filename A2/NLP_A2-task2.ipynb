{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [SICK Training data](http://www.site.uottawa.ca/~diana/csi5386/A2_2019/SICK_train.txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "##### max length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max_hyp 28\n",
      "Max_evi 32\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import csv\n",
    "file_name=\"data/training.txt\"\n",
    "with open(file_name,\"r\") as data:\n",
    "    train = csv.DictReader(data , delimiter='\\t')\n",
    "    max_evi, max_hyp = 0, 0 \n",
    "    count = 1\n",
    "    for row in train:\n",
    "        hyp = len(row[\"sentence_A\"].split())\n",
    "        if hyp > max_hyp:\n",
    "            max_hyp = hyp\n",
    "        evi = len(row[\"sentence_B\"].split())\n",
    "        if evi > max_evi:\n",
    "            max_evi = evi\n",
    "    print(\"Max_hyp %s\" % str(max_hyp))        \n",
    "    print(\"Max_evi %s\" % str(max_evi))            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# glove_zip_file = \"data/glove.6B.zip\"\n",
    "# glove_vectors_file = \"data/glove.6B.300d.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import zipfile, urllib.request, shutil, os\n",
    "    \n",
    "# #large file - 862 MB\n",
    "# if (not os.path.isfile(glove_zip_file) and\n",
    "#     not os.path.isfile(glove_vectors_file)):\n",
    "#     with urllib.request.urlopen(\"http://nlp.stanford.edu/data/glove.6B.zip\") as response, open(glove_zip_file, 'wb') as out_file:\n",
    "#         shutil.copyfileobj(response, out_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import zipfile\n",
    "# def unzip_single_file(zip_file_name, output_file_name):\n",
    "#     \"\"\"\n",
    "#         If the outFile is already created, don't recreate\n",
    "#         If the outFile does not exist, create it from the zipFile\n",
    "#     \"\"\"\n",
    "#     if not os.path.isfile(output_file_name):\n",
    "#         with open(output_file_name, 'wb') as out_file:\n",
    "#             with zipfile.ZipFile(zip_file_name) as zipped:\n",
    "#                 for info in zipped.infolist():\n",
    "#                     if output_file_name in info.filename:\n",
    "#                         with zipped.open(info) as requested_file:\n",
    "#                             out_file.write(requested_file.read())\n",
    "#                             return\n",
    "\n",
    "# unzip_single_file(glove_zip_file, glove_vectors_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# glove_wordmap = {}\n",
    "# with open(glove_vectors_file, \"r\", encoding=\"utf8\") as glove:\n",
    "#     for line in glove:\n",
    "#         name, vector = tuple(line.split(\" \", 1))\n",
    "#         glove_wordmap[name] = np.fromstring(vector, sep=\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'csv' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-5d4125641ea3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m             \u001b[0mlabels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"entailment_judgment\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mCounter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmost_common\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[0msplit_data_into_scores\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1414\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2793\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1414\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m720\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2793\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2793\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1414\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m720\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-1-5d4125641ea3>\u001b[0m in \u001b[0;36msplit_data_into_scores\u001b[1;34m(file_name)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"r\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m         \u001b[0mtrain\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcsv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDictReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'\\t'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m         \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'csv' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# from collections import Counter\n",
    "# def split_data_into_scores(file_name=\"data/training.txt\"):\n",
    "\n",
    "#     with open(file_name,\"r\") as data:\n",
    "#         train = csv.DictReader(data , delimiter='\\t')\n",
    "#         labels = []\n",
    "#         for row in train:\n",
    "#             labels.append(row[\"entailment_judgment\"])\n",
    "#     print(Counter(labels).most_common())\n",
    "# split_data_into_scores()\n",
    "# print(1414/(2793+1414+720))\n",
    "# print(2793/(2793+1414+720))\n",
    "# print(720/(2793+1414+720))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.random.seed(1337)  # for reproducibility\n",
    "import os\n",
    "from keras.regularizers import l2, l1_l2\n",
    "from keras.callbacks import *\n",
    "# from visualizer import *\n",
    "from keras.models import *\n",
    "from keras.optimizers import *\n",
    "from keras import utils\n",
    "from keras.utils.np_utils import *\n",
    "from keras.layers.core import *\n",
    "from keras.layers import Input, Embedding, LSTM, Dense, merge, TimeDistributed\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "# from keras.utils.visualize_util import plot  # THIS IS BAD\n",
    "# from data_reader import *\n",
    "import logging\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AccCallBack(Callback):\n",
    "    def __init__(self, xtrain, ytrain, xdev, ydev, xtest, ytest, vocab, opts):\n",
    "        self.xtrain = xtrain\n",
    "        self.ytrain = ytrain\n",
    "        self.xdev = xdev\n",
    "        self.ydev = ydev\n",
    "        self.xtest = xtest\n",
    "        self.ytest = ytest\n",
    "        self.vocab = vocab\n",
    "        self.opts = opts\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        train_acc = compute_acc(self.xtrain, self.ytrain, self.vocab, self.model, self.opts)\n",
    "        dev_acc = compute_acc(self.xdev, self.ydev, self.vocab, self.model, self.opts)\n",
    "        test_acc = compute_acc(self.xtest, self.ytest, self.vocab, self.model, self.opts)\n",
    "        logging.info('----------------------------------')\n",
    "        logging.info('Epoch ' + str(epoch) + ' train loss:' + str(logs.get('loss')) + ' - Validation loss: ' + str(\n",
    "            logs.get('val_loss')) + ' train acc: ' + str(train_acc[0]) + '/' + str(train_acc[1]) + ' dev acc: ' + str(\n",
    "            dev_acc[0]) + '/' + str(dev_acc[1]) + ' test acc: ' + str(test_acc[0]) + '/' + str(test_acc[1]))\n",
    "        logging.info('----------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "def compute_acc(X, Y, vocab, model, opts):\n",
    "    scores = model.predict(X, batch_size=opts['batch_size'])\n",
    "    return stats.pearsonr(scores, Y)\n",
    "    print(\"***\")\n",
    "    col_sum = np.sum(np.array(scores), axis=0)\n",
    "    print(col_sum)\n",
    "    prediction = np.zeros(scores.shape)\n",
    "    for i in range(scores.shape[0]):\n",
    "        l = np.argmax(scores[i])\n",
    "        prediction[i][l] = 1.0\n",
    "    assert np.array_equal(np.ones(prediction.shape[0]), np.sum(prediction, axis=1))\n",
    "    plabels = np.argmax(prediction, axis=1)\n",
    "    tlabels = np.argmax(Y, axis=1)\n",
    "    acc = sum([1 if x==y else 0 for x,y in list(zip(tlabels, plabels))])/len(tlabels)\n",
    "    return acc, acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "def get_H_n(X):\n",
    "    ans = X[:, -1, :]  # get last element from time dim\n",
    "    return ans\n",
    "\n",
    "\n",
    "def get_Y(X, xmaxlen):\n",
    "    return X[:, :xmaxlen, :]  # get first xmaxlen elem from time dim\n",
    "\n",
    "\n",
    "def get_R(X):\n",
    "    Y, alpha = X[0], X[1]\n",
    "    ans = K.T.batched_dot(Y, alpha)\n",
    "    return ans\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "main_input (InputLayer)          (None, 64)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "x (Embedding)                    (None, 64, 75)        174525      main_input[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "dropout (Dropout)                (None, 64, 75)        0           x[0][0]                          \n",
      "____________________________________________________________________________________________________\n",
      "lstm_fwd (LSTM)                  (None, 64, 50)        25200       dropout[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "lstm_bwd (LSTM)                  (None, 64, 50)        25200       dropout[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "bilstm (Merge)                   (None, 64, 100)       0           lstm_fwd[0][0]                   \n",
      "                                                                   lstm_bwd[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "d_bilstm (Dropout)               (None, 64, 100)       0           bilstm[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "activation_13 (Activation)       (None, 64, 100)       0           d_bilstm[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dense_13 (Dense)                 (None, 64, 1)         101         activation_13[0][0]              \n",
      "====================================================================================================\n",
      "Total params: 225,026\n",
      "Trainable params: 225,026\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\python 36 64\\lib\\site-packages\\ipykernel_launcher.py:12: UserWarning: The `merge` function is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "  if sys.path[0] == '':\n",
      "f:\\python 36 64\\lib\\site-packages\\ipykernel_launcher.py:36: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(1, activation=\"tanh\", kernel_regularizer=<keras.reg...)`\n",
      "f:\\python 36 64\\lib\\site-packages\\ipykernel_launcher.py:42: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=Tensor(\"de...)`\n"
     ]
    }
   ],
   "source": [
    "def build_model(opts):\n",
    "    k = 2 * opts['lstm_units']  # 200\n",
    "    L = opts['xmaxlen']  # 35\n",
    "    N = opts['xmaxlen'] + opts['ymaxlen']\n",
    "    \n",
    "    main_input = Input(shape=(N,), dtype='int32', name='main_input') #(N,70)\n",
    "    x = Embedding(output_dim=opts['emb'], input_dim=len(VOCABULARY.keys())+1, input_length=N, name='x')(main_input)\n",
    "    drop_out = Dropout(0.6, name='dropout')(x) # 70,50\n",
    "    lstm_fwd = LSTM(opts['lstm_units'], return_sequences=True, name='lstm_fwd')(drop_out)\n",
    "    lstm_bwd = LSTM(opts['lstm_units'], return_sequences=True, go_backwards=True, name='lstm_bwd')(drop_out)\n",
    "    #70,100\n",
    "    bilstm = merge([lstm_fwd, lstm_bwd], name='bilstm', mode='concat')\n",
    "    #70,200\n",
    "    drop_out = Dropout(0.2, name=\"d_bilstm\")(bilstm)\n",
    "    \n",
    "    ###\n",
    "    \n",
    "    h_star = Activation('sigmoid')(drop_out)\n",
    "#     Y = Lambda(get_Y, arguments={\"xmaxlen\": L}, name=\"Y\", output_shape=(L, k))(h_star)\n",
    "#     WY = TimeDistributed(Dense(k, W_regularizer=l2(0.05)), name=\"WY\")(Y)#35,200\n",
    "\n",
    "#     M = Activation('tanh', name=\"M\")(WY)\n",
    "\n",
    "#     alpha_ = TimeDistributed(Dense(k, activation='linear'), name=\"alpha_\")(WY)\n",
    "\n",
    "#     alpha = Dense(k, activation='tanh', name=\"alpha\")(h_star) #35,200 Dense_33\n",
    "#     drop_out = Dropout(0.3, name=\"d2_bilstm\")(alpha)\n",
    "\n",
    "#     Wr = Dense(k, W_regularizer=l2(0.05), name=\"Dense_Wr\")(WY) #200,35\n",
    "#     #   \n",
    "#     h_star = Activation('tanh')(Wr)\n",
    "\n",
    "    \n",
    "    \n",
    "#     flat_h_star = Flatten(name=\"flat_h_star\")(h_star)\n",
    "    out = Dense(1, W_regularizer=l2(0.05), activation='tanh')(h_star)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    output = out\n",
    "    model = Model(input=[main_input], output=output)\n",
    "    model.summary()\n",
    "    # plot(model, 'model.png')\n",
    "    # # model.compile(loss={'output':'binary_crossentropy'}, optimizer=Adam())\n",
    "    # model.compile(loss={'output':'categorical_crossentropy'}, optimizer=Adam(options.lr))\n",
    "    \n",
    "    model.compile(loss='MSE',optimizer=Adam(opts['lr']))\n",
    "\n",
    "    return model\n",
    "    \n",
    "  \n",
    "model=build_model(opts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Model - Load & save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def save_model(model, wtpath, archpath, mode='yaml'):\n",
    "    if mode == 'yaml':\n",
    "        yaml_string = model.to_yaml()\n",
    "        open(archpath, 'w').write(yaml_string)\n",
    "    else:\n",
    "        with open(archpath, 'w') as f:\n",
    "            f.write(model.to_json())\n",
    "    model.save_weights(wtpath)\n",
    "\n",
    "\n",
    "def load_model(wtpath, archpath, mode='yaml'):\n",
    "    if mode == 'yaml':\n",
    "        model = model_from_yaml(open(archpath).read())  # ,custom_objects={\"MyEmbedding\": MyEmbedding})\n",
    "    else:\n",
    "        with open(archpath) as f:\n",
    "            model = model_from_json(f.read())  # , custom_objects={\"MyEmbedding\": MyEmbedding})\n",
    "    model.load_weights(wtpath)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# one hot encoding\n",
    "def score_setup(row):\n",
    "    convert_dict = {\n",
    "      'ENTAILMENT': 0,\n",
    "      'NEUTRAL': 1,\n",
    "      'CONTRADICTION': 2\n",
    "    }\n",
    "    score = np.zeros((3,))\n",
    "    tag = row[\"entailment_judgment\"]\n",
    "    score[convert_dict[tag]] += 1\n",
    "    return score\n",
    "#     return convert_dict[row[\"entailment_judgment\"]]\n",
    "\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "tokenizer = Tokenizer()\n",
    "\n",
    "def split_data_into_scores(max_hyp, max_evi, file_name=\"data/training.txt\"):\n",
    "\n",
    "    global VOCABULARY, tokenizer\n",
    "    import csv\n",
    "    with open(file_name,\"r\") as data:\n",
    "        train = csv.DictReader(data , delimiter='\\t')\n",
    "        evi_sentences = np.empty((0,max_evi))\n",
    "        hyp_sentences = np.empty((0,max_hyp))\n",
    "        labels = []\n",
    "        scores = []\n",
    "        count = 1\n",
    "        for row in train:\n",
    "            hyp = row[\"sentence_A\"].lower()\n",
    "            evi = row[\"sentence_B\"].lower()\n",
    "            tokenizer.fit_on_texts([hyp])\n",
    "            tokenizer.fit_on_texts([evi])\n",
    "            hyp_seq = np.array(tokenizer.texts_to_sequences([hyp])[0])\n",
    "            \n",
    "            padded_hyp = np.pad(hyp_seq,\n",
    "                                (max_hyp-np.shape(hyp_seq)[0],0),\n",
    "                                       'constant',\n",
    "                                       constant_values=(0,))\n",
    "            hyp_sentences = np.append(hyp_sentences, [padded_hyp], axis=0)\n",
    "            count += 1\n",
    "            \n",
    "            evi_seq = np.array(tokenizer.texts_to_sequences([evi])[0])\n",
    "            padded_evi = np.pad(evi_seq,\n",
    "                               (max_evi-np.shape(evi_seq)[0],0),\n",
    "                                'constant',\n",
    "                                constant_values=(0,))\n",
    "            evi_sentences = np.append(evi_sentences, [padded_evi], axis=0)\n",
    "            labels.append(row[\"entailment_judgment\"])\n",
    "            scores.append(float(row[\"relatedness_score\"]))\n",
    "        print(\"Vocabulary size: %s\" % str(len(tokenizer.word_counts.keys())+1))\n",
    "        VOCABULARY = tokenizer.word_index\n",
    "        VOCABULARY['unk'] = 0\n",
    "        print(np.shape(hyp_sentences))\n",
    "        print(np.shape(evi_sentences))\n",
    "        return hyp_sentences, evi_sentences, np.array(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### tokenizer scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# from keras.preprocessing.text import Tokenizer\n",
    "# t = Tokenizer()\n",
    "# fit_text = [\"The earth is an awesome place live\"]\n",
    "# t.fit_on_texts(fit_text)\n",
    "# print(t.word_index)\n",
    "# fit_text = [\"Ana has apples\"]\n",
    "# t.fit_on_texts(fit_text)\n",
    "# print(t.texts_to_sequences([\"Ana is an awesome apple\"]))\n",
    "# # \n",
    "# print(len(t.word_index.keys()))\n",
    "# # from keras.utils.np_utils import to_categorical\n",
    "# # print(to_categorical([1], num_classes=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# # from keras.preprocessing.sequence import TimeseriesGenerator, pad_sequences\n",
    "# import numpy as np\n",
    "# a=np.empty((0,))\n",
    "# for i in range(0,10):\n",
    "#     a = np.append(a, [1,2,3], axis=0)\n",
    "# print(a)\n",
    "\n",
    "# print(np.pad(a, (0,0), 'constant', constant_values=(0,)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model + options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VOCABULARY = {}\n",
    "\n",
    "opts = {\n",
    "    'lstm_units': 50,\n",
    "    'xmaxlen': 32,\n",
    "    'ymaxlen': 32,\n",
    "    'emb': 75, #dimension of the embedding\n",
    "    'max_features': len(VOCABULARY.keys())+1, #vocabulary dim+1\n",
    "    'batch_size': 32,\n",
    "    'lr': 0.05,\n",
    "    'epochs': 10\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "main_input (InputLayer)          (None, 64)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "x (Embedding)                    (None, 64, 100)       232700      main_input[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "dropout (Dropout)                (None, 64, 100)       0           x[0][0]                          \n",
      "____________________________________________________________________________________________________\n",
      "lstm_fwd (LSTM)                  (None, 64, 100)       80400       dropout[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "lstm_bwd (LSTM)                  (None, 64, 100)       80400       dropout[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "bilstm (Merge)                   (None, 64, 200)       0           lstm_fwd[0][0]                   \n",
      "                                                                   lstm_bwd[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "d_bilstm (Dropout)               (None, 64, 200)       0           bilstm[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "activation_4 (Activation)        (None, 64, 200)       0           d_bilstm[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "flat_h_star (Flatten)            (None, 12800)         0           activation_4[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "dense_4 (Dense)                  (None, 1)             12801       flat_h_star[0][0]                \n",
      "====================================================================================================\n",
      "Total params: 406,301\n",
      "Trainable params: 406,301\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\python 36 64\\lib\\site-packages\\ipykernel_launcher.py:12: UserWarning: The `merge` function is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "  if sys.path[0] == '':\n",
      "f:\\python 36 64\\lib\\site-packages\\ipykernel_launcher.py:36: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(1, activation=\"tanh\", kernel_regularizer=<keras.reg...)`\n",
      "f:\\python 36 64\\lib\\site-packages\\ipykernel_launcher.py:42: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=Tensor(\"de...)`\n"
     ]
    }
   ],
   "source": [
    "model = build_model(opts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Data manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 2326\n",
      "(4500, 32)\n",
      "(4500, 32)\n",
      "(4500, 32)\n",
      "[[  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    1.  66.  10. 125.   2.  12.   5.   1. 295.   6.  21. 259.   4.   2.\n",
      "   19.   5.   3. 202.]]\n",
      "out\n",
      "[  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   1.  66.  10. 125.   2.  12.   5.   1. 295.   6.  21. 259.   4.   2.\n",
      "  19.   5.   3. 202.]\n",
      "***\n",
      "(4500, 64)\n",
      "***\n",
      "Vocabulary size: 2326\n",
      "(500, 32)\n",
      "(500, 32)\n",
      "***\n",
      "(500, 64)\n",
      "***\n",
      "Vocabulary size: 2326\n",
      "(4927, 32)\n",
      "(4927, 32)\n"
     ]
    }
   ],
   "source": [
    "X_train,Y_train,Z_train=split_data_into_scores(opts['xmaxlen'], opts['ymaxlen'], \"data/training.txt\")\n",
    "print(np.shape(X_train))\n",
    "print(X_train[:1])\n",
    "\n",
    "print(\"out\")\n",
    "print(X_train[0])\n",
    "xy_train = np.concatenate((X_train, Y_train), axis=1)\n",
    "print(\"***\")\n",
    "print(np.shape(xy_train))\n",
    "print(\"***\")\n",
    "\n",
    "X_dev,Y_dev,Z_dev=split_data_into_scores(opts['xmaxlen'], opts['ymaxlen'], \"data/dev.txt\")\n",
    "xy_dev = np.concatenate((X_dev, Y_dev), axis=1)\n",
    "print(\"***\")\n",
    "print(np.shape(xy_dev))\n",
    "print(\"***\")\n",
    "\n",
    "X_test,Y_test,Z_test=split_data_into_scores(opts['xmaxlen'], opts['ymaxlen'], \"data/test_labeled.txt\")\n",
    "xy_test = np.concatenate((X_test, Y_test), axis=1)\n",
    "\n",
    "train_dict = {'input': xy_train, 'output': Z_train}\n",
    "dev_dict = {'input': xy_dev, 'output': Z_dev}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'lstm_units': 100, 'xmaxlen': 32, 'ymaxlen': 32, 'emb': 100, 'max_features': 1, 'batch_size': 32, 'lr': 0.005, 'epochs': 10}\n",
      "2326\n"
     ]
    }
   ],
   "source": [
    "# history = model.fit(xy_train[:2],Z_train[:2])#,\n",
    "#                             batch_size=opts['batch_size'])\n",
    "#                             epochs=opts['epochs'])\n",
    "print(opts)\n",
    "print(len(VOCABULARY.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4500,)\n",
      "[4.5 3.2 4.7]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error when checking target: expected dense_12 to have 3 dimensions, but got array with shape (4500, 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-62-06203ce612e0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m                             \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mopts\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'batch_size'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m                             \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mopts\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'epochs'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m                             \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxy_dev\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mZ_dev\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m                             )\n\u001b[0;32m      9\u001b[0m \u001b[0mscores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxy_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mopts\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'batch_size'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mf:\\python 36 64\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1520\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1521\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1522\u001b[1;33m             batch_size=batch_size)\n\u001b[0m\u001b[0;32m   1523\u001b[0m         \u001b[1;31m# Prepare validation data.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1524\u001b[0m         \u001b[0mdo_validation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mf:\\python 36 64\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, check_batch_axis, batch_size)\u001b[0m\n\u001b[0;32m   1380\u001b[0m                                     \u001b[0moutput_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1381\u001b[0m                                     \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1382\u001b[1;33m                                     exception_prefix='target')\n\u001b[0m\u001b[0;32m   1383\u001b[0m         sample_weights = _standardize_sample_weights(sample_weight,\n\u001b[0;32m   1384\u001b[0m                                                      self._feed_output_names)\n",
      "\u001b[1;32mf:\\python 36 64\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_input_data\u001b[1;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[0;32m    130\u001b[0m                                  \u001b[1;34m' to have '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshapes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    131\u001b[0m                                  \u001b[1;34m' dimensions, but got array with shape '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 132\u001b[1;33m                                  str(array.shape))\n\u001b[0m\u001b[0;32m    133\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mref_dim\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshapes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    134\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mj\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Error when checking target: expected dense_12 to have 3 dimensions, but got array with shape (4500, 1)"
     ]
    }
   ],
   "source": [
    "print(np.shape(Z_train))\n",
    "print(Z_train[:3])\n",
    "\n",
    "history = model.fit(xy_train,Z_train,\n",
    "                            batch_size=opts['batch_size'],\n",
    "                            epochs=opts['epochs'],\n",
    "                            validation_data=(xy_dev, Z_dev)\n",
    "                            )\n",
    "scores = model.predict(xy_test, batch_size=opts['batch_size'])\n",
    "scores = np.reshape(scores, (1,np.shape(scores)[0]))[0]\n",
    "\n",
    "print(scores[:3])\n",
    "print(np.shape(scores))\n",
    "print(Z_test[:3])\n",
    "test_acc = stats.pearsonr(scores, Z_test)\n",
    "print(test_acc)\n",
    "\n",
    "# train_acc = compute_acc(xy_train, Z_train, VOCABULARY, model, opts)\n",
    "# dev_acc = compute_acc(xy_dev, Z_dev, VOCABULARY, model, opts)\n",
    "# test_acc = compute_acc(xy_test, Z_test, VOCABULARY, model, opts)\n",
    "\n",
    "# print(train_acc)\n",
    "# print(dev_acc)\n",
    "# print(test_acc)\n",
    "\n",
    "# if test_acc[0] > 0.59:\n",
    "opts_name = \"task2-%s\" % str(test_acc[0])\n",
    "save_model(model, 't2-model_weights-%s-%s.weights' % (str(opts_name), str(test_acc[0])),\n",
    "           't2-model_arch_att-%s-%s.yaml' % (str(opts_name), str(test_acc[0])))\n",
    "with open(opts_name, \"w\") as f:\n",
    "    f.write(str(opts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1299. 2536.  665.]\n",
      "[144. 282.  74.]\n",
      "[1414. 2793.  720.]\n"
     ]
    }
   ],
   "source": [
    "print(np.sum(Z_train, axis=0))\n",
    "print(np.sum(Z_dev, axis=0))\n",
    "print(np.sum(Z_test, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # load_model('model_weights.weights', 'model_arch_att.yaml')\n",
    "# train_acc = compute_acc(xy_train, Z_train, VOCABULARY, model, opts)\n",
    "# dev_acc = compute_acc(xy_dev, Z_dev, VOCABULARY, model, opts)\n",
    "# test_acc = compute_acc(xy_test, Z_test, VOCABULARY, model, opts)\n",
    "\n",
    "# print(train_acc)\n",
    "# print(dev_acc)\n",
    "# print(test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/test_labeled.txt\",\"r\") as data:\n",
    "    train = csv.DictReader(data , delimiter='\\t')\n",
    "    IDS = []\n",
    "    for row in train:\n",
    "        IDS.append(row[\"pair_ID\"])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model2 = load_model('model_weights-opts-6-0.5902171706921048-0.5902171706921048.weights', 'model_arch_att-optsss-0.5254718895879845-0.5254718895879845.yaml')\n",
    "# model2 = load_model('model_weights-opts-4-0.5810838238278871-0.5810838238278871.weights', 'model_arch_att-opts-4-0.5810838238278871-0.5810838238278871.yaml')\n",
    "model2 = load_model('model_weights-opts-3-0.5786482646640958.weights', 'model_arch_att-opts-3-0.5786482646640958.yaml')\n",
    "\n",
    "train_acc = compute_acc(xy_train, Z_train, VOCABULARY, model2, opts)\n",
    "dev_acc = compute_acc(xy_dev, Z_dev, VOCABULARY, model2, opts)\n",
    "test_acc = compute_acc(xy_test, Z_test, VOCABULARY, model2, opts)\n",
    "\n",
    "\n",
    "convert_dict = {\n",
    "      0:'ENTAILMENT',\n",
    "      1:'NEUTRAL',\n",
    "      2:'CONTRADICTION'\n",
    "    }\n",
    "\n",
    "print(train_acc)\n",
    "print(dev_acc)\n",
    "print(test_acc)\n",
    "scores = model2.predict(xy_test, batch_size=opts['batch_size'])\n",
    "print(\"***\")\n",
    "col_sum = np.sum(np.array(scores), axis=0)\n",
    "print(col_sum)\n",
    "prediction = []\n",
    "with open(\"Results.txt\", \"r\") as f:\n",
    "    train = csv.DictReader(data , delimiter='\\t')\n",
    "    count = 1\n",
    "    for row in train[:3]:\n",
    "        print(row)\n",
    "        \n",
    "#     for i in range(scores.shape[0]):\n",
    "#         l = np.argmax(scores[i])\n",
    "#         prediction.append(convert_dict[l])\n",
    "#         f.write(\"%s\\t%s\\n\" % (str(IDS[i]), prediction[i]))\n",
    "\n",
    "\n",
    "\n",
    "    convert_dict = {\n",
    "      'ENTAILMENT': 0,\n",
    "      'NEUTRAL': 1,\n",
    "      'CONTRADICTION': 2\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
